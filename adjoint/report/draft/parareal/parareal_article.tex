\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
\usepackage{cite}


\newtheorem{theorem}{Theorem}

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\usepackage{graphicx}

\title{Parareal explanation}


\begin{document}
\maketitle
\section{Introduction}
Want to summarize the paper \cite{lions2001resolution}, and explain the parareal scheme. The parareal scheme is used to parallelize differential equations in temporal direction, by decomposing the time interval $I=[0,T]$. The equation looks like the following:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial u}{\partial t} + Au = f \ 				\textit{For $t \in I$} \\
		u(0)=u_0
	\end{array}
   \right.			
\end{align} 
Decomposing the interval $I$, means dividing the interval into $N$ subintervals $\{I_n = [T^{n},T^{n+1}]\}_{n=0}^{N-1}$, with length $\Delta T = T/N$. We also define new equations for each interval:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial u^n}{\partial t} + Au^n = f \ 				\textit{For $t \in I^n$} \\
		u^n(T^n)=\lambda^n
	\end{array}
\right.	
\end{align}
Here $\lambda^0=u_0$. If the $\lambda$s are known, we can solve the equations independently on each interval. The problem is that the $\lambda$s depend on the previous intervals, and need to be calculated by solving the equation. The parareal scheme is a way of dealing with this.
\section{The parareal scheme} 
The parareal scheme finds the $\lambda$s, by solving the equation on the entire interval using an implicit euler scheme on a very course resolution, and then using this numerical solution $Y$ at the decomposed interval boundaries $\{T^n\}_{n=1}^{N-1}$ as $\lambda$s, for the real solver $y$. We can then repeat this process, by propagating the jumps $S^n=y^{n-1}(T^n)-Y^n$ using the course scheme. This creates an iteration, that looks like this:
\begin{align*}
&(i) \ \textit{Set $S^n_k = y_k^{n-1}(T^n)-Y_k^n$} \\
&(ii) \ \textit{Propogate the jumps with the couarse scheme $\delta_k$ using (\ref{propagator})} \\
&(iii) \ \textit{Update $Y_{k+1}^n=y_k^{n-1}(T^n) + \delta_k^n$}
\end{align*} 
\\
\\
To illustrate how it works, I will set up the parareal scheme for a simple ODE:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y}{\partial t}(t)=-ay(t) \ 				\textit{on $[0,T]$} \\
		y(0)=y_0
	\end{array}
\right.	\label{ODE_eks}
\end{align}
If we discretize (\ref{ODE_eks}) using implicit euler, we get:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{Y^{n+1}-Y^{n}}{\Delta T}+aY^{n+1}=0  \\
		Y^0=y_0
	\end{array}
\right.	\label{couarse_euler}
\end{align}
Notice that the interval $I$, is discretized using the same time difference as the time decomposition. Then we introduce $N$ new equations on each interval, i.e.
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y^n}{\partial t}(t)=-ay^n(t) \ 				\textit{on $[T_n,T_{n+1}]$} \\
		y(T_n)=Y^n
	\end{array}
\right. \label{interval_eqs}
\end{align}
We can now solve (\ref{interval_eqs}) independently either exactly or using some numerical scheme. So if we first solve (\ref{couarse_euler}) and then (\ref{interval_eqs}), and define $Y_1^n=Y^n$, $y_1^n(t)=y^n(t)$, we can set an initial jump $S_1^n=y_1^{n-1}(T^n)-Y_1^n$ and start the iterative jump propagation process. Lets now specify, whet is meant by \textit{propogate the jumps with the couarse scheme}:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\delta_k^{n+1}-\delta_k^{n}}{\Delta T}+a\delta_k^{n+1}=\frac{S_k ^n}{\Delta T}  \\
		\delta_k^0=0
	\end{array}
\right. \label{propagator}
\end{align} 
\section{Error estimate}
According to \cite{lions2001resolution} we have the following error estimate for the parareal scheme of (\ref{ODE_eks}):
\begin{gather*}
\forall \ n,0\leq n\leq N-1 \\ |Y_k^n-y(T^n)| + \max_{t\in[T^N,T^{n+1}]}|y_k^n(t)-y(t)| \leq c_k\Delta T^k
\end{gather*} 
This suggest that the max norm difference between exact and parareal solution looks like this:
\begin{align*}
\max_{t\in[0,T]}|y_k(t)-y(t)| \leq C_k\Delta T^k
\end{align*}
The error estimate for a first order scheme, like implicit euler, we know looks like this:
\begin{align*}
\max_{t\in[0,T]}|y_{\Delta t}(t)-y(t)| \leq C\Delta t
\end{align*}
It would then be reasonable that the number of iterations for the parareal scheme needed to reach the same error as the fine first order solver, would be:
\begin{align*}
k\approx\frac{\log(\Delta t)}{\log(\Delta T)}
\end{align*}
This assumes that the constants $C_k$ and $C$ are similar. This is a reasonable assumption, since these constants depend on the parameters of the equation (\ref{ODE_eks}). 
\\
\\
To illustrate this lets try to solve a version of (\ref{ODE_eks}) using an implicit euler scheme and parareal with a fine implicit euler solver. I will use  the parameters $a=1.3$, $T=1$ and $y_0= 3.52$. The fine resolution will be held constant at $\Delta t=\frac{1}{10000}$. I then calculated how many propagation iterations I needed to to reduce the max-norm error to below $\Delta t$ for different time interval decompositions $N$. Table follows below:
 \begin{center}
    \begin{tabular}{| l | l | l | l |}
    \hline
     & $\frac{\log(\Delta t)}{\log(\Delta T)}$&iterations  & $\max_{t\in[0,T]}|y_k(t)-y(t)|$   \\ \hline
    $N=1$ &0 & 1& 8.416625e-5 	\\ \hline
    $N=2$ &13.3 &2& 8.416625e-5 	\\ \hline
    $N=5$ &5.7&4& 8.416625e-5	\\ \hline
    $N=10$ &4&4& 8.399513e-5	\\ \hline
    $N=25$ &2.8&3& 	8.737827e-5\\ \hline
    $N=100$ &2&2&	6.302522e-5\\ \hline
    \end{tabular}
\end{center}
Notice that for two intervals, the excpected number of iterations is $13$, however with $N=2$, we get the same error as the one iterval solution in only two iterations. The reason for this, is that doing $N$ iterations of the parareal scheme when using $N$ time decompositions, is the same as solving the equation without any decompositions. 
\section{Optimal control}
In \cite{maday2002parareal} the authours try to build a bridge between pararel and optimal control with time dependent PDE constraints. The problem PDE they look at is:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y}{\partial t}(t)+Ay=Bv  \ \textit{For $t \in [0,T]$}\\
		y(0)=y^0
	\end{array}
\right. \label{OC_PDE}
\end{align}
The functional they use is on the following form:
\begin{align}
J(v) = \frac{1}{2}\int_0^T ||v(t)||^2 dt + \frac{\alpha}{2}||y(T)-y^T||^2 \label{functional}
\end{align} 
We want to decompose this problem in time, and therefore split the interval $[0,T]$ into $N$ smaller parts. We then split the equation (\ref{OC_PDE}) into $N$ independent equations with initial condition $\{\lambda_n\}_{n=0}^{N-1}$:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y_n}{\partial t}(t)+Ay_n=Bv_n  \ \textit{For $t \in [T_n,T_{n+1}]$}\\
		y_{n}(T_n)=\lambda_i
	\end{array}
\right. \label{OC_PPDE}
\end{align} 
Here $\lambda_0=y^0$. In order to be able to solve the optimisation problem we need to introduce penalty terms in the functional (\ref{functional}). The new functional looks like this:
\begin{align}
\hat{J}(v,\lambda) = \frac{1}{2}\int_0^T ||v(t)||^2 dt + \frac{\alpha}{2}||y_{N-1}(T)-y^T||^2 + \frac{\mu}{2}\sum_{n=1}^{N-1} (y_{n-1}(T_{n})-\lambda_{n}) \label{penalty_func}
\end{align} 
What we now are interested in, is the gradient of (\ref{penalty_func}) and the so called adjoint equation. One can show that the directional derivative of (\ref{penalty_func}) is:
\begin{align}
\langle\hat{J}'(v,\lambda),(s,l) \rangle = \int_0^T (B^*p+v)sdt + \sum_{n=1}^{N-1}(p_n(T_n) - p_{n-1}(T_n))l_n \label{penalty grad}
\end{align} 
The $p$ function in (\ref{penalty grad}), is the adjoint equations :
\begin{align}
\left\{
     \begin{array}{lr}
		-\frac{\partial p_n}{\partial t}(t)+A^*p_n=0  \ \textit{For $t \in [T_n,T_{n+1}]$}\\
		p_{n}(T_{n+1}) = \mu(y_n(T_{n+1})) -\lambda_{n+1}) \ \textit{for $N\neq N-1$} \\
		p_{N-1}(T_N)=\alpha(y_{N-1}(T_{N}))-y^T) 
	\end{array}
\right. 
\end{align}\label{adjoints}
\section{Virtual problem}
To find the connection between the optimal control problem and parareal, \cite{maday2002parareal} looks at the virtual problem, i.e (\ref{OC_PDE}) with $B=0$ and (\ref{functional}) with $\alpha=0$. This yields the equation:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y}{\partial t}(t)+Ay=0  \ \textit{For $t \in [0,T]$}\\
		y(0)=y^0
	\end{array}
\right. \label{viritual}
\end{align}
The functional of the problem is simply:
\begin{align}
\hat{J}(\lambda) = \sum_{n=1}^{N-1} (y_{n-1}(T_{n})-\lambda_{n})
\end{align}
The solution to this problem is simply $\lambda_n=y_{n-1}(T_n)$. If we assume $A$ is time independent, we can define $\bold{F}_{\Delta T}(\omega)$ to be the evaluation at $\Delta T$ of the equation:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\partial y}{\partial t}(t)+Ay=0  \\
		y(0)=\omega
	\end{array} 
\right. \label{F_operator}
\end{align}
The solution of the problem can then be stated as $\lambda_n = \bold{F}_{\Delta T}(\lambda_{n-1})$. This problem then becomes a system on the following form:
\begin{align}
  \left[ \begin{array}{cccc}
   I & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & I & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & I  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} & I   \\
   \end{array}  \right] 
   \left[ \begin{array}{c}
   \lambda_0 \\
   \lambda_1 \\
   \cdots \\
   \lambda_{N-1} \\
   \end{array}  \right] =
   \left[ \begin{array}{c}
   y^0 \\
   0 \\
   \cdots \\
   0 \\
   \end{array}  \right]
\end{align}
Or just simply:
\begin{align}
M \ \Lambda \ = \ H \label{vir_mat_sys}
\end{align}
We now introduce a coarse solver $G_{\Delta T}(\omega)$, that is inspired by (\ref{couarse_euler}), i.e. we use implicit euler to calculate $G_{\Delta T}(\omega)$:
\begin{align}
\frac{G_{\Delta T}(\omega) -\omega }{\Delta T } + A G_{\Delta T}(\omega) = 0
\end{align}  
We then define the following iterative process to find the $\lambda$s:
\begin{align}
\lambda_{n+1}^{k+1} = G_{\Delta T}(\lambda_{n}^{k+1}) + \bold{F}_{\Delta T}(\lambda_{n}^{k})-G_{\Delta T}(\lambda_{n}^{k})
\end{align} 
On matrix form this looks like:
\begin{align}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}(H-M\Lambda^k) \label{matrix_iter1}
\end{align}
The matrix $\bar{M}$ is the $G_{\Delta T}$ version of $M$, i.e:
\begin{align}
\bar{M} = \left[ \begin{array}{cccc}
   I & 0 & \cdots & 0 \\  
   -G_{\Delta T} & I & 0 & \cdots \\ 
   0 &-G_{\Delta T} & I  & \cdots \\
   0 &\cdots &-G_{\Delta T} & I   \\
   \end{array}  \right]
\end{align}
Solving the system (\ref{vir_mat_sys}), solves the the problem $\hat{J}(\Lambda)=0$. What we are usually interested in, is solving $\hat{J}'(\Lambda)=0$. From (\ref{penalty grad}), we know that: $$\hat{J}'(\Lambda) = \{p_n(T_n)-p_{n-1}(T_n)\}_{n=1}^{N-1}$$ We also see that the adjoint equations (\ref{adjoints}) can be used to define $\bold{F}_{\Delta T}^*(\omega)$ as we did in (\ref{F_operator}). It then turns out that solving $\hat{J}'(\Lambda)=0$ is the same as solving the system:
\begin{align}
M^* \ M \ \Lambda \ = \ M^* \ H \label{vir_grad_sys}
\end{align}
where 
\begin{align}
M^*= \left[ \begin{array}{cccc}
   I & -\bold{F}_{\Delta T}^* & 0 & 0 \\  
   0 & I & -\bold{F}_{\Delta T}^* & \cdots \\ 
   \cdots &0 & I  & -\bold{F}_{\Delta T}^* \\
   0 &\cdots &\cdots & I   \\
   \end{array}  \right]
\end{align}
One could then solve the system (\ref{vir_grad_sys}), using an iteration as in (\ref{matrix_iter1}), but with $\bar{M}^{-1}\bar{M}^{-*}$ instead of $\bar{M}^{-1}$. \cite{maday2002parareal} then they propose $\bar{M}^{-1}\bar{M}^{-*}$ as a preconditioner for solving the original penalized control problem (\ref{OC_PPDE}-\ref{penalty_func}) using the gradient method. 
\bibliography{ppaper}
\bibliographystyle{plain}
\end{document}