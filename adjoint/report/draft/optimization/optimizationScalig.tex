\documentclass[11pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
\usepackage{cite}


\newtheorem{theorem}{Theorem}

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\usepackage{graphicx}

\title{Optimization}


\begin{document}
\section{Steepest descent}
The steepest descent method is a very simple line search method that is defined by the following iteration:
\begin{align*}
x^{k+1} = x^k - \alpha\nabla J(x^k)
\end{align*}
If we want to use this method to solve a timedecomposed problem using penalty terms, we would get the following:
\begin{align*}
(v^{k+1},\lambda^{k+1})= (v^k,\lambda^k)-\alpha\nabla J(v^k,\lambda^k)
\end{align*}
One problem with the steepest descent method, is that it is sensitive to bad scaling, and in many cases the size of the gradient with respect to $v$ and $\lambda$ may have a difference of several orders of magnitude. This would result in extremely slow convergence. One way of handling this is to split up the update:
\begin{align*}
v^{k+1} &= v^k - \alpha\nabla J(v^k)\\
\lambda^{k+1} &= \lambda^k - \alpha\nabla J(\lambda^k)
\end{align*}
\section{L-BFGS}

\end{document}