\chapter{Summary and Conclusions} \label{summary chap}
The topic of this thesis is the parallelization of optimization problems with time-dependent differential equation constraints. The method we have proposed is developed around reducible problems, where the state equation constraint is well posed. Our method has been explained through a simple ODE constrained example problem, and we have also used this example for verification and testing. Even though we only considered an ODE example, we believe that our method also is applicable to time-dependent PDE constrained problems. Examples of such constraints can be PDEs on form (\ref{PDE_exs}).
\begin{align}
\left\{
     \begin{array}{lr}
       	u_t(x,t) + Au(x,t)=f \quad \textrm{for } \ (x,t)\in U\times(0,T),\\
       	u(x,0)=u_0(x).
     \end{array}
   \right. \label{PDE_exs}
\end{align}
Here $A$ is a differential operator. The reason for making this claim, is that after the spatial discretization is taken care of, evolving equations of type (\ref{PDE_exs}) through time is done in the same way as for ODEs. 
\\
\\
The main contribution of this thesis is the analysis of the Parareal-based preconditioner $Q$ (\ref{Q_PC}) proposed in \cite{maday2002parareal} and the introduction of algorithm \ref{PPC_PEN_ALG} based on the quadratic penalty method from section \ref{penalty_sec} and a preconditioned BFGS algorithm. The analysis of the Parareal-based preconditioner showed that $Q$ is positive definite and an approximation of the inverse Hessian of the penalized objective function. This made us able to use the preconditioner from \cite{maday2002parareal} in the BFGS algorithm as an initial inverse Hessian approximation. The Parareal-preconditioned BFGS algorithm is the central part of algorithm \ref{PPC_PEN_ALG}. We use the preconditioned BFGS method to minimize the penalized objective function $\hat J_{\mu}$ for increasing penalty parameters $\mu$. The idea is that when $\mu$ gets sufficiently large, the minimizer of $\hat J_{\mu}$ will also approximate the minimizer of $\hat J$.
\\
\\
An important aspect of our method is the evaluation of the reduced and penalized objective function $\hat J_{\mu}(v,\Lambda)$ and its gradient. In chapter \ref{disc_chap} we explained how to discretize $\hat J_{\mu}$ in context of the example problem, and we also explained how we can parallelize the evaluation of $\hat J_{\mu}$ and $\hat J_{\mu}'$. In chapter \ref{Verification chapter} we verified different features of our implementations of both the sequential and parallel algorithms for the example problem. In particular, we looked at the consistency of our method. We observed that the solution obtained by algorithm \ref{PPC_PEN_ALG} approached the numerical solution of the sequential algorithm when we increased the penalty parameter $\mu$. However, we also noticed, that when the difference between the function values of the sequential and parallel algorithm became close to machine precision, the control solution of the parallel algorithm stopped converging towards the sequential solution. This observation underlines a limitation of our algorithm. This limitation is that we can not always guarantee the consistency of our method, especially when the time steps used to dicretize the state equation become small. What we however can expect from the iterates $\{(v^k,\Lambda^k)\}$ obtained by algorithm \ref{PPC_PEN_ALG} is that they will converge to a feasible point.
\\
\\
In chapter \ref{Experiments chapter} we tested the performance of algorithm \ref{PPC_PEN_ALG} on an example problem. We measured the performance in both accuracy and potential speedup. What we were particularity interested in was investigating whether the performance of the preconditioned algorithm is independent of the parameters $N$, $n$ and $\mu$, representing number of decompositions, number of fine time steps and penalty parameter. What we found was that increasing the first two of these parameters did not significantly worsen the performance of algorithm \ref{PPC_PEN_ALG}. In the case of number of processes and decomposed intervals $N$, we even observed improved results when $N$ was increased. The most likely cause of this, is that when we increase $N$, the Parareal-based preconditioner $Q$ becomes an improved approximation of the inverse Hessian of $\hat J_{\mu}$. For the penalty parameter $\mu$, the picture became more nuanced. We observed that the computational cost of minimizing $\hat J_{\mu}$ increased sharply when $\mu$ became vary large. We did however also see that this effect could be diminished by doing multiple iterations of algorithm \ref{PPC_PEN_ALG}. In chapter \ref{Experiments chapter} we also measured actual wall clock speedup for as many as 120 cores. This experiment was conducted on the Abel computer cluster, and we were able to achieve actual speedup. We for example obtained a speedup of 23.5 when using 120 cores. 
\section{Future Work}
The algorithm proposed in this thesis experiences trouble, when the penalty parameter gets large. Several strategies for improving the method and the Parareal preconditioner could be taken. One example is to replace the quadratic penalty method for removing the virtual constraints with the more advanced augmented Lagrangian method used in \cite{rao2016time}. Instead of using the BFGS method for minimizing the penalized objective function, other optimization algorithms and techniques could be considered. In \cite{maday2003parallel, maday2007monotonic} for example, the authors used an alternating direction decent method to minimize the penalized functional of an optimal control problem. Another potential improvement can perhaps be to find a preconditioner $Q_v$ that approximates the inverted Hessian of $\hat J(v)$, and then alter the Parareal-based preconditioner in the following way:
\begin{align*}
Q = \left[ \begin{array}{cc}
	Q_v & 0 \\
	0 & Q_{\Lambda} \\
	\end{array} \right]
\end{align*}
Different approaches to parallelization of optimal control in temporal direction might also be considered. One could for example try to restrict the parallelization to the differential equations. By this we mean solving the optimal control problem in the traditional serial way, but when we need to solve the state and adjoint equations, we solve these using for example the Parareal algorithm. Using this strategy to parallelize optimal control problems, would simplify the optimization, but also make solving the state and adjoint equations more involved. Load bearing, which is simple for the method we have proposed would also become a more complicated issue if this alternative strategy is chosen.
\\
\\
In this thesis we have tested algorithm \ref{PPC_PEN_ALG} for only one problem. It would be interesting to investigate how our proposed algorithm performs for other more complex problems. Since the problem we used was so easily solved, it was difficult to compute with the execution time of the serial algorithm. If a more challenging problem were solved, the potential for higher speedups might therefore be higher. Another issue that we have not considered, is how to choose the coarse propagator $\bold G_{\Delta T}$, that we use to construct the preconditioner. This becomes more important when we are solving PDE constrained problems, since we then also have a spatial discretization. One could then consider a Parareal-based preconditioner defined by a propagator $\bold G_{\Delta T}$  that uses a coarse dicretization in both space and time.



