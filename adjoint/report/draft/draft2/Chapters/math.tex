\chapter{Optimal control with ODE constraints}
\section{General optimal control problem}
In this thesis we are only looking at optimal control problems with time dependent differential equation(DE) constraints. This problem is only a part of the more general control problem, which can be formulated as:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y,v) \\
\textrm{Subject to:} \ &E(y,v)=0.
\end{align}
Here $J: Y\times V\rightarrow\mathbb{R}$ is the objective function that we want to minimize, and the spaces $Y,V$ are reflexive Banach spaces. The operator $E:Y\times V \rightarrow Z$, where $Z$ is a Banach spaces, is called the state equation when we set it equal to zero. The state equation should have the property that $\forall v \in V$, $\exists! y(v)\in Y$ that satisfies:
\begin{align*}
E(y(v),v)=0.
\end{align*}
The first question we need to ask ourself, is under what conditions does the above problem even have a solution. This depends on the operators $J,E$ and the spaces $Y,V,Z$, and we will therefore come back to this question when we have defined our example problem. Instead let us assume for now that a solution exists, and try to derive a way to find it. If we replace $y$ with $y(v)$ we can define the reduced problem:
\begin{align}
\underset{v\in V}{\text{min}} \ \hat J(v)=\underset{v\in V}{\text{min}} \ J(y(v),v)  \label{reduced problem}
\end{align}
The problem (\ref{reduced problem}) is called the reduced problem because we have moved the differential equation constraints into the functional, and by doing so transformed the constrained problem into an unconstrained one. Since the aim is to find the minimum of a function, it is natural, that to solve (\ref{reduced problem}), we need to be able to differentiate the objective function $\hat{J}$ with respect to the control $v$. There are different ways of doing that, but I will focus on the so called adjoint approach, which is the most computational effective way of calculating the gradient of $\hat{J}$.
\subsection{Example problem} \label{example_sec}
To better understand the adjoint approach to gradient evaluation of the reduced objective function, we will define a simple optimal control problem with ODE constraints, so that we later can derive its adjoint equation and gradient. The problem will also be used to test and verify the implementation in chapter \ref{Verification chapter} and \ref{Experiments chapter}. In our example both the state $y$ and the control $v$ will be functions on an interval $[0,T]$. This allows us to define the objective function:
\begin{align}
J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 \label{exs_J}
\end{align}
The state equation $E(y,v)=0$ is a linear, first order equation with the control as a source term:
\begin{align}
\left\{
     \begin{array}{lr}
       	y'(t)=ay(t) + v(t) \ \textrm{for } \ t\in(0,T),\\
       	y(0)=y_0.
     \end{array}
   \right. \label{exs_E}
\end{align}
Now that we actually have an optimal control problem, we can return to the question of solvability. To achieve this, we will write up a general result from \cite{hinze2008optimization} about linear quadratic problems, which also includes problem (\ref{exs_J}-\ref{exs_E}). Linear quadratic problems are a quite simple class of problems, and more general theorems for problems with non-linear equations and non-quadratic objective functions do however exist, and can be found in \cite{hinze2008optimization}.
\begin{theorem}
The linear quadratic optimization problems, that can be written on the form:
\begin{align*}
&\underset{y\in Y,v\in V}{\text{min}}J(y,v) = \frac{1}{2}||Qy-q||_H^2 + \frac{\alpha}{2}||v||_V^2 \\
&\textit{Subject to:} \ Ay + Bv = g
\end{align*}
Where H,V are Hilbert spaces, Y,Z are Banach spaces, $q\in H$, $g\in Z$ and $A:Y\rightarrow Z$, $B:V\rightarrow Z$ and $Q:Y\rightarrow H$ are bounded linear operators. If $\alpha>0$, the above written problem has a unique solution pair $(y,v)\in Y\times V$.
\end{theorem}   
\begin{proof}
See \cite{hinze2008optimization}
\end{proof}
It is obvious, that the objective function (\ref{exs_J}) of our example problem is a quadratic function that fits into the setting of the above theorem, and since the equation (\ref{exs_E}) is linear, with solution $y(t) = e^{a t}(C(y_0)+\int_0^te^{-a\tau}v(\tau)d\tau)$, which is unique for every integrable $v$, it is clear that problem (\ref{exs_J}-\ref{exs_E}) is a linear quadratic optimization problem. 
\section{The adjoint equation and the gradient}
The usual way of finding the minimum(or maximum) value of a function $J$, is to solve the equation $J'(x)=0$. Solving this equation usually requires us to be able to evaluate, or have an expression for the derivative of $J$. With this in mind let us try to find the gradient $\hat{J}'(v)$ of the reduced problem (\ref{reduced problem}).
\begin{align*}
\hat{J}'(v) = DJ(y(v),v) = y'(v)^*J_y(y(v),v) + J_v(y(v),v)
\end{align*}
The problematic term in the above expression, is $y'(v)^*$, since the function $y(v)$ is implicitly defined through $E$. We can however find an equation for $y'(v)^*$ if we differentiate the state equation with respect to $v$, and assume that the operator $E_y(y(v),v):Y\rightarrow Z$ is continuously invertible.
\begin{align*}
DE(y(v),v)=0 &\Rightarrow E_y(y(v),v)y'(v)=-E_v(y(v),v) \\ &\Rightarrow y'(v)=-E_y(y(v),v)^{-1}E_v(y(v),v) \\ &\Rightarrow y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}.
\end{align*}
Before inserting $y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}$ into our gradient expression, we first define the adjoint equation as:
\begin{align}
E_y(y(v),v)^{*}p=J_y(v). \label{general adjoint}
\end{align}
This now allows us to write up the gradient as follows:
\begin{align}
\hat{J}'(v)&= y'(v)^*J_y(y(v),v) + J_v(y(v),v)\\
&=-E_v(y(v),v)^*E_y(y(v),v)^{-*}J_y(y(v),v) + J_v(y(v),v) \\
&= -E_v(y(v),v)^*p +J_v(y(v),v). \label{gradient}
\end{align}
Evaluating the gradient for a control variable $v\in V$ typically requires solving both the state and adjoint equation, and then inserting the solutions into the expression for the gradient (\ref{gradient}). To better illustrate how gradient evaluation works let us derive the adjoint equation and the gradient of the problem introduced in section \ref{example_sec}.
\subsection{Example adjoint}
We want to derive the gradient of problem (\ref{exs_J}-\ref{exs_E}), and in order to do so, we need the adjoint equation of the problem, which we now state in the theorem below, followed by its derivation.
\begin{theorem}
The adjoint equation of the problem (\ref{exs_J}-\ref{exs_E}) is:
\begin{align*}     
-p'(t) &= ap(t) \\
p(T) &= \alpha(y(T)-y^T)     
\end{align*}
\end{theorem}
\begin{proof}
Before we calculate the different terms used to derive the adjoint equation, we want to fit our ODE into an expression $E$. We do this by writing up the weak formulation of the equation:
\begin{gather*}
\textrm{$y \in L^2(0,T)$ such that}\\
L[y,\phi] = \int_0^T-y(t)\phi'(t)-ay(t)\phi(t)dt -y_0\phi(0)+y(T)\phi(T)-\int_0^Tv(t)\phi(t)=0\\ \forall \ \phi \in C^{\infty}((0,T))
\end{gather*}
To derive the adjoint we need $E_y$ and $J_y$. For $E_y$ we define $(\cdot,\cdot)$ to be the $L^2$ inner product over $(0,T)$. Since the weak formulation includes evaluation at $t=0$ and $t=T$, I want to define an operator $\delta_{\tau}$ that represent function evaluation in an $L^2$ inner product setting. Do this in the following way: Let $\tau \in [0,T]$ then:
\begin{align*}
(v,\delta_{\tau}w) =(\delta_{\tau}v,w) = \int_0^Tv(t)\delta_{\tau}w(t)dt = v(\tau)w(\tau)
\end{align*}
Using the above notation, we can write $E_y$ quite compactly as:
\begin{align*}
E_y=L_y[\cdot,\phi]=(\cdot,(-\frac{\partial}{\partial t} - a + \delta_T)\phi)  
\end{align*}
Let us be more thorough with $J_y$, which is the right hand side of the adjoint equation.
\begin{align*}
J_y(y(v),v) &= \frac{\partial}{\partial y}(\frac{1}{2}\int_0^Tv^2dt + \frac{\alpha}{2}(y(T)-y^T)^2) \\ &= \frac{\partial}{\partial y} \frac{\alpha}{2}(y(T)-y^T)^2 \\
&= \frac{\partial}{\partial y}\frac{\alpha}{2}(\int_0^T \delta_T(y-y^T)dt)^2 \\
&= \alpha\delta_T\int_0^T \delta_T(y(t)-y^T)dt \\
&= \alpha\delta_T(y(T)-y^T)
\end{align*}
We have $E_y=(\cdot,(-\frac{\partial}{\partial t} - a + \delta_T)\phi)$, but we want to find its adjoint $E_y^*$. Therefore let us explain what is ment by an adjoint on the context of the $L^2$ inner product $(\cdot,\cdot)$. Let $B:L^2(0,T)\rightarrow L^2(0,T)$ be a linear operator. Then the adjoint of $B$, $B^*$ is an operator on $ L^2(0,T)$, such that $\forall v,w\in L^2(0,T)$:
\begin{align*}
(Bv,w)=(v,B^*w).
\end{align*}
The adjoint of the bilinear form $E_y=L_y$, would therefore be a bilinear form $L_y^*=E_y^*$ such that $\forall v,w\in L^2(0,T)$:
\begin{align*}
L_y[v,w]=L_y^*[w,v].
\end{align*}
Therefore to derive the adjoint of $E_y$, we will insert two functions $v$ and $w$ into $L_y[v,w]$, and try to change the places of $v$ and $w$.
\begin{align*}
E_y&=L_y[v,w]=\int_0^T-v(t)(w'(t)+aw(t))dt + v(T)w(T) \\
&=\int_0^Tw(t)(v'(t)-av(t))dt + v(T)w(T)-v(T)w(T) +v(0)w(0) \\
&=\int_0^Tw(t)(v'(t)-a v(t))dt+v(0)w(0) \\
&=L_y^*[w,v]=E_y^*
\end{align*}
If we multiply $J_y$ with a test function $\psi\in C^{\infty}((0,T))$ and set $L_y^*[p,\psi]=(J_y,\psi)$, we get the following equation: Find $p$ such that:
\begin{align*}
&\int_0^Tp(t)\psi'(t)-a p(t)\psi(t)dt + p(0)\psi(0)= \alpha(y(T)-y^T)\psi(T)\ \quad\forall \ \psi \in C^{\infty}((0,T))
\end{align*}
If we then do partial integration, the equation reads: Find $p$ such that:
\begin{align*}
&\int_0^T(-p'(t)-ap(t))\psi(t)dt +p(T)\psi(T)= \alpha(y(T)-y^T)\psi(T)\ \quad\forall \ \psi \in C^{\infty}((0,T))
\end{align*}
Using this we get the strong formulation:
\begin{align*}
   \left\{
     \begin{array}{lr}
       -p'(t) = ap(t) \\
       p(T) = \alpha( y(T)-y^T)
     \end{array}
   \right.
\end{align*}
\end{proof}
With the adjoint we can find the gradient of $\hat{J}$. Lets state the result first.
\begin{theorem}
The gradient of the reduced objective function $\hat{J}$ with respect to v is
\begin{align}
\hat{J}'(v)=v+p. \label{exsample_grad}
\end{align} 
\end{theorem}
\begin{proof}
Firstly we need $J_v$ and $E_v^*$:
\begin{align*}
J_v &= v \\
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi).
\end{align*}
Since $L_v[\cdot,\phi]$ is symmetric, $E_v^*=E_v$, and strongly formulated, $E_v=-1$. By inserting relevant terms into (\ref{gradient}), we get the gradient:
\begin{align*}
\hat{J}'(v)&=-E_v^*p + J_v \\
&= p+v. 
\end{align*} 
\end{proof}
Evaluating the gradient of our example problem can now be boiled down to the following three steps:
\begin{align*}
1.\quad & \textrm{Solve the state equation for $y$}\\
2.\quad & \textrm{Use $y$ to solve the adjoint equation for $p$}\\
3.\quad & \textrm{Insert $p$ and control $v$ into gradient formula (\ref{exsample_grad})}
\end{align*}
To see why the above procedure is computationally effective, let us compare it with the finite difference approach to evaluating the gradient. Using finite difference we can find an approximation of the directional derivative of $(\hat J'(v),h)_V$ in direction $h\in V$, by choosing a small $\epsilon>0$ and setting:
\begin{align*}
(\hat J'(v), h)_V\approx\frac{\hat J(v+\epsilon h)-\hat J(v)}{\epsilon}
\end{align*} 
Looking at the above procedure for gradient evaluation, we can see why the adjoint approach is computationally efficient. No matter the problem size, we only need to solve two ODEs to find the gradient. One alternative to the adjoint approach is to find the gradient using finite difference.  In comparison evaluating the gradient using finite difference for example, would force us to solve the state equation one time for each component of the control. In a setting where the control is the source term of the state equation, using finite difference would mean that the number of ODEs we would have to solve would be equal to the number of time steps we used do discretize the problem.
\subsection{Exact solution of the example problem}

\section{Optimization algorithms}
Deriving and solving the adjoint equation gives us a way of evaluating the gradient of optimal control problems with ODE constraints. With the gradient we can now solve our optimal control problems by using some optimization algorithm. There exists many different optimization algorithms, but here we will only mention line search methods that are useful to me in the thesis. These are the steepest descent method, the related BFGS and L-BFGS methods.  
\subsection{Line search methods and steepest descent}
Line search methods are algorithms that help us solve problems of the type: 
\begin{align*}
\min_x f(x), \ f:\mathbb{R}^n\longrightarrow\mathbb{R}
\end{align*}
Line search methods are iterative methods that starts at an initial guess $x^0$ and generate a sequence $\{x^k\}$ that hopefully will converge to a solution. The k-th iteration in the algorithm can be described in the following way:
\begin{align*}
1. &\textit{Choose direction $p_k\in\mathbb{R}^n$} \\
2. &\textit{Choose step length $\alpha_k\in\mathbb{R}$} \\
3. &\textit{Set $x^{k+1}=x^k + \alpha_kp_k$} 
\end{align*}
If $f$ is differentiable, a necessary condition for a point $x^*\in\mathbb{R}^n$ to be a minimizer of $f$, is that $\nabla f(x^*)=0$. This optimality condition is used to create a stopping criteria for line search methods in the following way: Given a tolerance $\tau>0$ stop the line search iteration when
\begin{align}
||\nabla f(x^k)||<\tau \label{opti_con}
\end{align}  
What separates different line search methods, is how one chooses descent direction $p_k$ and step length $\alpha_k$. Let us start with how to choose a good step length. There are several ways of doing this, but for our purposes the so called Wolfe conditions will suffice. The Wolfe conditions consists of two conditions on $f$, presented below:
\begin{align*}
f(x^k + \alpha_kp_k)&\leq f(x^k) + c_1\alpha_k\nabla f(x^k)\cdot p_k \\
\nabla f(x^k + \alpha_kp_k) \cdot p_k &\geq c_2 \nabla f(x^k)\cdot p_k
\end{align*}
Here we use constants $0<c_1<c_2<1$. The first Wolfe condition ensures that the decrease in function value is proportional to both step length and direction. The second condition is that the gradient of $f$ at $x^k + \alpha_kp_k$, should be less steep than at $x^k$, and therefore closer to fulfilling the optimality condition (\ref{opti_con}). If we can find a step length that satisfies these conditions we will use it. How to actually find a step length that satisfies the Wolfe conditions is quite involved, and we will therefore not go into this topic any further. Instead let us look into a couple of line search methods that will be used later in the thesis, starting with steepest descent.
\\
\\
The steepest descent method is a very simple line search method, where the step length $p_k$ is set to the negative gradient direction at point $x^k$, i.e $p_k = -\nabla f(x^k)$. This gives us the following update for each iteration:
\begin{align}
x^{k+1} = x^k - \alpha_k \nabla f(x^k) \label{SD_itr}
\end{align} 
The problem with steepest descent is that it converges quite slowly. This fact we get from the following theorem given in \cite{nocedal2006numerical}
\begin{theorem}
\label{SD_con}
Assume that $f:\mathbb{R}^n\longrightarrow\mathbb{R}$ is twice continuously differentiable, that the steepest decent method converge to a point $x^*$, and that the Hessian of $f$ at this point, $\nabla^2 f(x^*)$ is positive definite. Then the following holds:
\begin{align*}
f(x^{k+1})-f(x^*) \leq (\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2 (f(x^{k})-f(x^*))
\end{align*}  
Here $\lambda_1\leq\cdots\leq \lambda_n$ denotes the eigenvalues of $\nabla^2 f(x^*)$.
\end{theorem}   
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\subsection{BFGS and L-BFGS}
Since steepest descent has slow convergence, one usually uses faster line search methods to solve numerical optimization problems. One alternative is Newtons method, that includes the inverse Hessian in the search direction, i.e:
\begin{align}
x^{k+1} = x^k - \nabla^2 f(x^k)^{-1}\nabla f(x^k) \label{Newton}
\end{align}
The problem with this method is that we need to evaluate the inverse Hessian in gradient direction, which often is computationally expensive procedure. An alternative to the Newton method is so called quasi-Newton methods, which construct an approximate Hessian from information of previous iterates. One such method is the BFGS method\cite{broyden1970convergence,fletcher1970new,goldfarb1970family, shanno1970conditioning}. In BFGS we find the inverse Hessian approximation by using the following recursive formula:
\begin{align}
H^{k+1} &= (\mathbbold{1}-\rho_kS_k\cdot Y_k)H^k(\mathbbold{1} -\rho_kY_k\cdot S_k) + S_k\cdot S_k, \label{inv_H_apr} \\
S_k &= x^{k+1}-x^{k}, \\
Y_k &= \nabla f(x^{k+1})-\nabla f(x^{k}),\\
\rho_k &= \frac{1}{Y_k\cdot S_k}, \\
H^0&=\beta\mathbbold{1}.
\end{align}
The above formula is designed in such a way, that $H^k$ is symmetric positive definite. This gives us a requirement for the initial inverted Hessian approximation $H^0$, namely that it needs to be symmetric positive definite. The usual choice however, is just identity or a multiple $\beta$ of identity, where the multiple reflects the scaling of the variables. Strategies of how to chose a scaling factor $\beta$ is detailed in \cite{liu1989limited} and  \cite{gilbert1989some}. Each line search iteration for BFGS looks like:
\begin{align}
x^{k+1} = x^k - \alpha H^{k}\nabla f(x^k) \label{BFGS_itr}
\end{align} 
In the BFGS method information from all previous iterations is used to create the inverse Hessian approximation for the new iteration. An alternative to this is to limit the number of iterations the recursive formula remembers to only the latest iterations. This variation of the BFGS method is called L-BFGS\cite{nocedal1980updating}. The length of the memory need to be chosen in advance, and the typical choice is 10. Two advantages L-BFGS has over BFGS is firstly that it requires less memory storage than BFGS. Secondly one would think that recent iterations possess more valuable information than early iterations, and therefore should be given more weight. 
\subsubsection{Convergence results for Newton and quasi-Newton methods}
Both Newton and quasi-Newton methods converge faster than the steepest descent method. To show this we will include a couple of theorems from \cite{nocedal2006numerical}
concerning this topic. Let us however first look at a definition classifying convergence.
\begin{definition}
We say that a sequence $\{x^k\}$ converges linearly to a limit $L$, if there exists $\epsilon\in(0,1)$ such that
\begin{align*}
\lim_{k\rightarrow\infty}\frac{||x^{k+1}-L||}{||x^k-L||} =\epsilon.
\end{align*} 
If $\epsilon=0$ we say that $\{x^k\}$ converges superlinearly to $L$. Lastly we say that $\{x^k\}$ converges quadratically towards $L$, if
\begin{align*}
\lim_{k\rightarrow\infty}\frac{||x^{k+1}-L||}{||x^k-L||^2} =\epsilon.
\end{align*}
\end{definition}
Using the above definition we note that the convergence result for steepest descent from theorem \ref{SD_con} can be restated as: $f(x^k)$ converges linearly towards $f(x^*)$, with $\epsilon=(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2$. For problems where $\nabla^2 f(x^*)$ is badly conditioned this number is close to 1, and we would then experience bad convergence. Let us now continue with convergence results for Newton and quasi-Newton methods starting with Newton.
\begin{theorem}
\label{Newton_con}
Suppose $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is twice continuously differentiable, and that the Hessian $\nabla^2 f(x)$ is Lipschitz continuous in neighbourhood of a solution $x^*$ that satisfies $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ positive definite. Then the following holds for the Newton iteration \ref{Newton}:
\begin{align*}
1.\quad&\textrm{If $x^0$ is suffichently close to $x^*$, the sequence of iterates converge to $x^*$.}\\
2.\quad&\textrm{The rate of convergence of $\{x^k\}$ is quadratic}\\
3.\quad&\textrm{The sequence of gradient norms $ \{||\nabla f(x^k)||\}$ converges towards zero quadratically}
\end{align*}
\end{theorem}
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
The quadratic convergence of the Newton iteration is big improvement in comparison with steepest descent, however theorem \ref{Newton_con} also highlights one of the problems with the method. Since we need to invert $\nabla^2 f(x^k)$ to find the search direction at $x^k$, we need an initial $x^0$ sufficiently close to the actual solution for the iteration to even work. This problem does not arise in BFGS and L-BFGS, since the Hessian approximation is designed to be invertible. Unfortunately though, these quasi-Newton methods does not have the convergence properties of Newtons method, as the next result shows.
\begin{theorem}
Assume $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is three times differentiable. Consider then the quasi-Newton iteration $x^{k+1}=x^k-\alpha_k B_k^{-1}\nabla f(x^k)$, where $B_k$ is an approximation of the Hessian along the search direction $p_k=-B_k^{-1}\nabla f(x^k)$, satisfying the condition:
\begin{align*}
\lim_{K\rightarrow\infty}\frac{||(B_k-\nabla^2f(x^k))p_k||}{||p_k||}=0
\end{align*}
If the sequence $\{x^k\}$ originating from the quasi-Newton iteration converges to a point $x^*$, where $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite, the convergence is superlinear. 
\end{theorem} 
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
Even though quasi-Newton methods do not posses the quadratic convergence of the Newton method, superlinear convergence is still a lot better than the linear one for steepest descent.