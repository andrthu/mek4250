\chapter{Optimal control with ODE constraints}
\section{General optimal control problem}
In this thesis we are only looking at optimal control problems with time dependent DE constraints. This problem is only a part of the more general control problem, which can be formulated as:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y,v) \\
\textit{Subject to:} \ &E(y,v)=0
\end{align}
Here $J: Y\times V\rightarrow\mathbb{R}$ is the objective function that we want to minimize, while $E:Y\times V \rightarrow Z$, is an operator such that $\forall v \in V$, $\exists! y(v)\in Y$ that satisfies the state equation:
\begin{align*}
E(y(v),v)=0
\end{align*}
The first question we need to ask ourself, is under what conditions does the above problem even have solution. This is a complicated question that we will look into when we have defined our example problem. Instead let us assume for now that a solution exists, and try to derive a way to find it. If we replace $y$ with $y(v)$ we can define the reduced problem:
\begin{align}
\underset{v\in V}{\text{min}} \ \hat J(v)=\underset{v\in V}{\text{min}} \ J(y(v),v) \ \textit{Subject to:} \ E(y(v),v)=0 \label{reduced problem}
\end{align}
The problem (\ref{reduced problem}) is called the reduced problem because we in reality have moved the differential equation constraints into the function, and by doing so transformed the constrained problem into an unconstrained one. Since we really just want to find the minimum of a function, it is only natural, that to solve (\ref{reduced problem}), we need to be able to differentiate the objective function $\hat{J}$ with respect to the control $v$. There are different ways of doing that, but I will focus on the so called adjoint approach, which is the most computational effective way of calculating the gradient of $\hat{J}$.
\subsection{The adjoint equation and the gradient}
To find the gradient $\hat{J}'(v)$, lets start by differentiating $\hat J$:
\begin{align*}
\hat{J}'(v) = DJ(y(v),v) = y'(v)^*J_y(y(v),v) + J_v(y(v),v)
\end{align*}
The problematic term in the above expression, is $y'(v)^*$. To calcualte this we need to differentiate the state equation:
\begin{align*}
DE(y(v),v)=0 &\Rightarrow E_y(y(v),v)y'(v)=-E_v(y(v),v) \\ &\Rightarrow y'(v)=-E_y(y(v),v)^{-1}E_v(y(v),v) \\ &\Rightarrow y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}
\end{align*}
Instead of inserting $y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}$ into our gradient expression, we first define the adjoint equation as:
\begin{align}
E_y(y(v),v)^{*}p=J_y(v) \label{general adjoint}
\end{align}
This now allows us to write up the gradient as follows:
\begin{align}
\hat{J}'(v)&= y'(v)^*J_y(y(v),v) + J_v(y(v),v)\\
&=-E_v(y(v),v)^*E_y(y(v),v)^{-*}J_y(y(v),v) + J_v(y(v),v) \\
&= -E_v(y(v),v)^*p +J_v(y(v),v) \label{gradient}
\end{align}
Evaluating the gradient for a control variable $v\in V$ typically requires us to solve both the state and adjoint equation, and then inserting the solutions into the expression for the gradient.
\section{Example problem}
To better understand the adjoint approach to gradient evaluation of the reduced objective function, I will define a simple optimal control problem with ODE constraints, and derive its adjoint equation and gradient. The problem will also be used later to test the implementation. In our example both the state $y$ and the control $v$ will be functions on an interval $[0,T]$. This allows us to define the objective function:
\begin{align}
J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 \label{exs_J}
\end{align}
Our state equation $E(y,v)$ will be a simple linear first order equation with the control as a source term:
\begin{align}
\left\{
     \begin{array}{lr}
       	y'(t)+ay(t) = v(t) \ t\in(0,T)\\
       	y(0)=y_0
     \end{array}
   \right. \label{exs_E}
\end{align}
Before we derive the adjoint and gradient of the reduced version of the above problem, I want to return to the question of existence of solution. To achive this, I will write up a general result from \cite{hinze2008optimization} about linear quadratic problems, which also includes problem (\ref{exs_J}-\ref{exs_E}).
\begin{theorem}
The linear quadratic optimization problems, that can be written on the form:
\begin{align*}
&\underset{y\in Y,v\in V}{\text{min}}J(y,v) = \frac{1}{2}||Qy-q||_H^2 + \frac{\alpha}{2}||v||_V^2 \\
\textit{Subject to:} \ Ay + Bv = g
\end{align*}
Where H,V are Hilbert spaces, Y,Z are Banach spaces, $q\in H$, $g\in Z$ and $A:Y\rightarrow Z$, $B:V\rightarrow Z$ and $Q:Y\rightarrow H$ are bounded linear operators. If $\alpha>0$, the above written problem has a unique solution pair $(y,v)\in Y\times V$.
\end{theorem}   
\begin{proof}
See \cite{hinze2008optimization}
\end{proof}
It is obvious, that the objective function (\ref{exs_J}) of our example problem fits into the setting of the above theorem, and since the equation (\ref{exs_E}) has solution $y(t) = e^{-a t}(C(y_0)+\int_0^te^{a\tau}v(\tau)d\tau)$, and is uniquely solvable for every integrable $v$, it is clear that problem (\ref{exs_J}-\ref{exs_E}) is a linear quadratic optimization problem. Knowing this, we can safely continue by stating and deriving the adjoint and gradient expression for our example problem (\ref{exs_J}-\ref{exs_E}): 
\begin{theorem}
The adjoint equation of the problem (\ref{exs_J}-\ref{exs_E}) is:
\begin{align*}     
-p'(t) &= ap(t) \\
p(T) &= \alpha(y(T)-y^T)     
\end{align*}
\end{theorem}
\begin{proof}
Before we calculate the different terms used to derive the adjoint equation, we want to fit our ODE into an expression $E$. We do this by writing up the weak formulation of the equation:
\begin{gather*}
\textit{Find $y \in L^2$ such that}\\
L[y,\phi] = \int_0^T-y(t)\phi'(t)-ay(t)\phi(t)dt -y_0\phi(0)+y(T)\phi(T)-\int_0^Tv(t)\phi(t)=0\\ \forall \ \phi \in C^{\infty}((0,T))
\end{gather*}
To derive the adjoint we need $E_y$ and $J_y$. For $E_y$ we define $(\cdot,\cdot)$ to be the $L^2$ inner product over $(0,T)$. This gives us:
\begin{align*}
E_y=L_y[\cdot,\phi]=(\cdot,(\frac{\partial}{-\partial t} - a + \delta_T)\phi)  
\end{align*}
Lets be more thorough with $J_y$, which is the right hand side in the adjoint equation.
\begin{align*}
J_y(y(v),v) &= \frac{\partial}{\partial y}(\frac{1}{2}\int_0^Tv^2dt + \frac{\alpha}{2}(y(T)-y^T)^2) \\ &= \frac{\partial}{\partial y} \frac{\alpha}{2}(y(T)-y^T)^2 \\
&= \frac{\partial}{\partial y}\frac{\alpha}{2}(\int_0^T \delta_T(y-y^T)dt)^2 \\
&= \alpha\delta_T\int_0^T \delta_T(y(t)-y^T)dt \\
&= \alpha\delta_T(y(T)-y^T)
\end{align*}
We have $E_y=(\cdot,(-\frac{\partial}{\partial t} - a + \delta_T)\phi)$, but for the adjoint equation we need to find $E_y^*$.
To derive the adjoint of $E_y$, we will insert two functions $v$ and $w$ into $L_y[v,w]$, and try to change the places of $v$ and $w$.
\begin{align*}
E_y&=L_y[v,w]=\int_0^T-v(t)(w'(t)+aw(t))dt + v(T)w(T) \\
&=\int_0^Tw(t)(v'(t)-av(t))dt + v(T)w(T)-v(T)w(T) +v(0)w(0) \\
&=\int_0^Tw(t)(v'(t)-a v(t))dt+v(0)w(0) \\
&=L_y^*[w,v]=E_y^*
\end{align*}
If we multiply $J_y$ with a test function $\psi$ and set $L_y^*[p,\psi]=(J_y,\psi)$, we get the following equation:
\begin{align*}
&\textit{Find $p$ such that}\\
&\int_0^Tp(t)\psi'(t)-a p(t)\psi(t)dt + p(0)\psi(0)= \alpha(y(T)-y^T)\psi(T)\ \forall \ \psi \in C^{\infty}((0,T))
\end{align*}
If we then do partial integration, we get:
\begin{align*}
&\textit{Find $p$ such that}\\
&\int_0^T(-p'(t)-ap(t))\psi(t)dt +p(T)\psi(T)= \alpha(y(T)-y^T)\psi(T)\ \forall \ \psi \in C^{\infty}((0,T))
\end{align*}
Using this we get the strong formulation:
\begin{align*}
   \left\{
     \begin{array}{lr}
       -p'(t) = ap(t) \\
       p(T) = \alpha( y(T)-y^T)
     \end{array}
   \right.
\end{align*}
\end{proof}
With the adjoint we can find the gradient of $\hat{J}$. Lets state the result first.
\begin{theorem}
The gradient of the reduced objective function $\hat{J}$ with respect to v is
\begin{align}
\hat{J}'(v)=v+p 
\end{align} 
\end{theorem}
\begin{proof}
Firstly we need $J_v$ and $E_v^*$:
\begin{align*}
J_v &= v \\
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi)
\end{align*}
Since $L_v[\cdot,\phi]$ is symmetric, $E_v^*=E_v$, and strongly formulated, $E_v=-1$. The expression for the gradient is then simply:
\begin{align*}
\hat{J}'(v)&=-E_v^*p + J_v \\
&= p+v 
\end{align*} 
\end{proof}
\section{Optimization algorithms}
Deriving and solving the adjoint equation gives us a way of evaluating the gradient of optimal control problems with ODE constraints. With the gradient we can now solve our optimal control problems by using some optimization algorithm. There exists many different optimization algorithms, but here I will only mention line search methods that are useful to me in my thesis. These are the steepest descent method, the related BFGS and L-BFGS methods.  
\subsection{Line search methods and steepest descent}
Line search methods are algorithms that help us solve problems of the type: 
\begin{align*}
\min_x f(x), \ f:\mathbb{R}^n\longrightarrow\mathbb{R}
\end{align*}
The methods are iterative and generate a sequence $\{x^k\}$ that hopefully will converge to a solution. This means that an initial guess $x^0$ is required. The k-th iteration in the algorithm can be described in the following way:
\begin{align*}
1. &\textit{Choose direction $p_k$} \\
2. &\textit{Choose step length $\alpha_k$} \\
3. &\textit{Set $x^{k+1}=x^k + \alpha_kp_k$} 
\end{align*}
What separates different line search methods, is how one chooses direction $p_k$ and step length $\alpha_k$. How to choose a good step length is generally a more difficult question than how to choose the search direction. One way of finding a good search length, are the so called Wolfe conditions. These are as follows:
\begin{align*}
f(x^k + \alpha_kp_k)&\leq f(x^k) + c_1\alpha_k\nabla f(x^k)\cdot p_k \\
\nabla f(x^k + \alpha_kp_k) \cdot p_k &\geq c_2 \nabla f(x^k)\cdot p_k
\end{align*}
Here we use constants $0<c_1<c_2<1$. The first Wolfe condition ensures that the decrease in function value is proportional to both step length and direction. The second condition is that the gradient of $f$ at $x^k + \alpha_kp_k$, should be less steep than at $x^k$. If we can find a step length that satisfies these conditions we will use it. How to actually find a step length that satisfies the Wolfe conditions is quite involved, and I will therefore not go into this topic any further. Instead I will mention a couple of line search methods that will be used later in the thesis, starting with steepest descent.
\\
\\
The steepest descent method is a very simple line search method, where the step length $p_k$ is set to the negative gradient direction at point $x^k$, i.e $p_k = -\nabla f(x^k)$. This gives us the following update for each iteration:
\begin{align*}
x^{k+1} = x^k - \alpha_k \nabla J(x^k)
\end{align*} 
The problem with steepest descent is that it converges quite slowly. This fact we get from the following theorem given in \cite{nocedal2006numerical}
\begin{theorem}
Assume that $f:\mathbb{R}^n\longrightarrow\mathbb{R}$ is twice continuously differentiable, that the steepest decent method converge to a point $x^*$, and that the Hessian of $f$ at this point, $\nabla^2 f(x^*)$ is positive definite. Then the following holds:
\begin{align*}
f(x^{k+1})-f(x^*) \leq (\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2 f(x^{k})-f(x^*)
\end{align*}  
Here $\lambda_1\leq\cdots\leq \lambda_n$ denotes the eigenvalues of $\nabla^2 f(x^*)$.
\end{theorem}   
\begin{proof}
Given in \cite{nocedal2006numerical}
\end{proof}
\subsection{BFGS and L-BFGS}
Since steepest descent has slow convergence, one usually uses faster line search methods to solve numerical optimization problems. One alternative is Newtons method, that includes the inverse Hessian in the search direction, i.e:
\begin{align*}
x^{k+1} = x^k + \alpha \nabla^2 f(x^k)^{-1}\nabla f(x^k)
\end{align*}
The problem with this method is that we need to evaluate the inverse Hessian, which often is computationally expensive procedure. An alternative to the newton method is so called quasi-Newton methods, which try to approximate the Hessian, by using information from the previous iterates. One such method is the BFGS method, which approximates the inverse Hessian. This is done using the following recursive formula:
\begin{align*}
H^{k+1} &= (\mathbbold{1}-\rho_kS_k\cdot Y_k)H^k(\mathbbold{1} -\rho_kY_k\cdot S_k) + S_k\cdot S_k \\
S_k &= x^{k+1}-x^{k} \\
Y_k &= \nabla f(x^{k+1})-\nabla f(x^{k})\\
\rho_k &= \frac{1}{Y_k\cdot S_k} \\
H^0&=\mathbbold{1}
\end{align*}
The usual choice for the initial inverted Hessian approximation $H^0$ is simply identity, but depending on the problem, other choices can also work. Each line search iteration for BFGS looks like:
\begin{align*}
x^{k+1} = x^k + \alpha H^{k}\nabla f(x^k)
\end{align*} 
In the BFGS method information from all iterations is used to create the inverse Hessian approximation. An alternative to this is to limit the number of iterations the recursive formula remembers to only the latest iterations. This variation of the BFGS method is called L-BFGS. The length of the memory need to be chosen in advance, and the typical choise is 10. Two advantages L-BFGS has over BFGS is firstly that it is less computationally expensive. Secondly one would think that recent iterations possess more valuable information than early iterations, and therefore should be given more weight. 
