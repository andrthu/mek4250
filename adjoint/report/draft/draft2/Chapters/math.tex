\chapter{Optimal control with ODE constraints} \label{math_chap}
In this chapter we present the basic mathematical background that the rest of the thesis will be based on. The chapter covers three different subjects. The first subject is on general theory of optimal control problems with DE constraints. The second subject is on finite difference discretization of differential equations and numerical integration, and the last subject deals with optimization algorithms. In addition to the general theory, we present an example optimal control problem with ODE constraints, that will be used throughout the rest of the thesis. 
\section{General optimal control problem}
In this thesis we are only looking at optimal control problems with time dependent differential equation(DE) constraints. This problem is only a part of the more general control problem, which can be formulated as:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y,v) \\
\textrm{Subject to:} \ &E(y,v)=0.
\end{align}
Here $J: Y\times V\rightarrow\mathbb{R}$ is the objective function that we want to minimize, and the spaces $Y,V$ are reflexive Banach spaces. The operator $E:Y\times V \rightarrow Z$, where $Z$ is a Banach spaces, is called the state equation when we set it equal to zero. The state equation should have the property that $\forall v \in V$, $\exists! y(v)\in Y$ that satisfies:
\begin{align*}
E(y(v),v)=0.
\end{align*}
The first question we need to ask ourself, is under what conditions does the above problem even have a solution. This depends on the operators $J,E$ and the spaces $Y,V,Z$, and we will therefore come back to this question when we have defined our example problem. Instead let us assume for now that a solution exists, and try to derive a way to find it. If we replace $y$ with $y(v)$ we can define the reduced problem:
\begin{align}
\underset{v\in V}{\text{min}} \ \hat J(v)=\underset{v\in V}{\text{min}} \ J(y(v),v)  \label{reduced problem}
\end{align}
The problem (\ref{reduced problem}) is called the reduced problem because we have moved the differential equation constraints into the functional, and by doing so transformed the constrained problem into an unconstrained one. Since the aim is to find the minimum of a function, it is natural, that to solve (\ref{reduced problem}), we need to be able to differentiate the objective function $\hat{J}$ with respect to the control $v$. There are different ways of doing that, but we will focus on the so called adjoint approach, which is the most computational effective way of calculating the gradient of $\hat{J}$.
\subsection{Example problem} \label{example_sec}
To better understand the adjoint approach to gradient evaluation of the reduced objective function, we will define a simple optimal control problem with ODE constraints, so that we later can derive its adjoint equation and gradient. The problem will also be used to test and verify the implementation in chapter \ref{Verification chapter} and \ref{Experiments chapter}. In our example both the state $y$ and the control $v$ will be functions on an interval $[0,T]$. This allows us to define the objective function:
\begin{align}
J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 \label{exs_J}
\end{align}
The state equation $E(y,v)=0$ is a linear, first order equation with the control as a source term:
\begin{align}
\left\{
     \begin{array}{lr}
       	y'(t)=ay(t) + v(t) \ \textrm{for } \quad t\in(0,T),\\
       	y(0)=y_0.
     \end{array}
   \right. \label{exs_E}
\end{align}
Now that we actually have an optimal control problem, we can return to the question of solvability. To achieve this, we will write up a general result from \cite{hinze2008optimization} about linear quadratic problems, which also includes problem (\ref{exs_J}-\ref{exs_E}). Linear quadratic problems are a quite simple class of problems, and more general theorems for problems with non-linear equations and non-quadratic objective functions do however exist, and can be found in \cite{hinze2008optimization}.
\begin{theorem}
The linear quadratic optimization problems, that can be written on the form:
\begin{align*}
&\underset{y\in Y,v\in V}{\text{min}}J(y,v) = \frac{1}{2}||Qy-q||_H^2 + \frac{\alpha}{2}||v||_V^2 \\
&\textit{Subject to:} \ Ay + Bv = g
\end{align*}
Where H,V are Hilbert spaces, Y,Z are Banach spaces, $q\in H$, $g\in Z$ and $A:Y\rightarrow Z$, $B:V\rightarrow Z$ and $Q:Y\rightarrow H$ are bounded linear operators. If $\alpha>0$, the above written problem has a unique solution pair $(y,v)\in Y\times V$.
\end{theorem}   
\begin{proof}
See \cite{hinze2008optimization}
\end{proof}
\noindent
It is obvious, that the objective function (\ref{exs_J}) of our example problem is a quadratic function that fits into the setting of the above theorem, and since the equation (\ref{exs_E}) is linear, with solution $y(t) = e^{a t}(C(y_0)+\int_0^te^{-a\tau}v(\tau)d\tau)$, which is unique for every integrable $v$, it is clear that problem (\ref{exs_J}-\ref{exs_E}) is a linear quadratic optimization problem. 
\section{The adjoint equation and the gradient}
The usual way of finding the minimum(or maximum) value of a function $J$, is to solve the equation $J'(x)=0$. Solving this equation usually requires us to be able to evaluate, or have an expression for the derivative of $J$. With this in mind let us try to find the gradient $\hat{J}'(v)$ of the reduced problem (\ref{reduced problem}).
\begin{align*}
\hat{J}'(v) = DJ(y(v),v) = y'(v)^*J_y(y(v),v) + J_v(y(v),v)
\end{align*}
The problematic term in the above expression, is $y'(v)^*$, since the function $y(v)$ is implicitly defined through $E$. We can however find an equation for $y'(v)^*$ if we differentiate the state equation with respect to $v$, and assume that the operator $E_y(y(v),v):Y\rightarrow Z$ is continuously invertible.
\begin{align*}
DE(y(v),v)=0 &\Rightarrow E_y(y(v),v)y'(v)=-E_v(y(v),v) \\ &\Rightarrow y'(v)=-E_y(y(v),v)^{-1}E_v(y(v),v) \\ &\Rightarrow y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}.
\end{align*}
Before inserting $y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}$ into our gradient expression, we first define the adjoint equation as:
\begin{align}
E_y(y(v),v)^{*}p=J_y(v). \label{general adjoint}
\end{align}
This now allows us to write up the gradient as follows:
\begin{align}
\hat{J}'(v)&= y'(v)^*J_y(y(v),v) + J_v(y(v),v)\\
&=-E_v(y(v),v)^*E_y(y(v),v)^{-*}J_y(y(v),v) + J_v(y(v),v) \\
&= -E_v(y(v),v)^*p +J_v(y(v),v). \label{gradient}
\end{align}
Evaluating the gradient for a control variable $v\in V$ typically requires solving both the state and adjoint equation, and then inserting the solutions into the expression for the gradient (\ref{gradient}). To better illustrate how gradient evaluation works let us derive the adjoint equation and the gradient of the problem introduced in section \ref{example_sec}.
\subsection{Adjoint of the example problem}
We want to derive the gradient of problem (\ref{exs_J}-\ref{exs_E}), and in order to do so, we need the adjoint equation of the problem, which we now state in the theorem below, followed by its derivation.
\begin{theorem}
The adjoint equation of the problem (\ref{exs_J}-\ref{exs_E}) is:
\begin{align}     
-p'(t) &= ap(t) \label{exs_adjoint_eq}\\
p(T) &= \alpha(y(T)-y^T)   \label{exs_adjoint_ic}  
\end{align}
\end{theorem}
\begin{proof}
Before we calculate the different terms used to derive the adjoint equation, we want to fit our ODE into an expression $E$. We do this by writing up the weak formulation of the equation:
\begin{gather*}
\textrm{$y \in L^2(0,T)$ such that}\\
L[y,\phi] = \int_0^T-y(t)\phi'(t)-ay(t)\phi(t)dt -y_0\phi(0)+y(T)\phi(T)-\int_0^Tv(t)\phi(t)=0\\ \forall \ \phi \in C^{\infty}((0,T))
\end{gather*}
To derive the adjoint we need $E_y$ and $J_y$. For $E_y$ we define $(\cdot,\cdot)$ to be the $L^2$ inner product over $(0,T)$. Since the weak formulation includes evaluation at $t=0$ and $t=T$, we define an operator $\delta_{\tau}$ that represent function evaluation in an $L^2$ inner product setting. We do this in the following way: Let $\tau \in [0,T]$ then:
\begin{align*}
(v,\delta_{\tau}w) =(\delta_{\tau}v,w) = \int_0^Tv(t)\delta_{\tau}w(t)dt = v(\tau)w(\tau)
\end{align*}
Using the above notation, we can write $E_y$ quite compactly as:
\begin{align*}
E_y=L_y[\cdot,\phi]=(\cdot,(-\frac{\partial}{\partial t} - a + \delta_T)\phi)  
\end{align*}
Let us be more thorough with $J_y$, which is the right hand side of the adjoint equation.
\begin{align*}
J_y(y(v),v) &= \frac{\partial}{\partial y}(\frac{1}{2}\int_0^Tv^2dt + \frac{\alpha}{2}(y(T)-y^T)^2) \\ &= \frac{\partial}{\partial y} \frac{\alpha}{2}(y(T)-y^T)^2 \\
&= \frac{\partial}{\partial y}\frac{\alpha}{2}(\int_0^T \delta_T(y-y^T)dt)^2 \\
&= \alpha\delta_T\int_0^T \delta_T(y(t)-y^T)dt \\
&= \alpha\delta_T(y(T)-y^T)
\end{align*}
We have $E_y=(\cdot,(-\frac{\partial}{\partial t} - a + \delta_T)\phi)$, but we want to find its adjoint $E_y^*$. Therefore let us explain what is ment by an adjoint on the context of the $L^2$ inner product $(\cdot,\cdot)$. Let $B:L^2(0,T)\rightarrow L^2(0,T)$ be a linear operator. Then the adjoint of $B$, $B^*$ is an operator on $ L^2(0,T)$, such that $\forall v,w\in L^2(0,T)$:
\begin{align*}
(Bv,w)=(v,B^*w).
\end{align*}
The adjoint of the bilinear form $E_y=L_y$, would therefore be a bilinear form $L_y^*=E_y^*$ such that $\forall v,w\in L^2(0,T)$:
\begin{align*}
L_y[v,w]=L_y^*[w,v].
\end{align*}
Therefore to derive the adjoint of $E_y$, we will insert two functions $v$ and $w$ into $L_y[v,w]$, and try to change the places of $v$ and $w$.
\begin{align*}
E_y&=L_y[v,w]=\int_0^T-v(t)(w'(t)+aw(t))dt + v(T)w(T) \\
&=\int_0^Tw(t)(v'(t)-av(t))dt + v(T)w(T)-v(T)w(T) +v(0)w(0) \\
&=\int_0^Tw(t)(v'(t)-a v(t))dt+v(0)w(0) \\
&=L_y^*[w,v]=E_y^*
\end{align*}
If we multiply $J_y$ with a test function $\psi\in C^{\infty}((0,T))$ and set $L_y^*[p,\psi]=(J_y,\psi)$, we get the following equation: Find $p$ such that:
\begin{align*}
&\int_0^Tp(t)\psi'(t)-a p(t)\psi(t)dt + p(0)\psi(0)= \alpha(y(T)-y^T)\psi(T)\ \quad\forall \ \psi \in C^{\infty}((0,T))
\end{align*}
If we then do partial integration, the equation reads: Find $p$ such that:
\begin{align*}
&\int_0^T(-p'(t)-ap(t))\psi(t)dt +p(T)\psi(T)= \alpha(y(T)-y^T)\psi(T)\ \quad\forall \ \psi \in C^{\infty}((0,T))
\end{align*}
Using this we get the strong formulation:
\begin{align*}
   \left\{
     \begin{array}{lr}
       -p'(t) = ap(t) \\
       p(T) = \alpha( y(T)-y^T)
     \end{array}
   \right.
\end{align*}
\end{proof}
\noindent
With the adjoint we can find the gradient of $\hat{J}$. Lets state the result first.
\begin{theorem}
The gradient of the reduced objective function $\hat{J}$ with respect to v is
\begin{align}
\hat{J}'(v)=v+p. \label{exsample_grad}
\end{align} 
\end{theorem}
\begin{proof}
Firstly we need $J_v$ and $E_v^*$:
\begin{align*}
J_v &= v \\
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi).
\end{align*}
Since $L_v[\cdot,\phi]$ is symmetric, $E_v^*=E_v$, and strongly formulated, $E_v=-1$. By inserting relevant terms into (\ref{gradient}), we get the gradient:
\begin{align}
\hat{J}'(v)&=-E_v^*p + J_v \\
&= p+v. \label{exs_grad}
\end{align} 
\end{proof}
\noindent
Evaluating the gradient of our example problem can now be boiled down to the following three steps:
\begin{align*}
1.\quad & \textrm{Solve the state equation for $y$.}\\
2.\quad & \textrm{Use $y$ to solve the adjoint equation for $p$.}\\
3.\quad & \textrm{Insert $p$ and control $v$ into gradient formula (\ref{exsample_grad}).}
\end{align*}
To see why the above procedure is computationally effective, let us compare it with the finite difference approach to evaluating the gradient. Using finite difference we can find an approximation of the directional derivative $(\hat J'(v),h)_V$ in direction $h\in V$, by choosing a small $\epsilon>0$ and setting:
\begin{align}
(\hat J'(v), h)_V\approx\frac{\hat J(v+\epsilon h)-\hat J(v)}{\epsilon} \label{FD_approach}
\end{align} 
To calculate the above expression, we need to evaluate the objective function at $v+\epsilon h$ and $v$. Since objective function evaluation requires the solution of the state equation, finding the directional derivative of $\hat J$ in a direction $h$ involves solving two ODEs. We are however interested in the gradient of $\hat J$, not its directional derivatives. To find $\hat J'(v)$ we calculate (\ref{FD_approach}) for all unit vectors in $V$. This assumes that $V$ is a finite space, which is always true in the discrete case. If we now look at the discrete case and assume that $V=\mathbb{R}^n$ we can write up a recipe for finding $\hat J'(v)$ using finite difference. Let $e_i$ denote the i-th unit vector of $\mathbb{R}^n$. $\hat J'(v)$ can then be found in the following way:
\begin{align*}
1.\quad & \textrm{Evaluate $\hat J(v)$.}\\
2.\quad & \textrm{Evaluate $\hat J(v+\epsilon e_i)$ for $i=1,...,n$. }\\
3.\quad & \textrm{Set the i-th component of $\hat J'(v)$ to be $\frac{\hat J(v+\epsilon e_i)-\hat J(v)}{\epsilon}$.}
\end{align*}
To execute the above steps, we need to solve the state equation for $n+1$ different control variables. In comparison finding $\hat J'(v)$ using the adjoint approach only requires us to solve the state and adjoint equations once, independently of the dimension of $V$. For finite difference the computational cost of one gradient evaluation therfore depends linearly on the number of components in the control variable $v$, while the computational cost of the adjoint approach is independent of the size of $v$.
\subsection{Exact solution of the example problem} \label{exact_sec}
It turns out that we can find the exact solution of problem (\ref{exs_J}-\ref{exs_J}) by utilizing the adjoint equation (\ref{exs_adjoint_eq}-\ref{exs_adjoint_ic}) and the gradient of the reduced objective function (\ref{exs_grad}). Finding an exact solution to our example problem will be useful for us in chapter \ref{Verification chapter}, where we will be testing and verifying different aspects of our algorithm. The derivation of the solution is based on two key observations. The first observation is a relation between the optimal control $\bar v$ and the adjoint $p$, which is a result from the trivial fact that $\hat J'(\bar v)=0$ is a necessary condition for $\bar v$ being a minimizer of $\hat J$. Inserting expression (\ref{exs_grad}) into $\hat J'(\bar v)=0$ yields:
\begin{align}
\bar v(t)=-p(t). \label{obs1}
\end{align} 
The second observation concerns the solution of the adjoint equation (\ref{exs_adjoint_eq}-\ref{exs_adjoint_ic}). Given a state $y(t)$, the solution of the adjoint equation is:
\begin{align}
p(t) = \alpha(y(T)-y^T)e^{a(T-t)} = \omega e^{-at}. \label{obs2}
\end{align}
Combining observation (\ref{obs1}) with observation (\ref{obs2}) suggests that a minimizer $\bar v$ of $\hat J$ should be on the form:
\begin{align}
\bar v(t) = C_0 e^{-at}. \label{v_ansatz}
\end{align}
It turns out that plugging anstatz (\ref{v_ansatz}) into the state equation, and then using the resulting state to solve the adjoint equation makes us able to find the solution of our example problem. The solution is stated in proposition \ref{exact_prop} followed by its derivation.
\begin{proposition} \label{exact_prop}
Assume $a\neq0$ and $\alpha>0$. Then the solution of optimal control problem(\ref{exs_J}-\ref{exs_J}) is the following function:
\begin{align}
\bar v(t) = \alpha\frac{e^{aT}(y^T-e^{aT}y_0)}{1+\frac{\alpha e^{aT}}{2a}(e^{aT}-e^{-aT})}e^{-at}
\end{align}
\end{proposition}
\begin{proof}
We start the proof by writing up the state equation (\ref{exs_E}) with (\ref{v_ansatz}) as source term:  
\begin{align*}
\left\{
     \begin{array}{lr}
       	y'(t)=ay(t) + C_0 e^{-at}\quad \textrm{for } \ t\in(0,T),\\
       	y(0)=y_0.
     \end{array}
   \right. 
\end{align*}
This is a first order linear ODE with solution:
\begin{align}
y(t) = y_0e^{at} +\frac{C_0}{2a}(e^{at}-e^{-at}) \label{C_state}
\end{align}
If we insert the state (\ref{C_state}) into the formula for the adjoint (\ref{obs2}), we can express the adjoint $p(t)$ in terms of the constant $C_0$:
\begin{align}
p(t) &=  \alpha(y(T)-y^T)e^{a(T-t)} \\
&= \alpha e^{aT}(y_0e^{aT} +\frac{C_0}{2a}(e^{aT}-e^{-aT})-y^T)e^{-at} \label{C_adjoint}
\end{align}
The last step is to plug $v(t) =C_0 e^{-at}$ and $p(t)$ from (\ref{C_adjoint}) into observation (\ref{obs1}) and then solve for $C_0$:
\begin{align*}
v(t)=-p(t) &\iff C_0 e^{-at} = -\alpha e^{aT}(y_0e^{aT} +\frac{C_0}{2a}(e^{aT}-e^{-aT})-y^T)e^{-at}\\
&\iff C_0(1+\frac{\alpha e^{aT}}{2a}(e^{aT}-e^{-aT})) = \alpha e^{aT}(y^T-y_0e^{aT}) \\
&\iff C_0 = \alpha\frac{e^{aT}(y^T-e^{aT}y_0)}{1+\frac{\alpha e^{aT}}{2a}(e^{aT}-e^{-aT})}
\end{align*}
Division by $(1+\frac{\alpha e^{aT}}{2a}(e^{aT}-e^{-aT}))$ is always allowed, since $\frac{1}{a}(e^{aT}-e^{-aT})>0, \forall a\neq0$ and $\forall T>0$. 
\end{proof} 
\section{Finite difference}
To be able to solve optimal control problems numerically, we need to discretize the objective function and the state and adjoint equations. We are mainly interested in time dependent equations, and the standard way of discretizing ODEs or PDEs in temporal direction, is to use a finite difference method. Since the objective function includes an integral term, we also need methods for numerical integration. In this section we will only look at first order equations on the following form:
\begin{align}
\left\{
     \begin{array}{lr}
	\frac{\partial}{\partial t} y(t) = F(y(t),t),\quad t\in I \\
	y(0)=y_0 
	\end{array}
\right.\label{general_1st_ODE}
\end{align}
Both the state and adjoint equation of our example problem can be formulated as an equation on form (\ref{general_1st_ODE}), and understanding the numerics of (\ref{general_1st_ODE}) is therefore sufficient for the purposes of this thesis. Before we introduce numerical methods for solving ODEs and evaluating integrals, we need to explain how we discritize the time domain $I=[0,T]$. We do this by dividing $I$ into $n$ parts of length $\Delta t=\frac{T}{n}$, and then setting $t_k=k\Delta t$. This gives us a sequence $I_{\Delta t}=\{t_k\}_{k=0}^{n}$ as a discrete representation of the interval $I$. Numerically solving a differential equation for $y$ on $I_{\Delta t}$ means that we try to find $y(t_k)$ for $k=0,...,n+1$. For the rest of this section we let the notation $y_k$ denote evaluating the function $y$ at time $t_k$.
\subsection{Discretizing ODEs using finite difference} \label{FD_sub_sec}
Finite difference is a tool for approximating derivatives of functions. When we have a discretized domain $I_{\Delta t}$ with time step $\Delta t$, the derivative of a function $y$ at point $t_k$ is approximated by:
\begin{align}
\frac{\partial}{\partial t} y(t_k) \approx \frac{y_k-y_{k-1}}{\Delta t}. \label{FD_diff_approx}
\end{align}
By exploiting approximation (\ref{FD_diff_approx}) we can create methods for solving ODEs. This is done by relating $y_k$ to neighbouring values $y_j$, $j\neq k$ through the ODE. The most simplistic examples of such finite difference methods are the explicit and implicit Euler methods. We write up these methods applied to (\ref{general_1st_ODE}) in definition \ref{Euler_def} below.
\begin{definition} \label{Euler_def}
Explicit Euler applied to equation (\ref{general_1st_ODE}) means that for $k=1,...,n$ the value of $y_k$ is determined by the following formula:
\begin{align}
y_k = y_{k-1} +\Delta tF(y_{k-1},t_{k-1}).\label{EE_formula}
\end{align} 
If one instead uses implicit Euler the expression for $y_k$ is:
\begin{align}
y_k = y_{k-1} +\Delta tF(y_{k},t_{k}). \label{IE_formula}
\end{align}
\end{definition}
\noindent
By looking at expression (\ref{EE_formula}) and (\ref{IE_formula}) we see the origin of the names of the Euler methods. In the formula for implicit Euler, $y_k$ appears on both sides of the equal sign, and is therefore implicitly defined. For the explicit Euler scheme $y_k$ only appears on the left-hand side of expression (\ref{EE_formula}), which means $y_k$ is defined explicitly, and hence the name explicit Euler. Another thing to notice about the finite difference schemes in definition \ref{Euler_def}, is that they solve the equation forwardly. This means that given $y$ at time $t_K$, we can use (\ref{EE_formula}) and (\ref{IE_formula}) to find $y_j$ for $j>K$. The adjoint equation of optimal control problem with time dependent DE constraints is however solved backwards in time. We therefore need finite difference schemes for solving ODEs backwards. This is easily achieved by rearranging expression (\ref{EE_formula}) and (\ref{IE_formula}) in definition \ref{Euler_def}. A backwards solving explicit Euler scheme is found by adjusting the forward solving implicit Euler scheme, while a backwards implicit Euler method is derived by rearranging the forward explicit Euler formula. These modified backwards solving schemes are written up in definition \ref{Euler_adjoint_def}. 
\begin{definition} \label{Euler_adjoint_def}
An explicit Euler finite difference scheme for equation (\ref{general_1st_ODE}) with initial condition at $t=T$ instead of $t=0$ yields the following formula for $y_k$:
\begin{align}
y_k = y_{k+1} -\Delta tF(y_{k-1},t_{k-1}).\label{EE_adjoint_formula}
\end{align} 
If one instead uses implicit Euler the expression for $y_k$ is:
\begin{align}
y_k = y_{k+1} -\Delta tF(y_{k},t_{k}). \label{IE_adjoint_formula}
\end{align}
\end{definition}
\noindent
We say that both the explicit and implicit Euler methods have an accuracy of order one. To explain what we mean by this, let us assume that we know that the function $\hat y$ solves equation (\ref{general_1st_ODE}) for a given $F$, and that $\hat y$ is sufficiently smooth. If we then use method (\ref{EE_formula}) or (\ref{IE_formula}) with some $\Delta t$ to solve (\ref{general_1st_ODE}) numerically, there exists a constant $C$ such that the  following error bound between $\hat{y}$ and numerical solution $y$ holds:
\begin{align}
\max_{k=0,...,n}|y_k-\hat y(t_k)|\leq C\Delta t \label{E_error_bound}
\end{align}
A more accurate but still simple alternative to the Explicit and implicit Euler finite difference methods, is the so called Crank-Nicolson method\cite{crank1947practical}. We write up this method in a definition:
\begin{definition}
The Crank-Nicolson finite difference scheme applied to equation (\ref{general_1st_ODE}) produces the following formula for $y_k$:
\begin{align}
y_k = y_{k-1} +\frac{\Delta t}{2}(F(y_{k},t_{k})+F(y_{k-1},t_{k-1})). \label{CN_formula}
\end{align}
In a setting where we are solving (\ref{general_1st_ODE}) backwards in time, the expression for $y_k$ is changed to: 
\begin{align}
y_k = y_{k+1} -\frac{\Delta t}{2}(F(y_{k},t_{k})+F(y_{k-1},t_{k-1})). \label{CN_adjoint_formula}
\end{align}
\end{definition}
\noindent
When comparing (\ref{CN_formula}) with (\ref{EE_formula}) and (\ref{IE_formula}) we notice that the formula for $y_k$ in the Crank-Nicolson method is simply the average between the formulas for $y_k$ in the explicit and implicit Euler methods. We improve the accuracy by one order if we use Crank-Nicolson instead of the Euler methods. This means that the bound stated in (\ref{E_error_bound}) is improved to:
\begin{align}
\max_{k=0,...,n}|y_k-\hat y(t_k)|\leq C\Delta t^2 \label{CN_error_bound}
\end{align}
Other more accurate finite difference schemes exist, but in this thesis we restrict the usage of finite difference methods to the ones presented in this section. 
\subsection{Numerical integration} \label{num_int_sub_sec}
In this subsection we present three simple methods for numerical integration. We need such methods since the objective function in our example problem (\ref{exs_J}) includes an integral. The methods that we present in definition \ref{num_int_def} are called the left-hand rectangle rule, the right-hand rectangle rule and the trapezoid rule. Their names stem from the geometrical objects used to estimate the area under the function we want to integrate.
\begin{definition} \label{num_int_def}
We want to estimate the integral $S=\int_0^T v(t) dt$ numerically with a discretized time domain $I_{\Delta t}=\{t_k\}_{k=0}^{n}$. The left-hand rectangle rule approximates $S$ using the following formula:
\begin{align}
S_l = \Delta t\sum_{k=0}^{n-1} v_k \label{LH_INT}
\end{align}
A slightly different approach to estimating $S$ is the right-hand rectangle rule, defined by a formula similar to (\ref{LH_INT}): 
\begin{align}
S_r = \Delta t\sum_{k=1}^{n} v_k \label{RH_INT}
\end{align}
A third way of approximating $S$ is the trapezoid rule:
\begin{align}
S_{trap} = \Delta t\frac{v_0+v_n}{2}+\Delta t\sum_{k=1}^{n-1} v_k \label{TRAP_INT}
\end{align}
\end{definition}
\noindent
The rectangle methods in definition \ref{num_int_def} are of accuracy order one, while the trapezoid rule is of second order. It turns out that the above presented numerical methods are analogue to the three finite difference schemes stated in section \ref{FD_sub_sec}. The left- and right-hand rectangle methods are related to the explicit and implicit Euler schemes, while the trapezoid rule is connected with Crank-Nicolson. When making numerical solvers for optimal control problems it therefore makes sense to discretize the differential equation and integral evaluation using analogue methods. 
\section{Optimization algorithms}
Deriving and solving the adjoint equation gives us a way of evaluating the gradient of optimal control problems with ODE constraints. With the gradient we can solve our optimal control problems numerically by using an optimization algorithm. There exists many different optimization algorithms, but here we will only look at line search methods that are useful to us in this thesis. The methods we present are the steepest descent method and the related BFGS and L-BFGS methods.  
\subsection{Line search methods and steepest descent}
Line search methods are algorithms used to solve problems of the type: 
\begin{align*}
\min_x f(x), \ f:\mathbb{R}^n\longrightarrow\mathbb{R}
\end{align*}
All line search methods are iterative methods that starts at an initial guess $x^0$ and generate a sequence $\{x^k\}$ that hopefully will converge to a solution. The k-th iteration in the algorithm can be described in the following way:
\begin{align*}
1. &\textit{Choose direction $p_k\in\mathbb{R}^n$} \\
2. &\textit{Choose step length $\alpha_k\in\mathbb{R}$} \\
3. &\textit{Set $x^{k+1}=x^k + \alpha_kp_k$} 
\end{align*}
If $f$ is differentiable, a necessary condition for a point $x^*\in\mathbb{R}^n$ to be a minimizer of $f$, is that $\nabla f(x^*)=0$. This optimality condition is used to create a stopping criteria for line search methods in the following way: Given a tolerance $\tau>0$ and a norm $||\cdot||$ stop the line search iteration when
\begin{align}
||\nabla f(x^k)||<\tau \label{opti_con}
\end{align}  
What separates different line search methods, is how one chooses descent direction $p_k$ and step length $\alpha_k$. Let us start with how to choose a good step length. There are several ways of doing this, but for our purposes the so called Wolfe conditions will suffice. The Wolfe conditions consists of two conditions on $f$, presented below:
\begin{align*}
f(x^k + \alpha_kp_k)&\leq f(x^k) + c_1\alpha_k\nabla f(x^k)\cdot p_k \\
\nabla f(x^k + \alpha_kp_k) \cdot p_k &\geq c_2 \nabla f(x^k)\cdot p_k
\end{align*}
Here we use constants $0<c_1<c_2<1$. The first Wolfe condition ensures that the decrease in function value is proportional to both step length and direction. The second condition is that the gradient of $f$ at $x^k + \alpha_kp_k$, should be less steep than at $x^k$, and therefore closer to fulfilling the optimality condition (\ref{opti_con}). If we can find a step length that satisfies these conditions we will use it. How to actually find a step length that satisfies the Wolfe conditions is quite involved, and we will therefore not go into this topic any further. Instead let us look into a couple of line search methods that will be used later in the thesis, starting with steepest descent.
\\
\\
The steepest descent method is a very simple line search method, where the step length $p_k$ is set to the negative gradient direction at point $x^k$, i.e $p_k = -\nabla f(x^k)$. This gives us the following update for each iteration:
\begin{align}
x^{k+1} = x^k - \alpha_k \nabla f(x^k) \label{SD_itr}
\end{align} 
The problem with steepest descent is that it converges quite slowly. To understand why let us first write up a definition that characterizes convergence rates.
\begin{definition} \label{convergence_def}
We say that a sequence $\{x^k\}$ converges linearly to a limit $L$, if there exists $\epsilon\in(0,1)$ such that
\begin{align*}
\lim_{k\rightarrow\infty}\frac{||x^{k+1}-L||}{||x^k-L||} =\epsilon.
\end{align*} 
If $\epsilon=0$ we say that $\{x^k\}$ converges superlinearly to $L$, while $\epsilon=1$ is characterized as sublinear convergence. Lastly we say that $\{x^k\}$ converges quadratically towards $L$, if
\begin{align*}
\lim_{k\rightarrow\infty}\frac{||x^{k+1}-L||}{||x^k-L||^2} =\epsilon.
\end{align*}
\end{definition}
\noindent
With definition \ref{convergence_def} in mind let us state a theorem from \cite{nocedal2006numerical} that specifies the convergence rate of the steepest descent method.
\begin{theorem}
\label{SD_con}
Assume that $f:\mathbb{R}^n\longrightarrow\mathbb{R}$ is twice continuously differentiable, that the steepest decent method converge to a point $x^*$, and that the Hessian of $f$ at this point, $\nabla^2 f(x^*)$ is positive definite. Then the following holds:
\begin{align*}
f(x^{k+1})-f(x^*) \leq (\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2 (f(x^{k})-f(x^*))
\end{align*}  
Here $\lambda_1\leq\cdots\leq \lambda_n$ denotes the eigenvalues of $\nabla^2 f(x^*)$.
\end{theorem}   
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\noindent
The bound for $f(x^{k+1})-f(x^*)$ given in theorem \ref{SD_con} corresponds to a linear convergence with $\epsilon=(\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2$. For badly conditioned Hessians $\nabla^2 f(x^*)$, meaning $\lambda_n>>\lambda_1$, $\epsilon$ will approach one, and the convergence rate becomes almost sublinear. In general the linear convergence rate of steepest descent is considered poor, and we need improved algorithms to get faster convergence.
\subsection{BFGS and L-BFGS}
Since steepest descent has slow convergence, one usually uses faster line search methods to solve numerical optimization problems. One alternative is Newtons method. In Newtons method the search direction $p_k$ is found by multiplying the inverse Hessian with the the negative gradient at $x^k$. This results in the following iteration:
\begin{align}
x^{k+1} = x^k - \nabla^2 f(x^k)^{-1}\nabla f(x^k) \label{Newton}
\end{align}
As we will see in theorem \ref{Newton_con}, the convergence of the Newton method relies on quite strict conditions on the Hessian $\nabla^2 f(x^k)$, which are not always satisfied. An alternative to the Newton method is so called quasi-Newton methods. Instead of applying $\nabla^2 f(x^k)^{-1}$ to the negative gradient direction, such methods apply approximations of the inverse Hessian to $-\nabla f(x^k)$. The approximate Hessians are constructed for each $x^k$, using information from previous iterates. One well known quasi-Newton method is the BFGS method\cite{broyden1970convergence, fletcher1970new, goldfarb1970family, shanno1970conditioning}. In BFGS the inverse Hessian approximation is calculated by the following recursive formula:
\begin{align}
H^{k+1} &= (\mathbbold{1}-\rho_kS_k\cdot Y_k)H^k(\mathbbold{1} -\rho_kY_k\cdot S_k) + S_k\cdot S_k, \label{inv_H_apr} \\
S_k &= x^{k+1}-x^{k}, \\
Y_k &= \nabla f(x^{k+1})-\nabla f(x^{k}),\\
\rho_k &= \frac{1}{Y_k\cdot S_k}, \\
H^0&=\beta\mathbbold{1}.
\end{align}
The above formula is designed in such a way, that $H^k$ is symmetric positive definite. This gives us a requirement for the initial inverted Hessian approximation $H^0$, namely that it also needs to be symmetric positive definite. The usual choice however, is just identity or a multiple $\beta$ of the identity, where the multiple reflects the scaling of the variables. Strategies of how to chose a scaling factor $\beta$ is detailed in \cite{liu1989limited} and  \cite{gilbert1989some}. Each line search iteration for BFGS looks like:
\begin{align}
x^{k+1} = x^k - \alpha H^{k}\nabla f(x^k) \label{BFGS_itr}
\end{align} 
In the BFGS method information from all previous iterations is used to create the inverse Hessian approximation for the new iteration. An alternative to this is to limit the number of iterations the recursive formula remembers to only the latest iterations. This variation of the BFGS method is called L-BFGS\cite{nocedal1980updating}. The length of the memory need to be chosen in advance, and the typical choice is 10. Two advantages L-BFGS has over BFGS is firstly that it requires less memory storage than BFGS. The second advantage is that limiting the memory of the inverse Hessian approximation accelerates the convergence of BFGS. This is demonstrated in \cite{liu1989limited} for several different optimization problems. The reason for the improved convergence, is that more recent iterates possess more relevant information for the current Hessian, and by emphasizing the more relevant information, we improve the approximation of the Hessian.
\subsubsection{Convergence results for Newton and quasi-Newton methods}
Both Newton and quasi-Newton methods converge faster than the steepest descent method. To show this we will include a couple of theorems from \cite{nocedal2006numerical}
concerning this topic. We start with a result on the convergence rate of Newtons method.
\begin{theorem}
\label{Newton_con}
Suppose $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is twice continuously differentiable, and that the Hessian $\nabla^2 f(x)$ is Lipschitz continuous in the neighbourhood of a solution $x^*$ that satisfies $\nabla f(x^*)=0$ and that $\nabla^2 f(x^*)$ is positive definite. Then the following holds for the Newton iteration \ref{Newton}:
\begin{align*}
1.\quad&\textrm{If $x^0$ is suffichently close to $x^*$, the sequence of iterates converge to $x^*$.}\\
2.\quad&\textrm{The rate of convergence of $\{x^k\}$ is quadratic}\\
3.\quad&\textrm{The sequence of gradient norms $ \{||\nabla f(x^k)||\}$ converges towards zero quadratically}
\end{align*}
\end{theorem}
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\noindent
The quadratic convergence of the Newton iteration is a big improvement in comparison with steepest descent, however theorem \ref{Newton_con} also highlights one of the problems with the method. Since we need to invert $\nabla^2 f(x^k)$ to find the search direction at $x^k$, we need an initial $x^0$ sufficiently close to the actual solution for the iteration to even work. This problem does not arise in BFGS and L-BFGS, since the Hessian approximation is designed to be invertible. Unfortunately though, these quasi-Newton methods does not have the convergence properties of Newtons method, as the next result shows.
\begin{theorem}
Assume $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is three times differentiable. Consider then the quasi-Newton iteration $x^{k+1}=x^k-\alpha_k B_k^{-1}\nabla f(x^k)$, where $B_k$ is an approximation of the Hessian along the search direction $p_k=-B_k^{-1}\nabla f(x^k)$, satisfying the condition:
\begin{align*}
\lim_{K\rightarrow\infty}\frac{||(B_k-\nabla^2f(x^k))p_k||}{||p_k||}=0
\end{align*}
If the sequence $\{x^k\}$ originating from the quasi-Newton iteration converges to a point $x^*$, where $\nabla f(x^*)=0$ and $\nabla^2 f(x^*)$ is positive definite, the convergence is superlinear. 
\end{theorem} 
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\noindent
Even though quasi-Newton methods do not posses the quadratic convergence of the Newton method, superlinear convergence is still a lot better than the linear convergence of steepest descent.