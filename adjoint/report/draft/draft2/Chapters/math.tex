\chapter{Optimal control with ODE constraints}
\section{General optimal control problem}
In this thesis we are only looking at optimal control problems with ODE constraints. This problem is only a part of the more general control problem, which can be formulated as:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y,v) \\
\textit{Subject to:} \ &E(y,v)=0
\end{align}
Here $J: Y\times V\rightarrow\Re$ is the objective function that we want to minimize, while $E:Y\times V \rightarrow Z$, is an operator such that $\forall v \in V$, $\exists! y(v)\in Y$ that satisfies the state equation:
\begin{align*}
E(y(v),v)=0
\end{align*}
Replacing $y$ with $y(v)$ allows us to define the reduced problem:
\begin{align}
\underset{v\in V}{\text{min}} \ \hat J(v)=\underset{v\in V}{\text{min}} \ J(y(v),v) \ \textit{Subject to:} \ E(y(v),v)=0 \label{reduced problem}
\end{align}
To find a solution to (\ref{reduced problem}), we need to be able to differentiate the objective function $\hat{J}$ with respect to the control $v$. There are different ways of doing that, but I will focus on the so called adjoint approach, which is the most computational effective way of calculating the gradient of $\hat{J}$.
\subsection{The adjoint equation and the gradient}
To find the gradient $\hat{J}'(v)$, lets start by differentiating $\hat J$:
\begin{align*}
\hat{J}'(v) = DJ(y(v),v) = y'(v)^*J_y(y(v),v) + J_v(y(v),v)
\end{align*}
The problematic term in the above expression, is $y'(v)^*$. To calcualte this we need to differentiate the state equation:
\begin{align*}
DE(y(v),v)=0 &\Rightarrow E_y(y(v),v)y'(v)=-E_v(y(v),v) \\ &\Rightarrow y'(v)=-E_y(y(v),v)^{-1}E_v(y(v),v) \\ &\Rightarrow y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}
\end{align*}
Instead of inserting $y'(v)^* = -E_v(y(v),v)^*E_y(y(v),v)^{-*}$ into our gradient expression, we first define the adjoint equation as:
\begin{align}
E_y(y(v),v)^{*}p=J_y(v) \label{general adjoint}
\end{align}
This now allows us to write up the gradient as follows:
\begin{align}
\hat{J}'(v)&= y'(v)^*J_y(y(v),v) + J_v(y(v),v)\\
&=-E_v(y(v),v)^*E_y(y(v),v)^{-*}J_y(y(v),v) + J_v(y(v),v) \\
&= -E_v(y(v),v)^*p +J_v(y(v),v) \label{gradient}
\end{align}
Evaluating the gradient for a control variable $v\in V$ typically requires us to solve both the state and adjoint equation, and then inserting the solutions into the expression for the gradient.
\section{Example problem}
To better understand the adjoint approach to gradient evaluation of the reduced objective function, I will define a simple optimal control problem with ODE constraints, and derive its adjoint equation and gradient. The problem will also be used later to test the implementation. In our example both the state $y$ and the control $v$ will be functions on an interval $[0,T]$. This allows us to define the objective function:
\begin{align}
J(y,v) = \frac{1}{2}\int_0^T|v(t)|^2dt + \frac{\alpha}{2}|y(T)-y^T|^2 \label{exs_J}
\end{align}
Our state equation $E(y,v)$ will be a simple linear first order equation with the control as a source term:
\begin{align}
\left\{
     \begin{array}{lr}
       	y'(t)+\alpha y(t) = v(t) \ t\in(0,T)\\
       	y(0)=y_0
     \end{array}
   \right. \label{exs_E}
\end{align}
Equation (\ref{exs_E}) has solution $y(t) = e^{-\alpha t}(C(y_0)+\int_0^te^{\alpha\tau}v(\tau)d\tau)$, and is therefore uniquely solvable for every integrable $v$. This means that we can reduce the objective function. Let us now state and derive the adjoint and gradient expression for the above defined problem: 
\begin{theorem}
The adjoint equation of the problem (\ref{exs_J}-\ref{exs_E}) is:
\begin{align*}     
-p'(t) &= \alpha p(t) \\
p(T) &= y(T)-y^T     
\end{align*}
\end{theorem}
\begin{proof}
Before we calculate the different terms used to derive the adjoint equation, we want to fit our ODE into an expression $E$. We do this by writing up the weak formulation of the equation:
\begin{gather*}
\textit{Find $y \in L^2$ such that}\\
L[y,\phi] = \int_0^T-y(t)\phi'(t)-\alpha y(t)\phi(t)dt -y_0\phi(0)+y(T)\phi(T)-\int_0^Tv(t)\phi(t)=0\\ \forall \ \phi \in C^{\infty}((0,T))
\end{gather*}
To derive the adjoint we need $E_y$ and $J_y$. For $E_y$ we define $(\cdot,\cdot)$ to be the $L^2$ inner product over $(0,T)$. This gives us:
\begin{align*}
E_y=L_y[\cdot,\phi]=(\cdot,(\frac{\partial}{-\partial t} - \alpha + \delta_T)\phi)  
\end{align*}
Lets be more thorough with $J_y$, which is the right hand side in the adjoint equation.
\begin{align*}
J_y(y(v),v) &= \frac{\partial}{\partial y}(\frac{1}{2}\int_0^Tv^2dt + \frac{1}{2}(y(T)-y^T)^2) \\ &= \frac{\partial}{\partial y} \frac{1}{2}(y(T)-y^T)^2 \\
&= \frac{\partial}{\partial y}\frac{1}{2}(\int_0^T \delta_T(y-y^T)dt)^2 \\
&= \delta_T\int_0^T \delta_T(y(t)-y^T)dt \\
&= \delta_T(y(T)-y^T)
\end{align*}
We have $E_y=(\cdot,(-\frac{\partial}{\partial t} - \alpha + \delta_T)\phi)$, but for the adjoint equation we need to find $E_y^*$.
To derive the adjoint of $E_y$, we will insert two functions $v$ and $w$ into $L_y[v,w]$, and try to change the places of $v$ and $w$.
\begin{align*}
E_y&=L_y[v,w]=\int_0^T-v(t)(w'(t)+\alpha w(t))dt + v(T)w(T) \\
&=\int_0^Tw(t)(v'(t)-\alpha v(t))dt + v(T)w(T)-v(T)w(T) +v(0)w(0) \\
&=\int_0^Tw(t)(v'(t)-\alpha v(t))dt+v(0)w(0) \\
&=L_y^*[w,v]=E_y^*
\end{align*}
If we multiply $J_y$ with a test function $\psi$ and set $L_y^*[p,\psi]=(J_y,\psi)$, we get the following equation:
\begin{align*}
&\textit{Find $p$ such that}\\
&\int_0^Tp(t)\psi'(t)-\alpha p(t)\psi(t)dt + p(0)\psi(0)= (y(T)-y^T)\psi(T)\ \forall \ \psi \in C^{\infty}((0,T))
\end{align*}
If we then do partial integration, we get:
\begin{align*}
&\textit{Find $p$ such that}\\
&\int_0^T(-p'(t)-\alpha p(t))\psi(t)dt +p(T)\psi(T)= (y(T)-y^T)\psi(T)\ \forall \ \psi \in C^{\infty}((0,T))
\end{align*}
Using this we get the strong formulation:
\begin{align*}
   \left\{
     \begin{array}{lr}
       -p'(t) = \alpha p(t) \\
       p(T) = y(T)-y^T
     \end{array}
   \right.
\end{align*}
\end{proof}
With the adjoint we can find the gradient of $\hat{J}$. Lets state the result first.
\begin{theorem}
The gradient of the reduced objective function $\hat{J}$ with respect to v is 
$$\hat{J}'(v)=v+p$$
\end{theorem}
\begin{proof}
Firstly we need $J_v$ and $E_v^*$:
\begin{align*}
J_v &= v \\
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi)
\end{align*}
Since $L_v[\cdot,\phi]$ is symmetric, $E_v^*=E_v$, and strongly formulated, $E_v=-1$. The expression for the gradient is then simply:
\begin{align*}
\hat{J}'(v)&=-E_v^*p + J_v \\
&= p+v 
\end{align*} 
\end{proof}
\section{Optimization algorithms}
Deriving and solving the adjoint equation gives us a way of evaluating the gradient of optimal control problems with ODE constraints. With the gradient we can now solve our optimal control problems by using some optimization algorithm. There exists many different optimization algorithms, but here I will only mention line search methods that are useful to me in my thesis. These are the steepest descent method, the BFGS and related L-BFGS methods.  
\subsection{Line search methods and steepest descent}
Line search methods are algorithms that help us solve problems of the type: 
\begin{align*}
\min_x f(x), \ f:\mathbb{R}^n\longrightarrow\mathbb{R}
\end{align*}
The methods are iterative and generate a sequence $\{x^k\}$ that hopefully will converge to a solution. This means that an initial guess $x^0$ is required. The k-th iteration in the algorithm can be described in the following way:
\begin{align*}
1. &\textit{Choose direction $p_k$} \\
2. &\textit{Choose step length $\alpha_k$} \\
3. &\textit{Set $x^{k+1}=x^k + \alpha_kp_k$} 
\end{align*}
What separates different line search methods, is how one chooses direction $p_k$ and step length $\alpha_k$. How to choose a good step length is generally a more difficult question than how to choose the search direction. One way of finding a good search length, are the so called Wolfe conditions. These are as follows:
\begin{align*}
f(x^k + \alpha_kp_k)&\leq f(x^k) + c_1\alpha_k\nabla f(x^k)\cdot p_k \\
\nabla f(x^k + \alpha_kp_k) \cdot p_k &\geq c_2 \nabla f(x^k)\cdot p_k
\end{align*}
Here we use constants $0<c_1<c_2<1$. The first Wolfe condition ensures that the decrease in function value is proportional to both step length and direction. The second condition is that the gradient of $f$ at $x^k + \alpha_kp_k$, should be less steep than at $x^k$. If we can find a step length that satisfies these conditions we will use it. How to actually find a step length that satisfies the Wolfe conditions is quite involved, and I will therefore not go into this topic any further. Instead I will mention a couple of line search methods that will be used later in the thesis, starting with steepest descent.
\\
\\
The steepest descent method is a very simple line search method, where the step length $p_k$ is set to the negative gradient direction at point $x^k$, i.e $p_k = -\nabla f(x^k)$. This gives us the following update for each iteration:
\begin{align*}
x^{k+1} = x^k - \alpha_k \nabla J(x^k)
\end{align*} 
The problem with steepest descent is that it converges quite slowly. This fact we get from the following theorem given in (CITE Numerical optimization Wright)
\begin{theorem}
Assume that $f:\mathbb{R}^n\longrightarrow\mathbb{R}$ is twice continuously differentiable, that the steepest decent method converge to a point $x^*$, and that the Hessian of $f$ at this point, $\nabla^2 f(x^*)$ is positive definite. Then the following holds:
\begin{align*}
f(x^{k+1})-f(x^*) \leq (\frac{\lambda_n-\lambda_1}{\lambda_n+\lambda_1})^2 f(x^{k})-f(x^*)
\end{align*}  
Here $\lambda_1\leq\cdots\leq \lambda_n$ denotes the eigenvalues of $\nabla^2 f(x^*)$.
\end{theorem}   
\begin{proof}
CITE: Numerical optimization Wright
\end{proof}
\subsection{BFGS and L-BFGS}
\begin{align*}
x^{k+1} = x^k + \alpha H^{k}\nabla J(x^k)
\end{align*} 
