\chapter{Introduction}
In todayâ€™s world high performance computing is an essential tool for scientists in many fields such as engineering, computational physics and chemistry, bioinformatics, weather forecasting and so on. Many problems that arise in these areas are so computationally costly, that they can not be solved efficiently or at all on a single processor. Instead we solve or accelerate the solution of such problems by running them on large-scale clusters of multiple processes in parallel. One of the main issues with parallel computing is that most numerical solvers are sequentially formulated, and the work of translating these algorithms into a parallel framework can often be time and effort intensive.
\\
\\
One class of large-scale problems suited for parallelization, that frequently pops up both in science and elsewhere, are time dependent partial differential equations(PDEs). The traditional approach to implementing parallel solvers for such problems is to restrict the parallel computations to operations in spacial dimension within each time step, while the time-integration is done sequentially. Letting the implementation be serial in temporal direction is the most intuitive way of parallelizing time dependent PDEs, since evolving an equation in time is a naturally sequential process. Decomposing in space on the other hand allows us to partition the problem into independent tasks. It is usually more efficient to limit the parallel computing to the spacial domain, however in cases where the number of available cores are high, introducing parallelism in temporal direction can reduce the overall solution time. We therefore want solvers for time dependent PDEs that are parallel in time.
\\
\\
There exists multiple methods for parallel in time solvers of evolution equations. The most famous and most developed of these parallel in time methods is the so called Parareal method introduced in \cite{lions2001resolution}. The parallelism of Parareal is restricted to the temporal dimension, and can therefore be used to parallelize both time dependent PDEs and ordinary differential equations(ODEs). It shares this feature with the related multiple shooting methods\cite{nievergelt1964parallel,bellen1989parallel}, while waveform relaxation methods\cite{lelarasmee1982waveform,gander1996overlapping} and multigrid methods\cite{hackbusch1985parabolic,lubich1987multi,horton1995space} achieves parallelism in time by parallelizing in both space and time simultaneously. In this thesis we will restrict our self to Parareal, and the other methods will not be touched upon any further.
\\
\\
The topic of this thesis is a subject closely related to time dependent differential equations(DE) namely optimal control with time dependent DE constraints. Optimal control with DE constraints are minimization problems on the following form:
\begin{align}
\underset{y,v}{\text{min}} \ J(y,v) \quad \textrm{Subject to:} \ E(y,v)=0. \label{OCP}
\end{align}
The functional $J$ that we want to minimize is usually referred to as the objective function, while the variables $y$ and $v$ are respectively named the state and the control. The goal of the control problem (\ref{OCP}) is to find a pairing $(\bar y,\bar v)$ that minimize the objective function, but also satisfy the constraints set up by the sate equation $ E(\bar y,\bar v)=0$. Optimal control problems with PDE constraints have many applications. Some examples are: Variational data assimilation, sensitivity analysis, goal-based error estimation and more.
\\
\\
Numerically solving optimal control problems with DE constraints involves solving multiple differential equations. The computational cost of solving these equations will dominate the overall computational cost of any optimal control solver. Parallelization of problems of type (\ref{OCP}) are therefore connected to the parallelization of differential equations. In this thesis we will investigate a Parareal-based parallel in time framework for optimal control with time dependent DE constraints. To achieve this, we will use the same strategy as in \cite{maday2002parareal}, meaning that we enforce the dependency between decomposed intervals by altering the objective function. The Parareal algorithm is then applied as a preconditioner for the optimization algorithm. In \cite{maday2002parareal} the parallelization of time dependent optimal control problems is done by applying the Parareal preconditioner to the steepest descent method. We will instead test out an implementation where the same Parareal preconditioner is used in combination with the BFGS method. The algorithm that we present in this thesis is in theory applicable to optimal control problems with DE constraints. We will however in this thesis restrict our self to example problems with ordinary differential equation(ODE) constraints.
\section{Summary}
The overall goal of this thesis is to establish a parallel in time algorithm for solving optimal control problems with time dependent DE constraints, and the structure of the work done can roughly divided into two parts:
\begin{align*}
&1.\quad \textrm{Backround and presentation of algorithm} \\
&2.\quad \textrm{Verification and experiments}
\end{align*}
The bulk of the thesis is found in the first part, where we present and motivate a parallel framework for parallization of the control problem. In chapter \ref{lit_chap} we give a short literature review of previous work done on the Parareal algorithm, its theory and its application, emphasizing possible extensions to optimal control. A more detailed, but still shallow presentation of the Parareal algorithm is found in chapter \ref{parareal_chap}. Of special importance in this presentation, is the section on the alternative algebraic formulation of Parareal, since this formulation is used later in chapter \ref{method_chap} to derive the Parareal based preconditioner. In chapter \ref{math_chap} we look at general theory for optimal control with DE constraints. Among other things the adjoint approach to gradient evaluation is presented. Also included in chapter \ref{math_chap} is one section on optimization algorithms and one on finite difference discretizations of ODEs.
\\
\\
How we translate the solution process of time dependent optimal control problems into a parallel framework, is detailed in \ref{method_chap}. Here we explain how to decompose the time domain of control problems, and how we can use the penalty method to enforce the continuity constraints that arises when decomposing in time. The rest of chapter \ref{method_chap} is dedicated to the presentation and derivation of the Parearal preconditioner. Since we want to use this preconditioner in combination with the BFGS optimization algorithm, we also need to check if it possesses the necessary properties for this to be possible. 
\\
\\
The second part of the thesis deals with testing and verification of the algorithm. In chapter \ref{disc_chap} we explain how we discretize the decomposed time domain and the non-penalized and penalized objective function for an example problem. In chapter \ref{Verification chapter} the discetized objective function and its gradient from chapter \ref{disc_chap} is verified using the Taylor test. In the second part of chapter \ref{disc_chap} we also explain how we implement the objective function and gradient evaluation in parallel using the message passing interface(MPI), with special attention on communication between processes and how this communication affects speedup. The theoretical speedup for function and gradient evaluation is also verified in \ref{Verification chapter}. Chapter \ref{Verification chapter} also contains a section showing how the solution of the discretized control problem converges to the exact solution, and a section that demonstrates the consistency of the penalty framework presented in chapter \ref{method_chap}. Chapter \ref{Experiments chapter} contains the results of experiments done using the method from \ref{method_chap}. The main focus of these results are the speedup this parallel algorithm produces. We measure speedup both in wall clock time and in a measure representing ideal speedup. This ideal speedup is based on the number of objective function and gradient evaluations done by the sequential and parallel algorithm.
