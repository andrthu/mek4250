\chapter{Introduction}
In todayâ€™s world, high performance computing is an essential tool for scientists in many fields such as engineering, computational physics and chemistry, bioinformatics or weather forecasting. Many problems that arise in these areas are so computationally costly, that they can not be solved efficiently or at all on a single processor. Instead we solve or accelerate the solution of such problems by running them on large-scale clusters of multiple processes in parallel. One of the main issues with parallel computing is that many numerical solvers are sequentially formulated, and the work of translating these algorithms into a parallel framework can often be time and effort intensive.
\\
\\
One class of large-scale problems suited for parallelization, that frequently occurs both in science and engineering, are time-dependent partial differential equations (PDEs). The traditional approach to implementing parallel solvers for such problems is to restrict the parallel computations to operations in spatial dimension at each time step, while the time-integration is done sequentially. Letting the implementation be serial in temporal direction is the most intuitive way of parallelizing time-dependent PDEs, since evolving an equation in time is a naturally sequential process. A lot of work has also been done on parallel solvers for spatially dicretized problems, meaning that methods and strategies for parallelization in space already are developed and tested \cite{elman2014finite}. However, for a fixed discretization, the achievable speedup through spatial parallelization is limited when the number of cores are high. Therefore, introducing parallelism in temporal direction is a way of increasing the speedup beyond this bound. It is therefore desirable to develop solvers for time-dependent PDEs that are parallel in time.
\\
\\
There exists multiple methods for parallel in time solvers of evolution equations. The most famous and most developed of these methods is the so called Parareal method introduced in \cite{lions2001resolution}. The parallelism of Parareal is restricted to the temporal dimension, and can therefore be used to parallelize both time-dependent PDEs and ordinary differential equations (ODEs). It shares this feature with the related multiple shooting methods\cite{nievergelt1964parallel,bellen1989parallel}, while waveform relaxation methods\cite{lelarasmee1982waveform,gander1996overlapping} and multigrid methods\cite{hackbusch1985parabolic,lubich1987multi,horton1995space} achieves parallelism in time by parallelizing in both space and time simultaneously. In this thesis we will restrict ourself to Parareal, and the other methods will not be touched upon any further.
\\
\\
Parallel differential equation (DE) solvers are well studied. In this thesis we will focus on a similarly important set of problems. Optimization problems constrained by time-dependent DEs. These occur for example in: Optimal control ,variational data assimilation and optimal design. Optimization with differential equation constraints are minimization problems of the following form:
\begin{align}
\underset{y,v}{\text{min}} \ J(y,v) \quad \textrm{subject to} \ E(y,v)=0. \label{OCP}
\end{align}
The functional $J$ that we want to minimize is usually referred to as the objective function. $E(y,v)=0$ represents the differential equation constraint, and is called the state equation. The state equation is solved for the state $y$, while the $v$ variable called the control represents parameters of the equation. The goal of the control problem (\ref{OCP}) is to find a pairing $(\bar y,\bar v)$ that minimizes the objective function, but also satisfies the constraints set up by the state equation $ E(\bar y,\bar v)=0$.  For further details on optimization we refer to \cite{hinze2008optimization}.
\\
\\
Different strategies for numerically solving the optimal control problem (\ref{OCP}) exists. One alternative is to set up and solve the optimality system stemming from the Lagrangian function associated with problem (\ref{OCP}). In this thesis we will not use this approach, but instead reduce the constrained problem (\ref{OCP}) into an unconstrained problem of type (\ref{red_OCP}), and then solve this new problem using techniques from unconstrained optimization. 
\begin{align}
\underset{v}{\text{min}} \ J(y(v),v). \label{red_OCP}
\end{align}
Choosing this approach will obviously limit us to optimization problems where this reduction is possible, which is the case for many practical problems. The process of finding the minimizer of problem (\ref{red_OCP}) involves repeatedly solving differential equations. The computational cost of solving these equations will dominate the overall computational cost of any optimal control solver based on the reduction approach. Parallelization of problems of type (\ref{red_OCP}) are therefore connected to the parallelization of differential equations. In this thesis we will develop and investigate a Parareal-based parallel in time framework for optimal control problems with time-dependent DE constraints. To achieve this, we will use the same strategy as in \cite{maday2002parareal}, meaning that we enforce the dependency between decomposed intervals by altering the objective function. The Parareal algorithm is then applied as a preconditioner for the optimization algorithm. 
\\
\\
In \cite{maday2002parareal} the parallelization of time-dependent optimal control problems is done by applying the Parareal preconditioner to the steepest descent method. We will instead propose and implement a parallel in time algorithm using the same Parareal preconditioner in combination with the BFGS method. We will also derive properties of the  Parareal-based preconditioner showing that it is compatible with BFGS. The consistency of our method is discussed both through theory and experiments. The algorithm that we present in this thesis is applicable to optimal control problems with ODE and PDE constraints, and for PDE constraints it can also be combined with spatial parallelization. For simplicity we will restrict the example problems to ordinary differential equation constraints.
\\
\\
Our algorithm is in many ways similar to the parallel in time algorithm for 4d variational data assimilation introduced in \cite{rao2016time}, since that algorithm also is based on the BFGS method. The main difference between our algorithmic framework and the one found in \cite{rao2016time}, is that we include the Parareal-based preconditioner from \cite{maday2002parareal}. Experiments conducted in \cite{rao2016time} showed that their algorithm experienced low to no speedup at all, and that the problems grew when more CPUs were added. Our algorithm does not share these scaling problems. In fact our experiments show that the algorithmic framework introduced in this thesis works better for higher numbers of processes (16+) than it does when we use a smaller number of processes.
\section{Summary}
The overall goal of this thesis is to establish a parallel in time algorithm for solving optimal control problems with time-dependent DE constraints. The structure of the work done can roughly divided into two parts:
\begin{align*}
&1.\quad \textrm{Background and presentation of the algorithms} \\
&2.\quad \textrm{Verification and experiments}
\end{align*}
The bulk of the thesis is found in the first part, where we present and motivate a parallel framework for parallelization of control problems. In chapter \ref{lit_chap} we give a short literature review of previous work done on the Parareal algorithm, its theory and its application, emphasizing possible extensions to optimal control. In chapter \ref{math_chap} we look at general theory for optimal control with DE constraints. Among other things the adjoint approach to gradient evaluation is presented. Chapter \ref{math_chap} also includes one section on optimization algorithms and one on finite difference discretizations of ODEs. A more detailed, but still shallow presentation of the Parareal algorithm is found in chapter \ref{parareal_chap}. Of special importance in this presentation, is the section on the alternative algebraic formulation of Parareal, since this formulation is used later in chapter \ref{method_chap} to derive the Parareal based preconditioner.
\\
\\
How we translate the solution process of time-dependent optimal control problems into a parallel framework, is detailed in \ref{method_chap}. Here we explain how to decompose the time domain of control problems, and how we can use the penalty method to enforce the continuity constraints that arises when decomposing in time. The rest of chapter \ref{method_chap} is dedicated to the presentation and derivation of the Parearal preconditioner. Since we want to use this preconditioner in combination with the BFGS optimization algorithm, we also need to check if the proposed preconditioner possesses the necessary mathematical properties for this to be possible. 
\\
\\
The second part of the thesis deals with implementation, testing and verification of the algorithm. In chapter \ref{disc_chap} we explain how we discretize the decomposed time domain and the non-penalized and penalized objective function for an example problem. In chapter \ref{Verification chapter} the discetized objective function and its gradient from chapter \ref{disc_chap} is verified using the Taylor test. In the second part of chapter \ref{disc_chap} we also explain how we implement the objective function and gradient evaluation in parallel using the message passing interface (MPI), with special attention on communication between processes and how this communication affects the speedup. The speedup of our implementation for function and gradient evaluation is also verified against the theoretical optimal speedup in chapter \ref{Verification chapter}. Chapter \ref{Verification chapter} also demonstrates how the solution of the discretized control problem converges to the exact solution, and the consistency of the penalty framework presented in chapter \ref{method_chap}. Chapter \ref{Experiments chapter} contains the results of experiments conducted using the method proposed in chapter \ref{method_chap}. The main focus of these results are the speedup this parallel algorithm produces. We measure speedup both in wall clock time and in a measure representing ideal speedup. This ideal speedup is based on the number of objective function and gradient evaluations done by the sequential and parallel algorithm.
