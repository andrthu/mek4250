\chapter{Verification}\label{Verification chapter}
\section{Taylor test}
The Taylor test is a good way to test the correctness of the gradient of a function. The test is as its name implies connected with Taylor expansions of a function, or more precisely the following two observations:
\begin{align*}
|J(v+\epsilon w)-J(v)| &= \mathcal{O}(\epsilon) \\
|J(v+\epsilon w)-J(v)-\epsilon\nabla J(v)\cdot w| &= \mathcal{O}(\epsilon^2)
\end{align*}
Here $w$ is a random direction in the same space as $v$, while $\epsilon$ is some constant. 
\\
\\
The test is carried out by evaluating $D=|J(v+\epsilon w)-J(v)-\epsilon\nabla J(v)\cdot w|$ for decreasing $\epsilon$'s, and if $D$ approaches 0 at 2nd order rate, we consider the test as passed.
\subsection{Verifying the numerical gradient using the Taylor test}
we will now use the Taylor test on the numerical gradient (\ref{num_grad}) that we get when solving the following problem:
\begin{align}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(1)-1.5)^2\\
&\left\{
     \begin{array}{lr}
       	y'(t)=0.9y(t) +v(t), \ t \in (0,1)\\
       	   y(0)=3.2
     \end{array}
   \right. 
\end{align}
We then discretize in time using $\Delta t=\frac{1}{100}$, and we set $v_i=1 \ \forall i$, while $w_i$ are chosen randomly from numbers between 0 and 100. Applying the Taylor test to this problem, and setting:
\begin{align}
D_1(\epsilon) &= |J(v+\epsilon w)-J(v)| \label{D1} \\
D_2(\epsilon) &=|J(v+\epsilon w)-J(v)-\epsilon \nabla J(v)\cdot w|\label{D2}
\end{align} 
yielded the following:
\\
\begin{table}[h]
\caption{Taylor test}
\label{Taylor_tab1}
\centering
\begin{tabular}{lrrrll}
\toprule
{} $\epsilon$&  $D_1$ &  $D_2$ &        $||\epsilon w||_{l_{\infty}}$ &    $ \log(\frac{D_1(10\epsilon)}{D_1(\epsilon)})$ &    $ \log(\frac{D_2(10\epsilon)}{D_2(\epsilon)})$ \\
\midrule
1.000000e+00 &  5956.494584 &        5.244487e+03 &  99.987417 &       -- &       -- \\
1.000000e-01 &   123.645671 &        5.244487e+01 &   9.998742 &  1.68281 &        2 \\
1.000000e-02 &     7.644529 &        5.244487e-01 &   0.999874 &  1.20883 &        2 \\
1.000000e-03 &     0.717253 &        5.244487e-03 &   0.099987 &  1.02768 &        2 \\
1.000000e-04 &     0.071253 &        5.244487e-05 &   0.009999 &  1.00287 &        2 \\
1.000000e-05 &     0.007121 &        5.244489e-07 &   0.001000 &  1.00029 &        2 \\
1.000000e-06 &     0.000712 &        5.244760e-09 &   0.000100 &  1.00003 &  1.99998 \\
1.000000e-07 &     0.000071 &        5.255194e-11 &   0.000010 &        1 &  1.99914 \\
\bottomrule
\end{tabular}
\end{table}
\\
\\
The above table clearly shows that $J(v+\epsilon w)-J(v)-\epsilon \nabla J(v)\cdot w|$ converges to zero at a second order rate. This means that the numerical gradient of our test problem passes the Taylor test. This again indicates that both the numerical gradient and the implementation of it are correct. Let us then check if this is also the case for the penalized problem.
\begin{figure}[h]
\caption{Plots showing the gradient, with focus on the two ends. Also included is a plot with a finite difference approximation to the gradient.}
\centering
\includegraphics[scale=0.5]{num_grad.png}
\end{figure}
\subsection{Verifying the penalized numerical gradient using the Taylor test}
We will now use the Taylor test on the penalized numerical gradient (\ref{num_pen_grad_lam}-\ref{num_pen_grad_v}) that we get when decomposing $I=[0,T]$ into $N=10$ subintervals while solving the following problem:
\begin{align}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(1)-1.5)^2\\
&\left\{
     \begin{array}{lr}
       	y'(t)=0.9y(t) +v(t), \ t \in (0,1)\\
       	   y(0)=3.2
     \end{array}
   \right. 
\end{align}
We then discretize in time using $\Delta t=\frac{1}{100}$. The control variable is now a vector $v\in\mathbb{R}^{N+m}$ and we set $v_k=0 \ \forall k=0,...,N+n-1$, while the $w_k$s are chosen randomly from numbers between 0 and 100. Applying the Taylor test to this problem, and letting $D_1$ and $D_2$ be defined as in (\ref{D1}-\ref{D2}) yielded the following:
\\
\begin{table}[!h]
\caption{Taylor test}
\centering
\label{Taylor_tab2}
\begin{tabular}{lrrrll}
\toprule
{}$\epsilon$&  $D_1$ &  $D_2$ &        $||\epsilon w||_{l_{\infty}}$ &    $ \log(\frac{D_1(10\epsilon)}{D_1(\epsilon)})$ &    $ \log(\frac{D_2(10\epsilon)}{D_2(\epsilon)})$  \\
\midrule
1.000000e+00 &  1.080513e+04 &        1.076907e+04 &  9.771288e+01 &       -- &       -- \\
1.000000e-01 &  1.112972e+02 &        1.076907e+02 &  9.771288e+00 &  1.98715 &        2 \\
1.000000e-02 &  1.437558e+00 &        1.076907e+00 &  9.771288e-01 &  1.88886 &        2 \\
1.000000e-03 &  4.683423e-02 &        1.076907e-02 &  9.771288e-02 &  1.48706 &        2 \\
1.000000e-04 &  3.714207e-03 &        1.076907e-04 &  9.771288e-03 &   1.1007 &        2 \\
1.000000e-05 &  3.617285e-04 &        1.076907e-06 &  9.771288e-04 &  1.01148 &        2 \\
1.000000e-06 &  3.607593e-05 &        1.076908e-08 &  9.771288e-05 &  1.00117 &        2 \\
1.000000e-07 &  3.606624e-06 &        1.076979e-10 &  9.771288e-06 &  1.00012 &  1.99997 \\
1.000000e-08 &  3.606527e-07 &        1.086074e-12 &  9.771288e-07 &  1.00001 &  1.99635 \\
\bottomrule
\end{tabular}
\end{table}
\\
\\
Again we see that $|J(v+\epsilon w)-J(v)-\epsilon \nabla J(v)\cdot w|$ converges to zero at a second order rate, meaning that the penalized numerical gradient also passes the Taylor test.
\begin{figure}[h]
\caption{Plots showing the $\Lambda$ and control part of the numerical gradient found using formula (\ref{num_pen_grad_lam}-\ref{num_pen_grad_v}) and finite difference}
\centering
\includegraphics[scale=0.5]{pen_num_grad.png}
\end{figure}
\section{Convergence rate of solution}
\begin{table}[h]
\label{CN_convergence}
\caption{Convergence of numerical sequential solver of optimal control problem.}
\centering
\begin{tabular}{lrrll}
\toprule
{} &          norm &           val &   norm r &    val r \\
\midrule
\midrule
0.02000 &  4.177702e-02 &  2.309886e-03 &       -- &       -- \\
0.01000 &  1.109500e-02 &  3.383020e-04 &  -1.9128 & -2.77144 \\
0.00100 &  1.189515e-04 &  3.931451e-07 & -1.96976 & -2.93475 \\
0.00010 &  1.421834e-06 &  3.992950e-10 & -1.92252 & -2.99326 \\
0.00001 &  1.480190e-08 &  3.978299e-13 & -1.98253 &  -3.0016 \\
\bottomrule
\end{tabular}
\end{table}
\section{Verifying function and gradient evaluation speedups} \label{ver S sec}
In \ref{analysis sec} we derived the theoretical speedup for numerical gradient and objective function evaluation when decomposing the time-interval. It would now be interesting to check if the implementation achieves the theoretical speedup for our example problem. The specific problem is:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-1)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)+y(t) = v(t) \ t\in(0,1)\\
       	y(0)=1
     \end{array}
   \right. 
\end{align*}
A computer with 6 cores was used to verify the results of section \ref{analysis sec}. Having 6 cores means that we can do gradient and function evaluation for $N=1,2,...,6$ decompositions with different time step sizes $\Delta t$. For each combination of $N$ and $\Delta t$, we will run the function and gradient evaluations ten times, and then choose the the smallest execution time produced by the ten runs. The speedup is then calculated by dividing the sequential execution time by the parallel execution time. Below follows tables showing runtime and speedup for both gradient and function evaluation for different $\Delta t$s and $N$s. All evaluations are done with control input $v=1$ and $\lambda_i=1$.  
\\
\begin{table}[!h]
\centering
\caption{$\Delta t=10^{-2}$}
\begin{tabular}{lrr}
\toprule
{} $N$&   function speedup &      function time \\
\midrule
1 &  1.000000 &  0.000196 \\
2 &  0.946860 &  0.000207 \\
3 &  0.780876 &  0.000251 \\
4 &  0.642623 &  0.000305 \\
5 &  0.544444 &  0.000360 \\
6 &  0.427948 &  0.000458 \\
\bottomrule
\end{tabular}
\begin{tabular}{lrr}
\toprule
{} &  gradient speedup &     gradient time \\
\midrule
1 &  1.000000 &  0.000217 \\
2 &  0.875000 &  0.000248 \\
3 &  0.753472 &  0.000288 \\
4 &  0.632653 &  0.000343 \\
5 &  0.547980 &  0.000396 \\
6 &  0.480088 &  0.000452 \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[!h]
\centering
\caption{$\Delta t=10^{-4}$}
\begin{tabular}{lrr}
\toprule
{} $N$&  function speedup &    function  time \\
\midrule
1 &  1.000000 &  0.008877 \\
2 &  1.983687 &  0.004475 \\
3 &  2.838823 &  0.003127 \\
4 &  3.582324 &  0.002478 \\
5 &  4.267788 &  0.002080 \\
6 &  4.519857 &  0.001964 \\
\bottomrule
\end{tabular}
\begin{tabular}{lrr}
\toprule
{} &  gradient speedup &     gradient time \\
\midrule
1 &  1.000000 &  0.015016 \\
2 &  1.946843 &  0.007713 \\
3 &  2.816204 &  0.005332 \\
4 &  3.677688 &  0.004083 \\
5 &  4.457109 &  0.003369 \\
6 &  4.978780 &  0.003016 \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[!h]
\centering
\caption{$\Delta t=10^{-5}$}
\begin{tabular}{lrr}
\toprule
{}$N$ &  function speedup &    function  time \\
\midrule
1 &  1.000000 &  0.087484 \\
2 &  2.006606 &  0.043598 \\
3 &  2.888595 &  0.030286 \\
4 &  3.913222 &  0.022356 \\
5 &  4.848102 &  0.018045 \\
6 &  5.425028 &  0.016126 \\
\bottomrule
\end{tabular}
\begin{tabular}{lrr}
\toprule
{} &  gradient speedup &     gradient time \\
\midrule
1 &  1.000000 &  0.154841 \\
2 &  2.046537 &  0.075660 \\
3 &  2.971198 &  0.052114 \\
4 &  4.003025 &  0.038681 \\
5 &  4.921368 &  0.031463 \\
6 &  5.755101 &  0.026905 \\
\bottomrule
\end{tabular}
\end{table}
\begin{table}[!h]
\centering
\caption{$\Delta t=10^{-7}$}
\begin{tabular}{lrr}
\toprule
{}$N$ &  function speedup &      time \\
\midrule
1 &  1.000000 &  8.350907 \\
2 &  1.987960 &  4.200743 \\
3 &  2.847662 &  2.932549 \\
4 &  3.812545 &  2.190376 \\
5 &  4.647839 &  1.796729 \\
6 &  5.479447 &  1.524042 \\
\bottomrule
\end{tabular}
\begin{tabular}{lrr}
\toprule
{} &  gradient speedup &     gradient  time \\
\midrule
1 &  1.000000 &  14.930247 \\
2 &  2.064043 &   7.233497 \\
3 &  2.966254 &   5.033368 \\
4 &  3.866428 &   3.861509 \\
5 &  4.833081 &   3.089178 \\
6 &  5.744552 &   2.599027 \\
\bottomrule
\end{tabular}
\end{table}
\\
\\
Since the parallel algorithm has some overhead, we do not expect any improvements for small problems. This is reflected in the above results, where we for $\Delta t = 10^{-2}$ see an increased execution time when running function and gradient evaluation in parallel. For $\Delta t = 10^{-4}$ we see only a modest speedup, that is significantly lower than the expected speedup from section \ref{analysis sec}. For $\Delta t \leq 10^{-5}$, however we see speedup results in line with what we expect from the theory.  
\section{Consistency}
When we introduced the penalty method in section \ref{penalty_sec}, we also presented a result showing that the iterates $\{v^k\}$ stemming from the penalty algorithmic framework converged towards the solution of the non-penalized problem $v$. We can write this up as:
\begin{align*}
\lim_{k\rightarrow\infty} v^k = v 
\end{align*}  
An alternative way of looking at this, is to let $v^{\mu}$ be the minimizer of $\hat J_{\mu}$, and instead write the above limit as:
\begin{align}
\lim_{\mu\rightarrow\infty} v^{\mu} = v \label{mu con}
\end{align}
The interpretation of the above limit, is that solving the penalized problem with an ever increasing penalty parameter $\mu$ should result in a solution that is getting closer and closer to the solution of the non-penalized problem. This means that the penalty algorithm is consistent, since it produces the same solution as the ordinary non-decomposed problem. It is therefore worth checking if the implementation of the penalized problem actually has the property (\ref{mu con}). The particular problem, that we will do the consistency test on, is: 
\begin{align}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-11.5)^2 \label{con J} \\
&\left\{
     \begin{array}{lr}
       	y'(t)=-3.9y(t) + v(t) \ t\in(0,1)\\
       	y(0)=3.2
     \end{array}
   \right. \label{con E}
\end{align}
We want to compare the solution we get by decomposing and then applying the penalty method on problem (\ref{con J}-\ref{con E}) with the solution we get by solving the undecomposed problem. To do this we discretize (\ref{con J}-\ref{con E}) using two different time steps. First we let $\Delta t = 10^{-2}$ and apply the penalty method for $N=2$ and $N=10$ decompositions, we then let $\Delta t = 10^{-3}$ and test the penalty method on $N=2$ and $N=7$ decompositions. For both time steps, the Crank-Nicolson will be used to discretize the state and adjoint equations. We use different metrics to compare the non-penalized and penalized solutions, so that we better see how the solution of the penalized problem behaves when we solve it for an increasing sequence of $\mu$ values. We define the metrics as follows:
\begin{align*}
\textrm{Realtive objective function differnce:}\quad A &= \frac{\hat{J}(v_{\mu})-\hat{J}(v)}{\hat{J}(v)}\\
\textrm{Realtive penalized objective function differnce:}\quad B &= \frac{\hat{J}_{\mu}(v)-\hat{J}_{\mu}(v_{\mu})}{\hat{J}_{\mu}(v)}\\
\textrm{Relative control $L^2$-norm difference:}\quad C&=\frac{||v_{\mu}-v||_ {L^2}}{||v||_{L^2}} \\
\textrm{Maximal jump in decomposed state equation:}\quad D&= \sup_i\{y_{k_i}^i-y_{k_i}^{i+1}\}\\
\end{align*}
Notice that both $A$ and $B$ should be grater than $0$, since $v$ and $v_{\mu}$ are the minimizers of $\hat J$ and $\hat J_ {\mu}$. The measure of jumps in the state equation $D$ is added to check that the penalty solution approaches a feasible solution in context of the continuity constraints (\ref{Extra constraints}). The results of the above detailed experiment are presented through logarithmic plots in figure \ref{Cons1_fig} and \ref{Cons2_fig}. 
\\
\\
\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{consistency1.png}
\caption{Logarithmic plot showing how solution of penalty method applied to problem (\ref{con J}-\ref{con E}) develops for $\Delta t = 10^{-2}$.}
\label{Cons1_fig}
\end{figure}
\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{consistency2.png}
\caption{Same as figure \ref{Cons1_fig}, only now $\Delta t = 10^{-3}$.}
\label{Cons2_fig}
\end{figure}
The plots in figure \ref{Cons1_fig} and \ref{Cons2_fig} all show a similar picture, and we observe that all measures decrease when the penalty parameter is increased. Still there are several parts of the plots worthy of note. The measure $A$ related to the unpenalised objective function is the value that converges to zero the fastest. If we look at the vales of $A$ before the machine precision is reached we see that $A$ is proportional to $\frac{C}{\mu^2}$. The convergence rate of $A$ for $\Delta t=10^{-2}$ and $N=2$ is shown in table \ref{Cosn_rate_table} together with the rate convergence rate of $C$. $C$ and the other measures converge to zero at a rate of one, however we see that the relative error between the controls $v$ and $v_{\mu}$ $C$ stops to decrease long before the machine precision is reached. It seems that this barrier is hit around the same time as $A$ approaches machine precision. The reason for this probably is that small changes in the control $v_{mu}$ no longer registers in $\hat J_{\mu}$, and it is therefore difficult to find an appropriate step length in the line search method.
\\
\\
$B$ and $D$ on the other hand continue to decrease steadily towards zero, even after $A$ has hit machine precision. The $B$ and $C$ metrics are both related to the $\frac{\mu }{2}\sum_{i=1}^{N-1}(y^i(T_{i})-\lambda_i)^2$ term, which is the part that enforces the continuity constraints (\ref{Extra constraints}). This means that after a certain point, the penalty method only improves the $\Lambda$ part of the control, while $v$ remains the same. 
\begin{table}[!h]
\centering
\caption{Convergence rates for $\Delta t=10^{-2}$ and $N=2$. Notice how the $||v_{\mu}-v||$ stops to decrease at around the same time as $\frac{J(v_{\mu})-J(v)}{J(v)}$ hits machine precision.}
\label{Cosn_rate_table}
\begin{tabular}{lrrll}
\toprule
{} $\mu$&  $\frac{J(v_{\mu})-J(v)}{J(v)}$ &   $||v_{\mu}-v||$ &        A rate &        C rate \\
\midrule
1.000000e+01 &      4.105697e-07 & 1.790231e-03 &            -- &            -- \\
1.000000e+02 &      4.119052e-09 & 1.793140e-04 & -1.998590 & -0.9992948 \\
1.000000e+03 &      4.120272e-11 & 1.793401e-05 & -1.999871 & -0.9999368 \\
1.000000e+04 &      4.137632e-13 & 1.796793e-06 & -1.998174 & -0.9991795 \\
2.000000e+04 &      1.058008e-13 & 9.076756e-07 & -1.967455 & -0.9851756 \\
5.000000e+04 &      1.789909e-14 & 3.733858e-07 & -1.939131 & -0.9694245 \\
7.000000e+04 &      7.968773e-15 & 2.444327e-07 & -2.405011 & -1.259160 \\
1.000000e+05 &      4.045685e-15 & 1.730018e-07 & -1.900553 & -0.9690555 \\
2.000000e+05 &      4.045685e-15 & 1.721746e-07 &  0.000000 & -0.0069153 \\
3.000000e+05 &      3.923088e-15 & 1.719606e-07 & -0.007589 & -0.0030671 \\
4.000000e+05 &      3.800492e-15 & 1.718652e-07 & -0.110360 & -0.0019278 \\
5.000000e+05 &      3.677895e-16 & 4.545076e-08 & -10.46580 & -5.960652e \\
\bottomrule
\end{tabular}

\end{table}
