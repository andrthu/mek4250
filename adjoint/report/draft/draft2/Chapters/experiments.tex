\chapter{Experiments}\label{Experiments chapter}
In this chapter we will through experiments investigate what speedup one might get by using the algorithm for parallelizing optimal control problems with  time dependent DE constraints in temporal direction, introduced in previous chapters. Unlike the parallel performance of gradient and objective function evaluation, the parallel performance of our overall algorithm is difficult to model. The reason for this is that it is difficult to say how many gradient and function evaluations are needed for the optimization algorithms to terminate. We are therefore unable to derive any theoretical expected speedup.
\\
\\
In section \ref{analysis sec} we explained that the best way of measuring performance of a parallel algorithm is to compare its execution time to the sequential execution time of the best sequential algorithm. When solving optimal control problems with DE constraints, the runtime of our solution algorithm will depend on how many times we have to evaluate the objective function and its gradient, since these evaluations require either the solution of the state equation or the state and adjoint equations. We know from theory in section \ref{analysis sec} and verification in section \ref{ver S sec}, that the speedup of parallel gradient and function evaluation depends linearly on the number of processes we use. An alternative way of measuring parallel performance is therefore to compare the sum of gradient and function evaluations in the sequential and parallel algorithms. Let us give this numbers a name:
\begin{align*}
L_s &= \textit{Number of function and gradient evaluations for sequantial algorithm}\\
L_{p_N} &= \textit{Number of function and gradient evaluations for parallel algorithm using N processes}
\end{align*} 
Using these definitions we define the ideal speedup $\hat{S}$, as the speedup one would expect based on $L_s$ and $L_{p_N}$ and the speedup results we have for function and gradient evaluations:
\begin{align}
\hat S = \frac{NL_s}{L_{p_N}} \label{ideal S}
\end{align}
With $\hat S$, it is possible to say something about the performance of the parallel algorithm without having to time it, or actually run it in parallel. It will also be useful to compare the ideal speedup with the measured speedup, as a way to check if the parallel implementation implemented efficiently.
\section{L-BFGS with and without parareal preconditioner}
In section \ref{pc sec} we introduced the parareal preconditioner, as an approximation to the Hessian. Using this preconditioner in our L-BFGS solver we hope that the number of gradient and function evaluations needed in our algorithm will be smaller than if we do not use it. To test this, let us again write up an example problem to be solved:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-1.5)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)=0.9y(t) + v(t) \ t\in(0,1)\\
       	y(0)=3.2
     \end{array}
   \right. 
\end{align*}
We will first solve this problem without decomposing the time interval, and then solve the decomposed problem using $N=2,3,4,5,6,8,16,32,64$ and penalty parameter $\mu=10^4$. This means that we will only use one penalty iteration, as we have found this to be the most effective way to solve the decomposed problem for this specific problem. For both the penalized and non-penalized problems we use L-BFGS with stop criteria:
\begin{align*}
||\nabla J||_{L^2} <10^{-6}
\end{align*}  
Since the point of this test is to compare the effectiveness of the parareal preconditioner, we solve the decomposed problems with and without it. In the result table below we have included the total number of gradient and function evaluations for the two cases as "pc L" and "non-pc L". We also measured the relative $L^2$-norm difference between the non-penalized control solution and all the penalized control solutions. The ideal speedup (\ref{ideal S}) is calculated for preconditioned and unpreconditioned solvers.
\\
\\
\begin{table}[h]
\centering
\caption{Comparing unpreconditioned and preconditioned solver}
\label{compare_table}
\begin{tabular}{lrrllrr}
\toprule
{}$N$ &  pc L &  non-pc L &       $||v_{seq}-v_{pc-par}||$ &  $||v_{seq}-v_{par}||$  &  pc $\hat{S}$ &  non-pc $\hat{S}$ \\
\midrule
1  &     31.0 &      31.0 &           -- &           -- &    1.000000 &        1.000000 \\
2  &     37.0 &      37.0 &   0.00027766 &   0.00027766 &    1.675676 &        1.675676 \\
3  &     55.0 &      55.0 &  0.000358143 &  0.000324047 &    1.690909 &        1.690909 \\
4  &     61.0 &      65.0 &  0.000359398 &  0.000207032 &    2.032787 &        1.907692 \\
5  &     65.0 &      71.0 &  0.000279559 &  0.000279684 &    2.384615 &        2.183099 \\
6  &     63.0 &      77.0 &  0.000501682 &  0.000352778 &    2.952381 &        2.415584 \\
8  &    137.0 &     133.0 &  0.000498978 &  0.000499069 &    1.810219 &        1.864662 \\
16 &     77.0 &     779.0 &   0.00110429 &   0.00108333 &    6.441558 &        0.636714 \\
32 &     95.0 &    1097.0 &   0.00225308 &   0.00224816 &   10.442105 &        0.904284 \\
64 &     73.0 &    2021.0 &   0.00455414 &   0.00455295 &   27.178082 &        0.981692 \\
\bottomrule
\end{tabular}
\end{table}
\\
\\
There are several things of note about the results in table \ref{compare_table}. First off we see that the normed difference in control between sequential and parallel solution lies in the range from $3\cdot 10^{-4}$ to $4\cdot 10^{-3}$, and that it gets bigger when we increase $N$. Another observation about the norm difference, is that for each $N$, the preconditioned and unpreconditioned solvers seems to produce roughly the same error. 
\\
\\
When we look at the total number of gradient and functional evaluations for the preconditioned and unpreconditioned solvers, we see that there are differences. While it seems to be little to no benefit to use the preconditioner for $N=1,...,8$, it becomes very important for the bigger $N$ values, where number of gradient and functional evaluations seems to explode for the unpreconditioned solver. If one accepts the above solutions as good enough, we see that we for the preconditioned solver get speedup for all decompositions, and that the ideal speedup seems to increase when we increase $N$. We do however see that the ideal speedup for each $N$ is consistently less then half of $N$. Another thing that we notice when looking at the sum of gradient and function evaluations for the preconditioned solver, is that it increases steadily up to $N=8$, and then starts to decline again for higher $N$s. The reason for this is that when we increase the number of decomposed subintervals, we also make the coarse solver in the parareal preconditioner finer. This means that the preconditioner becomes a better approximation of the Hessian, which makes the L-BFGS iteration converge faster. 
\section{Speedup results for $N=1,...,6$}
In the previous section we measured the effectiveness of the parareal preconditioner using the ideal speedup (\ref{ideal S}), for a quite large time step $\Delta t =10^{-3}$, which for $T=1$ translates to $n=1000$ timesteps. From section \ref{ver S sec}, we know that we need more time steps than that to get any benefit from the parallelization. This means that if we want to get actual wall clock time speedup, we need to use smaller time steps, or solve the equation on larger time interval. Let us again write up the version of the example problem that we want to solve for this experiment:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-1)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)=y(t) + v(t) \ t\in(0,1)\\
       	y(0)=1
     \end{array}
   \right. 
\end{align*}
This time we only use the preconditioned solver, but now we also measure the execution time so that it is possible to calculate the speedup. The time measurement is done by running each solver ten times and choosing the lowest time. Also included in the results is the number of function and gradient evaluations L, the ideal speedup $\hat S$, the relative difference in function value and between sequential and parallel solutions and the relative $l^2$-norm difference in control between sequential and parallel solutions. We used the same strict end criteria for the L-BFGS iteration on all solvers:
\begin{align*}
||\nabla J||<10^{-10}
\end{align*}
We solved the penalized problem using only one penalty iteration setting the penalty parameter $\mu=\frac{1}{\Delta t}$. Finally, we ran my experiments using \\$\Delta t =10^{-5}, 0.5(10^{-5}), 10^{-6}$. The results follow in table \ref{actual_speedup_1}-\ref{actual_speedup_3}.
\\
\\
\begin{table}[h]
\centering
\caption{$\Delta t = 10^{-5}$}
\label{actual_speedup_1}
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &          $\frac{J(v_1)-J(v_N)}{J(v_1)}$ &    $\frac{||v_1-v_N||_{l^2}}{||v_1||_{l^2}}$&   speedup &      time (s)\\
\midrule
1: &  43 &  1.000000 &           -- &       -- &  1.000000 &  4.948604 \\
2: &  49 &  1.755102 &  1.49099e-10 &    6e-06 &  1.349916 &  3.665862 \\
3: &  55 &  2.345455 &  6.32732e-10 &  1.1e-05 &  1.732804 &  2.855836 \\
4: &  61 &  2.819672 &  1.44678e-09 &  1.6e-05 &  2.083327 &  2.375337 \\
5: &  67 &  3.208955 &  2.62965e-09 &  2.1e-05 &  2.263943 &  2.185834 \\
6: &  61 &  4.229508 &   4.1827e-09 &  2.7e-05 &  2.877279 &  1.719890 \\
\bottomrule
\end{tabular}
\end{table}
\\
\begin{table}[h]
\centering
\caption{$\Delta t = 0.5(10^{-5})$}
\label{actual_speedup_2}
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &         $\frac{J(v_1)-J(v_N)}{J(v_1)}$ &    $\frac{||v_1-v_N||_{l^2}}{||v_1||_{l^2}}$&speedup &       time (s) \\
\midrule
1: &  45 &  1.000000 &           -- &       -- &  1.000000 &  10.199495 \\
2: &  51 &  1.764706 &  3.76222e-11 &    3e-06 &  1.322927 &   7.709792 \\
3: &  61 &  2.213115 &  1.58185e-10 &    6e-06 &  1.580280 &   6.454233 \\
4: &  71 &  2.535211 &  3.61721e-10 &    8e-06 &  1.802585 &   5.658261 \\
5: &  73 &  3.082192 &  6.57328e-10 &  1.1e-05 &  2.147440 &   4.749607 \\
6: &  69 &  3.913043 &  1.04562e-09 &  1.3e-05 &  2.374901 &   4.294703 \\
\bottomrule
\end{tabular}
\end{table}
\\
\begin{table}[h]
\centering
\caption{$\Delta t = 10^{-6}$}
\label{actual_speedup_3}
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &          $\frac{J(v_1)-J(v_N)}{J(v_1)}$ &    $\frac{||v_1-v_N||_{l^2}}{||v_1||_{l^2}}$&speedup &       time (s)\\
\midrule
1: &   51 &  1.000000 &           -- &       -- &  1.000000 &  56.968187 \\
2: &   97 &  1.051546 &  1.45061e-12 &    1e-06 &  0.871757 &  65.348693 \\
3: &   37 &  4.135135 &  1.19209e-07 &  0.00041 &  2.995382 &  19.018671 \\
4: &   95 &  2.147368 &  1.43499e-11 &    2e-06 &  1.538004 &  37.040329 \\
5: &   83 &  3.072289 &  4.19647e-11 &    5e-06 &  2.098120 &  27.152017 \\
6: &  115 &  2.660870 &  4.21117e-11 &    3e-06 &  1.780952 &  31.987489 \\
\bottomrule
\end{tabular}
\end{table}
\\
When comparing the ideal speedup $\hat S$ with the actual achieved wall clock speedup in the results found in \ref{actual_speedup_1}, we see that the ideal speedup is a lot higher. We do however notice that both ideal and actual speedup increase for larger $N$, which indicates that they might be somewhat related.
\\
\\
For $\Delta t = 0.5(10^{-5})$ and $\Delta t = 10^{-6}$, the results are a bit disappointing, however we do continue to see the a relation between ideal and actual speedup, which justifies the use of $\hat S$ as a measure of effectiveness for our algorithm. In the last table for $N=3$, we notice that the error is high, and that the number of gradient evaluations is low. The reason for this is that the last line search iteration for this specific case failed to satisfy the Wolfe conditions due to numerical errors. The next natural step would then be to do a second penalty iteration with increased penalty parameter and or decreased tolerances, for line search and L-BFGS iterations. This was however not done in this case, since the set-up for the experiment was only to do one penalty iteration.
\section{Speedup results for high number of decompositions}
To properly test the Parareal-based preconditioner, we have tested its use on the example problem on the Abel computer cluster. Using Abel, we are able to test our algorithm for a large number of CPUs. For all experiments the execution time of the sequential parallel algorithms is measured by timing the solvers ten times, and choosing the lowest execution time. All our tests are done using an implicit Euler discretization on example problem (\ref{exs_J}-\ref{exs_E}) with the following parameters:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^{100}v(t)^2dt + \frac{1}{2}(y(T)-1)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)=-0.1y(t) + v(t) \ t\in(0,1)\\
       	y(0)=1
     \end{array}
   \right. 
\end{align*}
We have chosen $T=100$, so that the small time steps $\Delta t$ does not get to small, when $n$ is large. We test the algorithm for $n=600000,1200000$ number of time steps. For $n=600000$, the error between the sequential solution $v_{seq}$ and the exact solution $v_e$ in $L$-norm is $||v_{seq}-v_{e}||\approx10^{-5}$. We have used the  
