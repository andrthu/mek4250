\chapter{Experiments}
In this chapter I will through experiments investigate what speedup one might get by using the algorithm for parallelizing optimal control problems with  time dependent DE constraints in temporal direction, introduced in previous chapters. Unlike the parallel performance of gradient and objective function evaluation, the parallel performance of our overall algorithm is difficult to model. This is due to the fact, that it is difficult to say how many gradient and function evaluations are needed in our line search methods, to reach the different stopping criteria. I am therefore unable to derive any theoretical expected speedup.
\\
\\
In section \ref{analysis sec} I explained that the best way of measuring performance of a parallel algorithm is to compare its execution time to the sequential execution time of the best sequential algorithm. When solving optimal control problems with DE constraints, the runtime of our solution algorithm will depend on how many times we have to evaluate the objective function and its gradient, since these evaluations require either the solution of the state equation or the state and adjoint equations. We know from theory in section \ref{analysis sec} and verification in section \ref{ver S sec}, that the speedup of parallel gradient and function evaluation depends linearly on the number of processes we use. An alternative way of measuring parallel performance is therefore to compare the number gradient and function evaluations in the sequential and parallel algorithms. Let us give this numbers a name:
\begin{align*}
L_s &= \textit{Number of function and gradient evaluations for sequantial algorithm}\\
L_{p_N} &= \textit{Number of function and gradient evaluations for parallel algorithm using N processes}
\end{align*} 
Using these definitions I will define what I will call ideal speedup $\hat{S}$, as the speedup one would expect based on $L_s$ and $L_{p_N}$ and the speedup results we have for function and gradient evaluation:
\begin{align}
\hat S = \frac{NL_s}{L_{p_N}} \label{ideal S}
\end{align}
With $\hat S$, it is possible to say something about the performance of the parallel algorithm without having to time it, or actually run it in parallel. It might also be useful to compare the ideal speedup, to the actual speedup, which we get when we look at execution time.
\section{L-BFGS with and without parareal preconditioner}
In section \ref{pc sec} I introduced the parareal preconditioner, as an approximation to the Hessian. Using this preconditioner in our L-BFGS solver we hope that the number of gradient and function evaluations needed in our algorithm will be smaller than if we do not use it. To test this, let us again write up an example problem to be solved:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-1.5)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)+0.9y(t) = v(t) \ t\in(0,1)\\
       	y(0)=3.2
     \end{array}
   \right. 
\end{align*}
I will first solve this problem without decomposing the time interval, and then solve the decomposed problem using $N=2,3,4,5,6,8,16,32,64$ and penalty parameter $\mu=10^4$. This means that I will only use one penalty iteration, as I have found this to be the most effective way to solve the decomposed problem for this specific problem. For both the penalized and non-penalized problems I use L-BFGS with stop criteria:
\begin{align*}
||\nabla J||_{l^2} <10^{-6}
\end{align*}  
Since The point of this test is to compare the effectiveness of the parareal preconditioner, I solve the decomposed problems with and without it. In the result table below I have included the total number of gradient and function evaluations for the two cases as pc L and non-pc L. I also measured the relative $L^2$ difference between the non-penalized control solution and all the penalized control solutions. The ideal speedup (\ref{ideal S}) is calculated and for precondition and unpreconditioned solvers.
\begin{tabular}{lrrllrr}
\toprule
{}$N$ &  pc L &  non-pc L &       $||v_{seq}-v_{pc-par}||$ &  $||v_{seq}-v_{par}||$  &  pc $\hat{S}$ &  non-pc $\hat{S}$ \\
\midrule
1  &     31.0 &      31.0 &           -- &           -- &    1.000000 &        1.000000 \\
2  &     37.0 &      37.0 &   0.00027766 &   0.00027766 &    1.675676 &        1.675676 \\
3  &     55.0 &      55.0 &  0.000358143 &  0.000324047 &    1.690909 &        1.690909 \\
4  &     61.0 &      65.0 &  0.000359398 &  0.000207032 &    2.032787 &        1.907692 \\
5  &     65.0 &      71.0 &  0.000279559 &  0.000279684 &    2.384615 &        2.183099 \\
6  &     63.0 &      77.0 &  0.000501682 &  0.000352778 &    2.952381 &        2.415584 \\
8  &    137.0 &     133.0 &  0.000498978 &  0.000499069 &    1.810219 &        1.864662 \\
16 &     77.0 &     779.0 &   0.00110429 &   0.00108333 &    6.441558 &        0.636714 \\
32 &     95.0 &    1097.0 &   0.00225308 &   0.00224816 &   10.442105 &        0.904284 \\
64 &     73.0 &    2021.0 &   0.00455414 &   0.00455295 &   27.178082 &        0.981692 \\
\bottomrule
\end{tabular}
\\
\\
There are several things of note about the above results. First off we see that the normed difference in control between sequential and parallel solution lies in the range from $3\cdot 10^{-4}$ to $4\cdot 10^{-3}$, and that it gets bigger when we increase $N$. Also of note about the norm difference, is that for each $N$, the preconditioned and unpreconditioned solvers seems to produce roughly the same error. When we look at the total number of gradient and functional evaluations for the preconditioned and unpreconditioned solvers, we see that there are differences. While it seems to be little to no benefit to use the preconditioner for $N=1,...,8$, it becomes very important for the bigger $N$ values, where number of gradient and functional evaluations seems to explode for the unpreconditioned solver. If one accepts the above solutions as good enough, we see that we for the preconditioned case solver get speedup for all decopmositions, and that the ideal speedup seems to increase when we increase $N$. We do however see that the ideal speedup for each $N$ is consistently less then half of $N$.
\section{Some speedup results}
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^1v(t)^2dt + \frac{1}{2}(y(T)-1)^2 \\
&\left\{
     \begin{array}{lr}
       	y'(t)+y(t) = v(t) \ t\in(0,1)\\
       	y(0)=1
     \end{array}
   \right. 
\end{align*}
$\Delta t = \frac{1}{100000}$
\\
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &          $J(v)-J(v_N)$ &    $||v-v_N||_{l^2}$ &   speedup &      time \\
\midrule
1:  &  43 &  1.000000 &           -- &       -- &  1.000000 &  4.948604 \\
2: &  49 &  1.755102 &  1.49099e-10 &    6e-06 &  1.349916 &  3.665862 \\
3: &  55 &  2.345455 &  6.32732e-10 &  1.1e-05 &  1.732804 &  2.855836 \\
4: &  61 &  2.819672 &  1.44678e-09 &  1.6e-05 &  2.083327 &  2.375337 \\
5: &  67 &  3.208955 &  2.62965e-09 &  2.1e-05 &  2.263943 &  2.185834 \\
6: &  61 &  4.229508 &   4.1827e-09 &  2.7e-05 &  2.877279 &  1.719890 \\
\bottomrule
\end{tabular}
\\
$\Delta t = \frac{1}{200000}$
\\
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &          $J(v)-J(v_N)$ &    $||v-v_N||_{l^2}$&speedup &       time \\
\midrule
1:  &  45 &  1.000000 &           -- &       -- &  1.000000 &  10.199495 \\
2: &  51 &  1.764706 &  3.76222e-11 &    3e-06 &  1.322927 &   7.709792 \\
3: &  61 &  2.213115 &  1.58185e-10 &    6e-06 &  1.580280 &   6.454233 \\
4: &  71 &  2.535211 &  3.61721e-10 &    8e-06 &  1.802585 &   5.658261 \\
5: &  73 &  3.082192 &  6.57328e-10 &  1.1e-05 &  2.147440 &   4.749607 \\
6: &  69 &  3.913043 &  1.04562e-09 &  1.3e-05 &  2.374901 &   4.294703 \\
\bottomrule
\end{tabular}
\\
\\
$\Delta t = \frac{1}{1000000}$
\\
\\
\begin{tabular}{lrrllrr}
\toprule
{} $N$&   L &        $\hat S$ &          $J(v)-J(v_N)$ &    $||v-v_N||_{l^2}$&speedup &       time \\
\midrule
1:  &   51 &  1.000000 &           -- &       -- &  1.000000 &  56.968187 \\
2: &   97 &  1.051546 &  1.45061e-12 &    1e-06 &  0.871757 &  65.348693 \\
3: &   37 &  4.135135 &  1.19209e-07 &  0.00041 &  2.995382 &  19.018671 \\
4: &   95 &  2.147368 &  1.43499e-11 &    2e-06 &  1.538004 &  37.040329 \\
5: &   83 &  3.072289 &  4.19647e-11 &    5e-06 &  2.098120 &  27.152017 \\
6: &  115 &  2.660870 &  4.21117e-11 &    3e-06 &  1.780952 &  31.987489 \\
\bottomrule
\end{tabular}
