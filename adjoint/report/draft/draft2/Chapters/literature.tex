\chapter{Literature review} \label{lit_chap}
The Parareal algorithm was introduced by Lions, Maday and Turinici in \cite{lions2001resolution} as a way to solve differential evolution equations $f(y(t),t)=0$ in parallel. This is done by combining a coarse and fine scheme for discretization in time. To introduce parallelism we first decompose the time domain $I=[0,T]$ into $N$ subintervals $I_i=[T_{i-1},T_i]$. This gives us $N$ equations $f_i(y_i(t),t)=0$ defined on each interval $I_i$. 
\\
\\
The first step of the Parareal algorithm is to solve $f(y(t),t)=0$ sequentially on the entire interval using the coarse scheme. This gives us a solution $Y(t)$ defined on the entire interval, and we can then use $\{\lambda_i^0=Y(T_i)\}_{i=1}^{N-1}$ as initial conditions for the decomposed equations $f_{i+1}(y_{i+1}^0(t),t)=0$. The second step is then to solve these equations in parallel using the fine scheme, which will result in one solution $y_i(t)$ on each interval $I_i$. The idea then, is to utilize the difference $S_i^0=y_i^0(T_{i})-\lambda_i^0$ between coarse and fine solution to repeat this process in an iteration. This is done by propagating the differences $S_i^0$ with the coarse solver, to update the initial conditions for each decomposed equation. These new initial conditions $\lambda_i^1$ can then be used to solve the decomposed equations $f_{i+1}(y_{i+1}^1(t),t)=0$ in parallel with the fine solver. We can then define updated differences $S_i^1=y_i^1(T_{i})-\lambda_i^1$ and repeat the iteration until we are satisfied with the solution. The version of Parareal presented in \cite{lions2001resolution} is most practical for use on linear equations. An alternative version of Parareal is found in \cite{baffico2002parallel}. The formulation given here is equivalent to the one in \cite{lions2001resolution} for linear equations, but is easier applied to non-linear equations.
\\
\\
A lot of the work done on the Parareal algorithm, has been focused on establishing its stability and convergence properties. The stability results are found in \cite{staff2005stability},\cite{maday2007parareal} and \cite{bal2005convergence}, where Staff in \cite{staff2005stability} derives sufficient conditions for the stability of Parareal for autonomous differential equations:
\begin{align}
\frac{\partial y}{\partial t} =\rho y,\quad y(0)=y_0,\quad  \rho<0 \label{autonom E}
\end{align}
while \cite{bal2005convergence} presents more general stability results for parabolic equations. The stability of Parareal applied to hyperbolic equations is a more difficult question as is explained in \cite{dai2013stable}. The convergence of Parareal is studied in \cite{lions2001resolution},\cite{bal2005convergence},\cite{gander2007analysis} and \cite{gander2007superlinear}. In \cite{lions2001resolution} Lions, Maday and Turinici show that $k$ iterations of the Parareal algorithm applied to equation (\ref{autonom E}) gives $\mathcal{O}(\Delta T^{k+1})$ order of accuracy if we use a coarse solver with order one accuracy. This result is extended in \cite{bal2005convergence} to more general equations, and the order of accuracy is shown to be improved to $\mathcal{O}(\Delta T^{p(k+1)})$ when the coarse solver has order $p$. \cite{gander2007analysis,gander2007superlinear} return to analysis of equation (\ref{autonom E}). Instead of looking at a fixed number of iterations $k$, Gander and Vandewalle show convergence properties for the Parareal algorithm as the iteration count increases.
\\
\\
Different applications of the algorithm exists for example on the Navier-Stokes problem\cite{fischer2005parareal}, for molecular-dynamics simulations\cite{baffico2002parallel}, on stochastic ordinary differential equations\cite{bal2003parallelization}, reservoir simulations \cite{garrido2005convergent}, fluid-structure\cite{farhat2003time}, or on the American put\cite{bal2002parareal}.
\\
\\
Since the Parareal algorithm is an iterative procedure, a stopping criteria for when to terminate the iteration is required. This is done in \cite{lepsa2010efficient}, where an error control mechanism for the Parareal algorithm to limit the number of Parareal iterations is introduced. The stopping criteria that they propose, is to stop the algorithm when the difference between coarse and fine solution at the subinterval boundaries $T_i$ are similar to the expected global error of the fine solver. One of many challenges associated with parallel computing is partitioning and load bearing. This issue also arises in the Parareal algorithm, where the difficulties originates from the following observation: After $k$ iterations of the Parareal algorithm, the solution in  the $k$ first subintervals is equal to the sequential solution. This means that after $k$ iterations, the the $k$-th process becomes idle. How to tackle this issue is described in \cite{aubanel2011scheduling}, where the authors also present a practical Parareal algorithm. 
\\
\\
As mentioned in \cite{gander2007superlinear} the Parareal algorithm is not the first attempt to parallelize the solution of time dependent differential equation in temporal direction, since Nievergelt already in 1964 proposed a procedure in \cite{nievergelt1964parallel} that eventually led to the so called multiple shooting methods. In \cite{gander2007superlinear} the authors also explain why the Parareal algorithm can be characterized as both a multiple shooting method and a multigrid method. A historic overview of the development of parallel in time algorithms can be found in \cite{gander201550}. Here we can also find presentations for the different strategies for parallelizing time dependent differential equations. One such strategy are the already mentioned multiple shooting methods \cite{nievergelt1964parallel,bellen1989parallel}, which also include the Parareal algorithm. What characterizes such methods is that they only decompose the time domain. This separates the multiple shooting methods from waveform relaxation methods\cite{lelarasmee1982waveform,gander1996overlapping}, where the spatial domain is decomposed through time. The difference in these decomposition techniques is illustrated in figure \ref{fig:fig}. Other strategies presented are multigrid \cite{hackbusch1985parabolic,lubich1987multi,horton1995space} methods and direct solvers in space-time\cite{miranker1967parallel,maday2008parallelization,guttel2013parallel}.
\\
\\
\begin{figure}[h!]
\centering
\begin{subfigure}{.6\textwidth}
  %\centering
  \includegraphics[width=.8\linewidth]{MSM.png}
  \caption{Multiple shooting decomposition}
  \label{fig:sfig1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  %\centering
  \includegraphics[width=.8\linewidth]{DDMinST.png}
  \caption{Waveform relaxation decomposition}
  \label{fig:sfig2}
\end{subfigure}
\caption{Different decomposition techniques for parallel in time algorithms. Both figures lifted from \cite{gander201550}.}
\label{fig:fig}
\end{figure}
\noindent
\\
\\
The Parareal algorithm parallelizes the solution process of time dependent differential equations. In \cite{maday2002parareal} extends Parareal, so that it can be used on optimal control problems with time dependent differential equation constraints. The problem looked at in \cite{maday2002parareal}, is: 
\begin{align*}
&\min_{y,u}J(y,u) = \frac{1}{2}\int_0^T||u(t)||_U^2dt + \frac{\alpha}{2}||y(T)-y^T||^2,\\
&\left\{
     \begin{array}{lr}
       	\frac{\partial y}{\partial t}+Ay = Bu\\
       	   y(0)=y_0
     \end{array}
   \right.
\end{align*}
We introduce parallelism in the same ways as we did for the differential equation case, by decomposing the time domain and equation. The continuity of the state equation between subintervals is enforced by adding a penalty term to the objective function $J$, that penalizes jumps in the solution of the state equation. This is based on the penalty method for constrained optimization described in \cite{nocedal2006numerical}. In \cite{maday2002parareal} they use quadratic penalty terms, which leads to the following modified objective function:
\begin{align}
J_{\mu}(y,u,\lambda_1,..,\lambda_{N-1})=J(y,u) +\frac{\mu }{2}\sum_{i=1}^{N-1}(y_i(T_i)-\lambda_i)^2 \label{lit_pen}
\end{align} 
Here $\lambda_i$ is the initial condition of the decomposed state equation $f(y_{i+1}(t),t)=0$. Solving both the original and modified optimal control problems require us to repeatedly evaluate the objective function and its gradient. Every time we do this we need to solve either the state equation, or the state equation and its adjoint. Decomposing the time interval allows us to solve these equations in parallel, and if we solve the modified problem with a sufficiently large penalty, we will end up with the solution of the original problem. One does not necessarily need a coarse level to make this parallel framework to produce a speedup. This is illustrated in \cite{rao2016time}, where the authors create a time-parallel algorithm for 4d variational data assimilation. The penalization of the objective function was done using the augmented Lagrangian approach, which is a variation of the penalization done in (\ref{lit_pen}). The experiments conducted in \cite{rao2016time} yielded limited success, since some speedup was achieved. However, the speedup was only attainable when using a parallel/sequential hybrid method, that first solved the penalized problem in parallel, for small penalty terms, and then used the parallel solution as an initial guess for the sequential algorithm. 
\\
\\
In \cite{maday2002parareal} the Parareal algorithm is reformulated as a preconditioner for the algebraic system that arises when we set $\lambda_i=y_{i}(T_i)$. Using this formulation the authors derive a preconditioner for the optimization algorithm that solves the penalized optimal control problem. The preconditioner they propose involves both a backward and a forward solve of the linearised state equation with a coarse solver, and it is to be applied to the $\lambda$ part of the gradient of $J_{\mu}$. The hope is that this Parareal based preconditioner will decrease the number of function and gradient evaluations needed for the optimization algorithm to converge, and the results in \cite{maday2002parareal} do indeed look promising. In an experiment with 100 cores, the authors report a theoretical speedup of around 400, which is superlinear. They do however believe that this result is due to properties of the example they choose, and do not expect superlinerar speedup as a general rule. 
\\
\\
The optimal control setting can also be used to modify the original Parareal algorithm. One example is \cite{chen2015adjoint}, where the preconditioner for the optimal control problem from \cite{maday2002parareal}  is used as in a modified Parareal algorithm to stabilize Parareal hyperbolic equations. The adjoint based Parareal algorithm is proposed in \cite{rao2014adjoint}. In this paper the authors address the bottleneck for speedup produced by having to repeatedly apply the coarse solver. This especially becomes a problem when the number of decompositions in time grows, while the problem size stays constant. The solution proposed in \cite{rao2014adjoint} is to only use the coarse solver once to get an initial guess for the intermediate initial conditions, and thereafter improve this initial guess by minimizing a functional of type (\ref{lit_pen}) using an optimization algorithm. The optimization steps can be done completely in parallel, and the scalability of the adjoint based Parareal algorithm is therefore a lot better than the original.
\\
\\
As mentioned the Parareal preconditioner presented in \cite{maday2002parareal} only affects the penalty terms. A more advanced preconditioner is derived in \cite{ulbrich2015preconditioners}, where the Parareal preconditioner is incorporated into a preconditioner for a control problem with time-dependent partial differential equation constraints and inequality constraints.
