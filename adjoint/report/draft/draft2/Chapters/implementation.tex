\chapter{Implementation}
In previous chapters we have derived the adjoint equation and the gradient for our example optimal control problem with ODE constraints. We have also explained how we can parallelize the solving of the state and adjoint equations using the penalty method, and we have introduced a prconditioner for our optimization algorithms based on the parareal scheme. Before we can start to test our parallel algorithm, we need to discretize the time domain, the equations, the objective function and its gradient. 
\\
\\
We discretize $I=[0,T]$ by dividing it into $n$ parts of length $\Delta t=\frac{T}{n}$, and settting $t_k=k\Delta t$. This gives us a sequence $I_{\Delta t}=\{t_k\}_{k=0}^{n}$ as a discrete representation of the interval $I$. Using $I_{\Delta t}$ we can start to discretize our example problem.
\section{Discretizing the non-penalized example problem}
Let us remember our example state equation (\ref{exs_E}) and objective function (\ref{exs_J}) 
\begin{align}
\left\{
     \begin{array}{lr}
       	y'(t)=a y(t) +v(t), \ t \in (0,T)\\
       	   y(0)=y_0
     \end{array}
   \right. \label{equation}
\end{align}
\begin{align}
J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2
\label{problem}
\end{align}
We know that the the reduced gradient of (\ref{problem}) is:
\begin{align}
\nabla\hat{J}(v) = v(t)+p(t) \label{gradiant}
\end{align}
where $p$ is the solution of the adjoint equation:
\begin{align}   
  \left\{
     \begin{array}{lr}
	-p'(t) = p(t) \\
	p(T) = \alpha( y(T)-y^T)     \
	\end{array}
   \right. \label{adjoint}
\end{align}
We now want to discretize (\ref{equation}-\ref{adjoint}), so we can solve the problem numerically. What we particularly want, is an expression for the gradient. 
\subsection{Finite difference schemes for state and adjoint equations}
Before I state what the numerical gradient will be for the implicit end explicit Euler schemes, I will write up these schemes for our equations (\ref{equation}) and (\ref{adjoint}). First the implicit for the state equation:
\begin{align}
\frac{y^k-y^{k-1}}{\Delta t} &= a y^{k} + v^{k} \\
(1-a\Delta t)y^{k} &= y^{k-1} +\Delta t v^{k} \\
y^k &=\frac{y^{k-1} +\Delta t v^{k}}{1-a\Delta t} \label{I_state}
\end{align}
Implicit Euler for the adjoint equation:
\begin{align}
-\frac{p^k-p^{k-1}}{\Delta t} -a p^{k-1} &=0 \\
(1-\Delta ta)p^{k-1}&=p^k \\
p^{k-1} &= \frac{p^k}{1-\Delta ta} \label{I_adjoint}
\end{align}
The explicit scheme for the state equation reads:
\begin{align}
\frac{y^{k+1}-y^{k}}{\Delta t} &= a y^{k} + v^{k} \\
y^{k+1}&=(1 +\Delta ta) y^{k} + \Delta t v^{k}\label{E_state}
\end{align} 
and the for the adjoint we have:
\begin{align}
-\frac{p^k-p^{k-1}}{\Delta t} -a p^{k} &=0 \\
p^{k-1} &=p^k(1 +\Delta ta)\label{E_adjoint}
\end{align}
With these formulas in mind lets find the numerical gradient of our example problem.
\subsection{Numerical gradient}
Now that we have discretized both the domain and the equations, we are forced to also evaluate the objective function (\ref{problem}) numerically. Since integration is involved in (\ref{problem}), we need to choose a numerical integration rule. The easiest way of integrating a vectorized function $x_{\Delta t} = (x_0,x_1,...,x_n)$ is the so called trapezoid rule defined as:
\begin{align}
trapz(x_{\Delta t})=\Delta t\frac{x_0+x_n}{2} +\sum_{i=1}^{n-1} \Delta t x_i \label{trap}
\end{align}
Using the trapezoid rule, we can define a discretized version of the objective function (\ref{problem}):
\begin{align}
\hat J_{\Delta t}(v_{\Delta t})&=\frac{1}{2} trapz(v_{\Delta t}^2)+ \frac{\alpha}{2}(y_n-y^T)^2 \\
&=\Delta t\frac{v_0^2+v_n^2}{4} + \frac{1}{2}\sum_{i=1}^{n-1} \Delta t v_i^2 + \frac{1}{2}(y_n-y^T)^2 \label{disc f}
\end{align}
The hope is now that the gradient of the above defined numerical objective function is somehow related to the gradient of the continuous objective function stated in (\ref{gradiant}). The next result shows that this is indeed the case. 
\begin{theorem}
If the implicit Euler finite difference scheme is used to evaluate the numerical objective function (\ref{disc f}), the gradient $\nabla \hat J_{\Delta t}$ of (\ref{disc f}) will be given as:
\begin{align}
\nabla \hat J_{\Delta t}(v_{\Delta t}) = Mv_{\Delta t} + Bp_{\Delta t} \label{num_grad}
\end{align}
where $M$ and $B$ are the matrices:
\begin{align*}
M=\left[ \begin{array}{cccc}
   \frac{1}{2}\Delta t & 0 & \cdots & 0 \\  
   0& \Delta t & 0 & \cdots \\ 
   0 &0 & \Delta t  & \cdots \\
   0 &\cdots &0 & \frac{1}{2}\Delta t   \\
   \end{array}  \right] 
,B = \left[ \begin{array}{cccc}
   0& 0 & \cdots & 0 \\  
   \Delta t& 0 & 0 & \cdots \\ 
   0 & \Delta t& 0  & \cdots \\
   0 &\cdots & \Delta t& 0   \\
   \end{array}  \right] 
\end{align*}
If one instead uses the explicit Euler finite difference scheme on the differential equations, the gradient will instead look like:
\begin{align*}
\nabla \hat J_{\Delta t}(v_{\Delta t}) = Mv_{\Delta t} + B^*p_{\Delta t}
\end{align*}
\end{theorem}
\begin{proof}
Let us start with the $Mv$ term of the gradient. This term comes from the integral $\int_0^T v(t)^2dt$, which we evaluate using the trapezoid rule. We can actually rewrite this term so that it depends on $M$:
\begin{align*}
trapz(v_{\Delta t}^2)=  \Delta t\frac{v_0^2+v_n^2}{2} + \sum_{i=1}^{n-1} \Delta t v_i^2 = v^*Mv
\end{align*} 
The function $f(v)=\frac{1}{2} v^*Mv$ obviously has $Mv$ as gradient. The second term of the gradient comes from the second term of the functional, namely $g(v)=\frac{1}{2}(y^n -y^T)^2$. To differentiate $g$ with respect to the i'th component of $v$, we will apply the chain rule multiple times. Lets first demonstrate by calculating $\frac{\partial g}{\partial v_n}$, in a setting where we have used implicit euler to solve the equations:
\begin{align*}
\frac{\partial g(v)}{\partial v_n} &= \frac{\partial g(v)}{\partial y_n}\frac{\partial y_n}{\partial v_n} = \alpha(y_n -y^T)\frac{\partial y_n}{\partial v_n}\\
&= \alpha(y_n -y^T)\frac{\Delta t}{1-a\Delta t}
\end{align*}
To get to the second line I used the implicit Euler formula (\ref{I_state}). If we then look at the scheme (\ref{I_adjoint}) for the adjoint equation, we see that:
\begin{align*}
\alpha(y_n -y^T)\frac{\Delta t}{1-a\Delta t} = \Delta t\frac{p_n}{1-a\Delta t} = \Delta t p_{n-1}
\end{align*} 
Using the same approach, we can find an expression for $\frac{\partial g(v)}{\partial v_i}$: 
\begin{align*}
\frac{\partial g(v)}{\partial v_i} &= \alpha(y_n -y^T) (\prod_{k=i+1}^{n}\frac{\partial y_{k}}{\partial y_{k-1}}) \frac{\partial y_i}{\partial v_{i}} = \frac{p_n}{(1-a\Delta t)^{n-i}}\frac{\Delta t}{1-a\Delta t} \\
&= \frac{p_n\Delta t}{(1-a\Delta t)^{n-i+1}}=\Delta t p_{i-1}
\end{align*}
since $v_0$ is not part of the scheme, $\frac{\partial g(v)}{\partial v_0}=0$. If we now write up the gradient of $g(v)$ on matrix form, you get $\nabla g(v) = Bp$. The expression for the gradient in the case where we use the explicit Euler scheme can be found in a similar fashion. 
\end{proof}

\section{Discretizing the decomposed time-domain}
Decomposing the time interval $I=[0,T]$ into $N$ equally sized subintervals $I_i=[T_i,T_{i+1}]$, and solving the state and adjoint equations separately on each subinterval, is what allows our algorithm to be run in parallel. Decomposing $I$ is simple in the continuous case, however we are solving these equations numerically, and in the discrete case, partitioning $I$ is more involved. To explain how we decompose $I$ in the discrete case, lets look at how to do it for a general differential equation $F$:
\begin{align*}
\left\{
     \begin{array}{lr}
		F(y(t),v(t))=0 \	\textit{For $t \in [0,T]$} \\
		y(0)=y_0
	\end{array}
   \right.	
\end{align*} 
We then decompose $I$, and assume that we have $N-1$ intermediate initial conditions $\{\lambda_i\}_{i=1}^{N-1}$, such that we get a solvable equation on each subinterval:
\begin{align*}
\left\{
     \begin{array}{lr}
		F^i(y_i(t),v(t))=0 \	\textit{For $t \in [T_i,T_{i+1}]$} \\
		y(T_i)=\lambda_i
	\end{array}
   \right.	
\end{align*} 
Now lets look at what happens when we discretize $I$. Lets divide $I$ into $n$ parts of length $\Delta t=\frac{T}{n}$, and set $t_k=k\Delta t$. This gives us a sequence $I_{\Delta t}=\{t_k\}_{k=0}^{n}$ as a discrete representation of the interval $I$. Using some finite difference scheme, we can transform the differential equation $F$ into a difference equation $F_{\Delta t}$:
    \begin{align*}
\left\{
     \begin{array}{lr}
		F_{\Delta t}(y^k,v(t_k))=0 \	\textit{For $k=1,...,n$} \\
		y^0=y_0
	\end{array}
   \right.	
\end{align*} 
The next step is to decompose the discrete interval $I_{\Delta t}$ into $N$ discrete subintervals. This is simply done by extracting a subsequence $\{t_{k_i}\}_{i=0}^N\subset I_{\Delta t}$ where $t_{k_0}=t_0$ and $t_{k_N}=t_n$. This results in $N$ sequences on the form $I_{\Delta t}^i=\{t_{k_i},t_{k_i+1},...,t_{k_{i+1}}\}$, and if we assume, as we did in the continuous case, that we have some intermediate initial conditions $\{\lambda_i\}_{i=1}^{N-1}$, we can solve $F_{\Delta t}$ separately on each $I_{\Delta t}^i$: 
\begin{align*}
\left\{
     \begin{array}{lr}
		F_{\Delta t}^i(y_i(t_k),v(t_k))=0 \	\textit{For $k \in \{k_i,k_i+1,...,k_{i+1} \}$} \\
		y(t_{k_i})=\lambda_i
	\end{array}
   \right.	
\end{align*} 
There is one minor issue with decomposing $I_{\Delta t}$, which I did not mention above, and that has to do with the choice of the subsequence $\{t_{k_i}\}_{i=0}^N$. In theory, one could of course freely chose any subsequence of $I_{\Delta t}$, but we generally want the difference $t_{k_i} -t_{k_{i+1}}$ to be constant for all $i$. This is however not always possible, since there is no guarantee that $n$ is divisible by $N$.
\subsection{Partitioning}
The general rule for partitioning a task between $N$ processes, is to distribute the work of the task as evenly as possible. The task in the above setting is solving $ F_{\Delta t}$, and the work to be distributed, is the computations required to move the solution from one time step to the next for all time steps. Since there are $n$ time steps, we should be able to say that the main task of solving $F_{\Delta t}$ can be divided into $n$ subtasks, and it is these $n$ tasks that we want to distribute between the $N$ processes. Now deciding how many subtasks each process should get is quite simple. Start with defining the numbers:
\begin{align*}
q &= \lfloor \frac{n}{N}\rfloor \\
r &= N \mod n
\end{align*} 
Then we give each process $q$ tasks, and then add one task to $r$ processes. To which processes you give the extra task does not really matter, but the most straightforward way of doing it is just to give the first $r$ processes the extra task. I however chose to look at the distributing problem slightly different, by instead of trying to distribute the $n$ tasks, looking at how to evenly divide the $n+1$ points $\{t_k\}_{k=0}^{n}$ among the $N$ processes. Again lets define $q$ and $r$ as:
\begin{align*}
q &= \lfloor \frac{n+1}{N}\rfloor \\
r &= N \mod n+1
\end{align*}
Every process now gets $q$ points, but due to overlap every process gets an extra point excluding the first process. Then the remaining $r$ points are given to the first $r$ processes. This allows me to define the sequence $\{k_{i}\}_{i=0}^N$ recursively as follows:
\begin{align*}
k_{i+1} = k_i + q+\delta_{i\neq0}+\delta_{i<r}
\end{align*}
Here the $\delta$s are conditional functions defined as:
\begin{align*}
\delta_{S}=\left\{
     \begin{array}{lr}
		1 \ S=\text{True} \\
		0 \ S=\text{False}
	\end{array}
   \right.	
\end{align*} 
We now have a way of decomposing the discrete time interval, and therefore also a way of partitioning the finite difference solver of the state equation in temporal direction. However both when we want try to find the gradient in a penalized optimal control problem, or when we just want to parallelize solving a differential equation using the parareal scheme, some communication between the processes is required. 
\subsection{Numerical gradient of penalized example problem}
Let us restate the decomposed example ODE, and the penalized objective function defined in (\ref{decomp_E}-\ref{penalty_func})
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y^i(t)+a y^i(t) = v(t) \ t\in(T_{i-1},T_{i})\\
       	y^i(T_{i-1})=\lambda_{i-1}
     \end{array}
   \right. \label{decomp_E2}
\end{align}
\begin{align}
\hat J_{\mu}(v,\Lambda) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{penalty_func2}
\end{align}
Let us also remember the gradient of (\ref{penalty_func2}) stated in (\ref{penalty grad})
\begin{align}
\hat J_{\mu}'(v,\lambda) = (v+p,p_{2}(T_1) -p_{1}(T_1),..., p_{N}(T_{N-1}) -p_{N}(T_{N-1})) \label{penalty grad2}
\end{align}
We now want to write up the numerical version of (\ref{penalty_func2}) and (\ref{penalty grad2}), as we did for the non-penalized problem. Discretizing the decomposed ODEs is straight forward, however the solution of the state and adjoint equations now consists of independent solution $y_{\Delta t}^i$ and $p_{\Delta t}^i$ on each subinterval $I_{\Delta t}^i$, where 
\begin{align*}
y_{\Delta t}^i &= (y_{k_{i-1}}^i,y_{k_{i-1}+1}^i,...,y_{k_{i}}^i) \ \text{and} \\
p_{\Delta t}^i &= (p_{k_{i-1}}^i,p_{k_{i-1}+1}^i,...,p_{k_{i}}^i)
\end{align*} 
One problem with $y_{\Delta t}^i$ and $p_{\Delta t}^i$ existing independently on each interval, is that we get an overlap on all the subinterval boundaries, which have the potential of complicating the penalized numerical objective function and its gradient. It turns out that for our example problem this problem only arises in the gradient. We can therefore quite simply write up the penalized numerical objective function:
\begin{align}
\hat J_{\mu,\Delta t}(v_{\Delta t},\Lambda)&=\frac{1}{2} trapz(v_{\Delta t}^2)+ \frac{\alpha}{2}(y_n^{N}-y^T)^2 +\frac{\mu }{2}\sum_{i=1}^{N} (y_{k_i}^{i}-\lambda_i)^2\\
&=\Delta t\frac{v_0^2+v_n^2}{4} + \frac{1}{2}\sum_{i=1}^{n-1} \Delta t v_i^2 + \frac{\alpha}{2}(y_n^N-y^T)^2+\frac{\mu }{2}\sum_{i=1}^{N} (y_{k_i}^{i}-\lambda_i)^2 \label{pen disc f}
\end{align}
\begin{theorem}
The gradient of (\ref{pen disc f}), $\hat J_{\mu,\Delta t}:\mathbb{R}^{N+m}\rightarrow\mathbb{R}$ consists of two parts. The second part $ \nabla\hat J_{\mu,\Delta t}(\Lambda)\in\mathbb{R}^{m-1}$ dealing with the penalty part of the control is independent of the choice of finite difference scheme, and is given by:
\begin{align}
\nabla\hat J_{\mu,\Delta t}(\Lambda) = (p_{k_1}^{2}-p_{k_1}^{1},p_{k_2}^{3}-p_{k_2}^{2},...,p_{N}^{k_{N-1}}-p_{N-1}^{k_{N-1}}) \label{num_pen_grad_lam}
\end{align} 
The first part $ \nabla\hat J_{\mu,\Delta t}(v_{\Delta t})\in\mathbb{R}^{N+1}$, which is connected to $v_{\Delta t}$, depend on the finite difference scheme used to discretize the adjoint and state equations. If we use the Implicit Euler scheme to evaluate (\ref{pen disc f}), the $v_{\Delta t}$ part of the gradient will be:
\begin{align}
\nabla\hat J_{\mu,\Delta t}(v_{\Delta t})= Mv_{\Delta t} + (B^1p_{\Delta t}^1,B^2p_{\Delta t}^2,...,B^Np_{\Delta t}^N) \label{num_pen_grad_v}
\end{align}
where $M\in \mathbb{R}^{(n+1)\times (n+1)}$ and $B^i\in \mathbb{R}^{n^i\times (n^i-1)}$ are the matrices defined below, and $n^i=k_i-k_{i-1}$ are the length of vector $p_{\Delta t}$.
\begin{align*}
M=\left[ \begin{array}{cccc}
   \frac{1}{2}\Delta t & 0 & \cdots & 0 \\  
   0& \Delta t & 0 & \cdots \\ 
   0 &0 & \Delta t  & \cdots \\
   0 &\cdots &0 & \frac{1}{2}\Delta t   \\
   \end{array}  \right] 
,B^i = \left[ \begin{array}{cccc}  
   \Delta t& 0 & \cdots & 0 \\ 
   0 & \Delta t& 0  & \cdots \\
   0 &\cdots & \Delta t& 0   \\
   \end{array}  \right] 
\end{align*}
If one instead uses the explicit Euler finite difference scheme on the differential equations, the gradient will instead look like:
\begin{align}
\nabla\hat J_{\mu,\Delta t}(v_{\Delta t})= Mv_{\Delta t} + (\bar B^1p_{\Delta t}^1,\bar B^2p_{\Delta t}^2,...,\bar B^Np_{\Delta t}^N)
\end{align}
where $\bar B^i\in \mathbb{R}^{n^i\times (n^i-1)}$ is the matrix:
\begin{align*}
\bar B^i = \left[ \begin{array}{cccc}  
   0 &\Delta t& 0 & \cdots  \\ 
   0 &0 & \Delta t  & \cdots \\
   0 &\cdots &  0& \Delta t   \\
   \end{array}  \right] 
\end{align*}
\end{theorem}
\begin{proof}
Let us begin with the $\Lambda$ part of the gradient. We find each component by differentiating $\hat J_{\mu,\Delta t} $ with respect to $\lambda_i$, for $i=1,...,N-1$. It turns out there are two cases, namely $i=N-1$ and $i\neq N-1$, these cases are however quite similar, so I will only do the $i\neq N-1$ case. For each $i=1,...,N-2$, there are only two terms in $\hat J_{\mu,\Delta t} $ that depend on $\lambda_i$, and these are $\lambda_i$ it self and $y_{k_{i+1}}^{i+1}$. With this in mind lets start to differentiate $\hat J_{\mu,\Delta t} $.
\begin{align*}
\frac{\partial\hat J_{\mu,\Delta t}}{\partial \lambda_i}(v_{\Delta t},\Lambda) &=-\mu (y_{k_i}^i-\lambda_i) +\mu(y_{k_{i+1}}^{i+1}-\lambda_{i+1})\frac{\partial y_{k_{i+1}}^{i+1}}{\partial \lambda_i}  \\
&=\mu(y_{k_{i+1}}^{i+1}-\lambda_{i+1})(\frac{1}{1-a\Delta t})^{k_{i+1}-k_i} -\mu (y_{k_i}^i-\lambda_i)
\end{align*}
To get the $(\frac{1}{1-a\Delta t})^{k_{i+1}-k_i}$ term I used the chain rule on $\frac{\partial y_{k_{i+1}}^{i+1}}{\partial \lambda_i}$ and the implicit Euler scheme for our particular equation given in (\ref{I_state}). The next step is done by noticing that the terms $\mu (y_{k_i}^i-\lambda_i)$ and $\mu(y_{k_{i+1}}^{i+1}-\lambda_{i+1})$ are the initial conditions of the $i$th and $i+1$th adjoint equations, which means that $\mu (y_{k_i}^i-\lambda_i)=p_{k_i}^{i}$ and $\mu(y_{k_{i+1}}^{i+1}-\lambda_{i+1})=p_{k_{i+1}}^{i+1}$. Inserting this we get:
\begin{align*}
\frac{\partial\hat J_{\mu,\Delta t}}{\partial \lambda_i}(v_{\Delta t},\Lambda) &=p_{k_{i+1}}^{i+1}(\frac{1}{1-a\Delta t})^{k_{i+1}-k_i} -p_{k_i}^{i} \\
&= p_{k_{i}}^{i+1}-p_{k_i}^{i}
\end{align*}
The last step is done by utilizing the implicit Euler scheme for our adjoint equation (\ref{I_adjoint}).
\\
\\
The $v_{\Delta t}$ part of the gradient is almost equal to the non-penalized gradient, the only difference being that the adjoint now is defined separately on each subinterval and not on the entire time interval $[0,T]$. We can again divide the functional (\ref{pen disc f}) into two parts, the integral over $v_{\Delta t}$ , $f(v_{\Delta t})=\frac{1}{2} trapz(v_{\Delta t}^2)=v_{\Delta t}^*Mv_{\Delta t}$ and 
\begin{align*}
g(v_{\Delta t}) = \frac{\alpha}{2}(y_n^N-y^T)^2+\frac{\mu }{2}\sum_{i=1}^{N} (y_{k_i}^{i}-\lambda_i)^2
\end{align*} 
As for the non-penalized gradient, the derivative of the $f$ term is quite easily seen to be $Mv_{\Delta t}$, the problems start when we want to differentiate $g$ with respect to a specific component $v_k$ in $v_{\Delta t}$. If we are using the implicit Euler scheme to dicretize the state and adjoint equations, the $k$th component of $v_{\Delta t}$ only affects the solution of one of the $i$ state equations. If $k\in \{k_{i-1}+1,k_{i-1}+2,...,k_{i}\}$, $v_k$ is used to find $y_{\Delta t}^i$, which means that the only term in $g$, that depend on $v_k$, is  $\frac{\mu }{2}(y_{k_i}^{i}-\lambda_i)^2$ if $i\neq N$, or $\frac{1}{2}(y_n^N-y^T)^2$ if $i=N$. If we now assume that $i\neq N$ and $k\in \{k_{i-1}+1,k_{i-1}+2,...,k_{i}\}$, we can differentiate $g$ with respect to $v_k$:
\begin{align*}
\frac{\partial g}{\partial v_k} &=\mu( y_{k_i}^{i}-\lambda_i) (\prod_{l=k+1}^{k_{i+1}}\frac{\partial y_{l}}{\partial y_{l-1}}) \frac{\partial y_k}{\partial v_{k}} = \frac{p_{k_i}^i}{(1-a\Delta t)^{k_{i}-k}}\frac{\Delta t}{1-a\Delta t} \\
&= \frac{p_{k_i}^i\Delta t}{(1-a\Delta t)^{k_i-k+1}}=\Delta t p_{k-1}^i
\end{align*}
The numerical gradient restricted to $ \{k_{i-1}+1,k_{i-1}+2,...,k_{i}\}$, will then be $B^ip_{\Delta t}^i$, which exactly what we claimed.
\end{proof}
\section{Communication without shared memory}
If we assume that we are solving our optimal control problem in parallel on processes that do not share any memory, there will have to be communication between the processes. The parts of the algorithm that we are parallelizing using the penalty method, is the evaluation of the objective function and its gradient for a given control variable $v$. The function evaluation requires us to solve the state equation, while we to calculate the gradient need both the solution of the state and adjoint equation. the required communication between the processes are different for function and gradient evaluation, so I will describe them separately. However they both share the same starting point, which I explain below.
\\
\\
Lets assume that we have $N$ processes, which we name $\{P_{i}\}_{i=0}^{N-1}$. Then assume that each process $P_i$ only knows the parts of the control that are required for the process to be able to solve the state equation and to locally evaluate the objective function. This also includes the the control variables $\{\lambda_i\}_{i=1}^{N-1}$ that originates from the penalty terms in the functional. After each $P_i$ then have solved their part of the state equation, they all have the following data stored locally:
\begin{align*}
&\textit{Control variable: } \ v_i \\
&\textit{Penalty control variable: } \ \lambda_i \\
&\textit{Solution to local state equation: } \ y^{i+1} =\{y_j^i\}_{j=k_{i-1}}^{ k_{i}}
\end{align*}
Using this data we should be able to evaluate the penalized objective function, or to calculate its gradient.
\subsection{Communication in functional evaluation}
The penalized objective function consists of two parts:
\begin{align*}
\hat J_{\mu}(v,\lambda) = \hat{J}(y(v),v) + \frac{\mu}{2}\sum_{j=1}^{N-1}(y^{j}(T_j)-\lambda_j)^2
\end{align*}
Lets begin with the penalty term. Since each process $P_i$ only have $\lambda_i$ and $y^{i+1}(T_{i+1})$ stored locally. This means that to calculate all penalty terms the processes will have to send either $\lambda_i$ or $y^{i+1}(T_i+1)$ to one of it neighbours. For example $P_i$ could send $\lambda_i$ to $P_{i-1}$ for $i=1,...,N-1$:
\begin{align*}
P_0\overset{\lambda_1}{\longleftarrow}P_1\overset{\lambda_2}{\longleftarrow}P_2 \overset{\lambda_3}{\longleftarrow}\cdots \overset{\lambda_{N-2}}{\longleftarrow}P_{N-2}\overset{\lambda_{N-1}}{\longleftarrow}P_{N-1}
\end{align*} 
For the evaluation of $ \hat{J}(y(v),v)$. lets assume that there exists functions $\hat{J}^i(y^{i+1}(v_i),v_i)$, such that:
\begin{align*}
\hat{J}(y(v),v)= \sum_{j=0}^{N-1}\hat{J}^j(y^j(v_j),v_j)
\end{align*}
If this is the case we can evaluate each part of the objective function locally, and then get the global $\hat{J}_{\mu}$ by doing one summation reduction. The penalized objective function evaluation algorithm will then look like the following:
\begin{align*}
1. \ &P_i \ \textit{calculates} \ y^{i+1} \ \forall i \\
2. \ &P_{i-1}\overset{\lambda_i}{\longleftarrow}P_{i} \ \textit{for} \ i=1,..,N-1 \\
3. \ &\hat{J}_{\mu}^i(y^{i+1}(v_i),v_i) = \hat{J}^i(y^{i+1}(v_i),v_i) + \frac{\mu}{2}(y^{i+1}(T_{i+1})-\lambda_{i+1})^2 \ \textit{for} \ i=0,..,N-2 \\
&\hat{J}_{\mu}^{N-1}(y^{N}(v_{N-1}),v_{N-1})= \hat{J}^{N-1}(y^{N}(v_{N-1}),v_{N-1}) \\
4. \ &\hat{J}_{\mu}(y(u),u) = \textbf{Reduce}(\hat{J}_{\mu}^i,+)
\end{align*}
\subsection{Communication in gradient computation}
The gradient of the penalized optimal control problem looks like the following:
\begin{align*}
\nabla \hat J_{\mu}(v,\lambda) = (J_v(y(v),v)-B^*p,\{p_{i+1}(T_i) - p_{i}(T_i)\}_{i=1}^{N-1})
\end{align*}
$p$ is here the solution to the adjoint equation, which has to be calculated before we can evaluate the gradient. For processes $P_i$, $i<N-1$, the initial condition of the adjoint equation is $p^i(T_{i})=\mu(y^i(T_{i}-\lambda_{i})$. This means that the first step after solving the state equations for gradient evaluation, is the same as for function evaluation, i.e. we have to send $\lambda_i$ from $P_{i}$ to $P_{i-1}$:
\begin{align*}
P_0\overset{\lambda_1}{\longleftarrow}P_1\overset{\lambda_2}{\longleftarrow}P_2 \overset{\lambda_3}{\longleftarrow}\cdots \overset{\lambda_{N-2}}{\longleftarrow}P_{N-2}\overset{\lambda_{N-1}}{\longleftarrow}P_{N-1}
\end{align*}
Each process can now solve its adjoint equation locally, and we can start to actually evaluate the gradient. The first step, would be to send $p_{i+1}(T_{i})$ from $P_i$ to $P_{i-1}$ so that we can find the penalty part of the gradient. Each process should also be able to calculate their own part of the gradient as $\nabla \hat J^i=(J_v(y^{i+1}(v^i),v^i)-B^*_ip^{i+1})$. The final step is now to gather all the local parts of the gradient to the form the actual gradient. In summation we get the following algorithm for gradient evaluation:
\begin{align*}
1. \ &P_i \ \textit{calculates} \ y^{i+1} \ \forall i \\
2. \ &P_{i-1}\overset{\lambda_i}{\longleftarrow}P_{i} \ \textit{for} \ i=1,..,N-1 \\
3. \ &P_i \ \textit{calculates} \ p^{i+1} \ \forall i \\
4. \ &P_{i-1}\overset{p^i(T_i)}{\longleftarrow}P_{i} \ \textit{for} \ i=1,..,N-1 \\
5. \ &\textit{Penalty part of gradient } \ p_{i+1}(T_i) - p_{i}(T_i) \ \textit{is stored for $i\neq N-1$} \\
&\textit{Local gradient if calcualated: } \ \nabla \hat J^i=(J_v(y^{i+1}(v^i),v^i)-B^*_ip^{i+1}) \\
6. \ &\nabla \hat J_{\mu} = \textbf{Gather}(\nabla \hat J^i,p_{i+1}(T_i) - p_{i}(T_i))
\end{align*}
\section{Analysing parallel performance}
Now that we know what type of communication is involved in objective function evaluation and gradient computation, we can try to model the expected performance of the two algorithms. One way to measure performance of algorithms is to look at their execution times. Therefore lets define $T_s$ as execution time of the sequential algorithm, and $T_p$ as parallel algorithm execution time. Let us also define the speedup $S=\frac{T_s}{T_p}$. Since we for now are only modelling performance we do not actually calculate the execution times, but we do know that the run time of the algorithms are related to the size of the problem, meaning the number of time-steps $n$. The last thing we need before we start our performance analysis, is a way to model communication between two processes. One way of modelling the communication time $T_c$ for sending a message of size $m$ between to processes, is proposed in \cite{grama2003introduction} as:
\begin{align*}
T_c = T_l + mT_w
\end{align*} 
Here $T_l$ is a constant representing latency or startup time, while $T_w$ is a constant representing the per message-unit transfer time. With these tools, we can now start analysing the performance of our algorithms.
\subsection{Objective function evaluation speedup}
The function evaluation consists of solving the state equation, which requires $O(n)$ operations, and then applying the functional on the control and the state on all $n+1$ time points, which probably also requires $O(n)$ operations, we can assume that the sequential objective function evaluation execution time is 
\begin{align*}
T_s = O(n)
\end{align*}
For our parallel algorithm we also solve the state equation and apply the functional, but since we divide the time steps equally between all processes, solving the state equation and applying the functional only requires $O(\frac{n}{N})$ operations. Since we also have penalty terms in our functional we get additional $O(N)$ operations. Now for the communication. We are doing two communication steps one is sharing the $\lambda$s between process neighbours, and the other is reducing the local function values into one global function value. The send and receive time is given by $T_c = T_l + \textit{dim}(\lambda_i)T_w$, which requires $O(1)$ operations, while the reduction time $T_{red}$ can be modelled as:
\begin{align*}
T_{red} &= \log N (T_l+T_w) \\
&= O(\log N)
\end{align*} 
Here we assume that the parallel architecture is made in a certain way, and that the local functional value is a float. This results in parallel function evaluation execution time:
\begin{align*}
T_p &= O(\frac{n}{N}) +O(N) + O(\log N) + O(1) \\
&=O(\frac{n}{N}) +O(N)
\end{align*}
The speedup is then:
\begin{align*}
S &= \frac{T_s}{T_p} = \frac{O(n)}{O(\frac{n}{N}) +O(N)} \\
&=O(N)
\end{align*}
This is an optimal speedup.
\subsection{Gradient speedup}
When we calculate the objective function gradient sequentially, we solve both the state and adjoint equations. Still the required operations are still in the order of number of time-steps, i.e:
\begin{align*}
T_s = O(n)
\end{align*}
For the parallel algorithm the operations required to solve the local state and adjoint equations are $O(\frac{n}{N})$. We then have two $O(1)$ send-receive communications similar to the send and receive for function evaluation. Lastly we need to model the gathering of the gradient. First define $L$ to be the length of the gradient. The run time of the gather $T_{gather}$, can then be modelled as:
\begin{align*}
T_{gather} &= T_l\log N + \frac{L}{N}T_w(N-1) \\
&= O(\log N) + O(L)
\end{align*}
The execution time of the parallel algorithm is therefore:
\begin{align*}
T_p = O(\frac{n}{N}) + O(\log N) + O(L)
\end{align*}
Again we find speedup by dividing $T_s$ by $T_p$:
\begin{align*}
S &= \frac{T_s}{T_p} = \frac{O(n)}{O(\frac{n}{N}) + O(\log N) + O(L)} \\
&=\frac{1}{\frac{1}{N} + \frac{\log N}{n}+\frac{L}{n}} = \frac{1}{\frac{1}{N} +\frac{L}{n}}
\end{align*}
If $L$ is independent of $n$, the speedup for gradient evaluation is $O(N)$, like it is for function evaluation, however if $L$ is dependent on $n$, this is not the case, and we would instead get speedup $S=O(\frac{n}{L(n)})$. In a case where the control for example is the source term in the state equation, we would actually get $S=O(1)$, which is really bad, and we would not expect any improvement when using parallel, at least for large $n$ values. There is however a way to get around this problem, which is to store both the gradient and the control locally, which means that you never have to do a gather call. If this is done, and if a solution spread between all processes is accepted, the speedup for gradient evaluation will also be $O(N)$.
