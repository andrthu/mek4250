\chapter{Parareal BFGS preconditioner}
In the previous chapter we saw that the parareal scheme allows us to parallelize time dependent differential equations in their temporal direction. In this chapter we will look at how to parallelize optimal control problems with time dependent differential equation constraints in temporal direction.  
\section{Optimal control problem with time-dependent DE constraints on a decomposed time interval}
Before we start to decompose the time interval, let us again state the general problem that we want to solve:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v) \label{initial problem1}\\
\textit{Subject to:} \ &E(y(t),v)=0 \ \quad t\in [0,T] \label{initial problem}
\end{align}
To introduce parallelism, we decompose $I=[0,T]$ into $N$ subintervals $I_i=[T_{i-1},T_i]$, with $T_0=0$ and $T_N=T$. To be able to solve the differential equation $E$ on each interval $I_i$, we introduce intermediate initial conditions $y(T_i)=\lambda_i$ for $i=1,...,N-1$. This means that instead of finding $y$ by solving $E$ on the entire time domain $I$, we can now find $y$ by solving $E$ separately on on each subinterval $I_i$. The problem (\ref{initial problem}) now reads:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v)  \label{decomposed problem1}\\
\textit{Subject to:} \ &E^i(y^i(t),v)=0 \ t\in [T_{i-1},T_i] \ \forall i \label{decomposed problem}
\end{align} 
Since we want the state $y$ to be continuous, we also need the following conditions:
\begin{align}
y^{i-1}(T_i)=y^i(T_i)=\lambda_i \ \ i=1,..,N-1 \label{Extra constraints}
\end{align} 
Both the problems (\ref{initial problem1}-\ref{initial problem}) and (\ref{decomposed problem1}-\ref{Extra constraints}) are constrained problems, which we solve by reducing them to unconstrained problems. In the original setting this can easily be done if we assume that each control variable $v$ corresponds to a unique solution $y$ of the state equation $E$. We can then define a reduced objective function $\hat{J}(v)$, and minimize it with respect to $v$, i.e solve the unconstrained problem:
\begin{align*}
\underset{v\in V}{\text{min}} \ \hat J(v)
\end{align*} 
Assuming that the decomposed state equations also can be uniquely resolved $\forall v$, we can again define a reduced objective function  $\hat{J}$. However because of the extra conditions (\ref{Extra constraints}) the reduction of (\ref{decomposed problem}) still produces a constrained problem:
\begin{align}
&\underset{v\in V}{\text{min}} \ \hat J(v) \label{constrained reduced j}\\
&y^{i-1}(T_i)=\lambda_i \ \ \forall i \label{constrained reduced}
\end{align} 
\section{The penalty method} \label{penalty_sec}
To solve the constrained problem (\ref{constrained reduced j}-\ref{constrained reduced}), we will use the penalty method, which transforms constrained problems into a series of unconstrained problems by incorporating the constraints into the functional. Incorporating the constraints means penalizing not satisfying the constraints. To use the penalty method on (\ref{constrained reduced j}-\ref{constrained reduced}) we first introduce the initial conditions to the decomposed state equations as variables $\Lambda = (\lambda_1,..,\lambda_{N-1})^T$, and then define the penalized objective function $\hat J_{\mu}$:
\begin{align}
\hat J_{\mu}(v,\Lambda) = \hat J(v) + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{pen_obj_J}
\end{align}
With $\mu>0$. If we now minimize $\hat{J}_{\mu}$ with respect to $(v,\Lambda)$, while letting $\mu$ tend to infinity, we hope that the solution satisfies the conditions (\ref{Extra constraints}), while also minimizing the actual problem (\ref{constrained reduced j}-\ref{constrained reduced}). The algorithmic framework of this reads:
\\
\\
\begin{algorithm}[H]
\KwData{Choose $\mu_0,\tau_0>0$, and some initial control $(v^0,\Lambda^0$}
\For{$k=1,2,...$}{
Find $(v^k,\Lambda^k)$ s.t. $\parallel\nabla \hat J_{\mu_{k-1}}(v^k,\Lambda^k)\parallel<\tau_{k-1}$\;
\eIf{STOP CRITERION satisfied}{
$\bold{Stop}$ algorithm\;
}{
Choose new $\tau_k\in(0,\tau_{k-1})$ and $\mu_k\in(\mu_{k-1},\infty) $\;
}
}
\caption{Penalty framework\label{PEN_ALG}}
\end{algorithm}
Assuming that we have a solution $v$ minimizing $\hat J$, one would hope that the iterates $\{v^k\}$ from the penalty method converges to the solution of the non-penalized problem, that is:
\begin{align*}
\lim_{k\rightarrow \infty} v^k =v
\end{align*}
From \cite{nocedal2006numerical} we get a result that deals with this:
\begin{theorem}
Assume $v^k$ is the exact global minimizer of $J_{\mu_k}$, then each limit point of the sequence $\{v^k\}$ is a solution of the problem (\ref{initial problem}).
\end{theorem}
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
The above result shows that penalty algorithm framework actually produces a solution to the original problem. There are however parts of the above framework, that still needs special attention, namely how to find $(v^k,\Lambda^k)$ in each iteration, how to update $\mu_k$ and $\tau_k$ and how to choose an adequate stopping criteria. Finding the optimal control for each iteration is done by applying an optimization method for unconstrained problems, that is dependent on the gradient at $(v^k,\Lambda^k)$. Let us therefore differentiate the penalized objective function.
\subsection{The gradient of the penalized objective function}
To find the gradient of \ref{pen_obj_J} we start by differentiating $\hat J_{\mu}(v,\Lambda)$:
\begin{align}
\hat J_{\mu}'(v,\Lambda) &= DJ_{\mu}(y(v,\Lambda),v,\Lambda) \\
&= y'(v,\Lambda)^*\frac{\partial}{\partial y} J_{\mu}(y(v,\Lambda),v,\Lambda) + (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu}(y(v,\Lambda),v,\Lambda)
\end{align} 
To find an expression for $y'(v,\Lambda)^*$ we differentiate the state equation $E$:
\begin{align*}
DE(y(v,\Lambda),v,\Lambda)=0 &\Rightarrow E_y(y(v,\Lambda),v,\Lambda)y'(v,\Lambda)=-E_v(y(v,\Lambda),v,\Lambda)- E_{\Lambda}(y(v,\Lambda),v,\Lambda)\\ &\Rightarrow y'(v)=-E_y(y(v,\Lambda),v,\Lambda)^{-1}((E_v(y(v,\Lambda),v,\Lambda)+E_{\Lambda}(y(v,\Lambda),v,\Lambda)) \\ &\Rightarrow y'(v,\Lambda)^* = -(E_v(y(v,\Lambda),v,\Lambda)^*+E_{\Lambda}(y(v,\Lambda),v,\Lambda)^*)E_y(y(v,\Lambda),v,\Lambda)^{-*}
\end{align*}
Inserting the above expression for $ y'(v,\Lambda)^*$ into the gradient yields:
\begin{align}
\hat J_{\mu}'(v,\Lambda) &=-(E_v^*+E_{\Lambda}^*)E_y^{-*}\frac{\partial}{\partial y} J_{\mu} + (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu} \\
&=-(E_v^*+E_{\Lambda}^*)p+ (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu} \label{pen_abs_grad}
\end{align}
Where $p$ is the solution of the adjoint equation:
\begin{align*}
E_y^*p=\frac{\partial}{\partial y}J_{\mu}
\end{align*} 
Notice that the state equation $E$ consists of several equations defined separately on the each of the decomposed subintervals. The result is that the adjoint equation also consists of several equations defined on each interval. To see this clearly we will derive the adjoint and the gradient for the example problem (\ref{exs_J}-\ref{exs_E}).
\subsection{Deriving the adjoint for the example problem}
Let us first recall the example optimal control problem with ODE constraints:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 \\
&\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t}y(t)=a y(t) + v(t) \ \quad t\in(0,T)\\
       	y(0)=y_0
     \end{array}
   \right.
\end{align*}
We can now decompose the interval $[0,T]$ into $N$ subintervals $\{[T_{i-1},T_{i}]\}_{i=1}^{N}$, and then define the above state equation on each interval, which forces us to penalize the objective function. The decomposed state equations will look like:
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y^i(t)=a y^i(t) + v(t) \ t\in(T_{i-1},T_{i})\\
       	y^i(T_{i-1})=\lambda_{i-1}
     \end{array}
   \right. \label{decomp_E}
\end{align}
The reduced penalized objective function will be given as:
\begin{align}
\hat J_{\mu}(v,\Lambda) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{penalty_func}
\end{align}
\begin{theorem}
The adjoint equation of problem (\ref{exs_J}-\ref{exs_E}) on interval $[T_{N-1},T_N]$ is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_N =a p_N  \\
	p_N(T_{N}) = \alpha( y_N(T_{N})-y_T)
	\end{array}
   \right. \label{end adjoint}
\end{align}
On $[T_{i-1},T_i]$ the adjoint equation is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_i =ap_i  \\
	p_i(T_{i}) = \mu(y_{i}(T_{i})-\lambda_{i} )
	\end{array}
   \right. \label{exs_adjoint}
\end{align}
\end{theorem} 
\begin{proof}
Lets begin as we did for the non-penalty approach, by writing up the weak formulation of the state equations:
\begin{gather*}
\textit{Find $y_i \in L^2(T_{i-1},T_i)$ such that }\\
L^i[y_i,\phi] = \int_{T_{i-1}}^{T_{i}}-y_i(t)(\phi'(t) +a \phi(t))+v(t)\phi(t)dt -\lambda_{i-1}\phi(T_{i-1})+ y_i(T_i)\phi(T_i) =0\\ \forall \ \phi \in C^{\infty}((T_{i-1},T_{i}))
\end{gather*} 
To find the adjoint equations we differentiate the $E^i$s and the functional $\hat J_{\mu}$ with respect to $y$. To simplify notation easier, let $(\cdot,\cdot)_i$ be the $L^2$ inner product of the interval $[T_{i-1},T_i]$. 
\begin{align*}
E_y^i=L_y^i[\cdot,\phi]=(\cdot,-(\frac{\partial}{\partial t} + a - \delta_{T_i})\phi)_i 
\end{align*}
Lets differentiate $\hat J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial y} \hat J_{\mu}= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) + \mu \sum_{i=1}^{N-1} \delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) 
\end{align*}
Since $y$ really is a collection of functions, we can differentiate $\hat J_{\mu}$ with respect to $y_i$. This gives us:
\begin{align*}
\frac{\partial}{\partial y_N}\hat J_{\mu}&= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) \\
\frac{\partial}{\partial y_i}\hat J_{\mu} &= \mu\delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) \ i\neq N
\end{align*}
We will now find the adjoint equations, by finding the adjoint of the $E_y^i$s. This is done as above, by inserting two functions $v$, $w$ into $L_y^i[v,w]$, and then moving the derivative form $w$ to $v$.
\begin{align*}
E_y^i&=L_y^i[v,w]=\int_{T_{i-1}}^{T_i}-v(t)(w'(t)+a w(t))dt + v(T_i)w(T_i) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-av(t))dt + v(T_i)w(T_i)-v(T_i)w(T_i) +v(T_{i-1})w(T_{i-1}) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-a v(t))dt + v(T_{i-1})w(T_{i-1}) \\
&=(L_y^i)^*[w,v]
\end{align*}
this means that $(E_y^i)^*=(L_y^i)^*[\cdot,\psi]$. The weak form of the adjoint equations is then found, by setting setting $(L_y^i)^*[p,\psi]=(J_{y_i},\psi)_i$. This gives two cases:
\\
\\
$i=N$ case:
\begin{align*}
&\textit{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}p_N(t)\psi'(t)-a p_N(t)\psi(t)dt +p_N(T_{N-1})\psi(T_{N-1})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textit{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}p_i(t)\psi'(t)-a p_i(t)\psi(t)dt +p_i(T_{i-1})\psi(T_{i-1})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
The strong formulation, is obtained by partial integration:
\\
\\
 $i=N$ case:
\begin{align*}
&\textrm{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}-p_N'(t)\psi(t)-a p_N(t)\psi(t)dt +p_N(T_{N})\psi(T_{N})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textit{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}-p_i'(t)\psi(t)-a p_i(t)\psi(t)dt +p_i(T_{i})\psi(T_{i})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
This gives us the ODEs we wanted.
\end{proof}
With the adjont equations we can find the gradient.
\begin{theorem}
The gradient of (\ref{penalty_func}), $\hat J_{\mu}'$, with respect to the control $(v,\Lambda)$ is:
\begin{align}
\hat J_{\mu}'(v,\Lambda) = (v+p,p_{2}(T_1) -p_{1}(T_1),..., p_{N}(T_{N-1}) -p_{N}(T_{N-1})) \label{penalty grad}
\end{align} 
and the directional derivative with respect to $L^2$-norm in direction $(s,l)$ is:
\begin{align*}
\langle \hat J_{\mu}'(v,\Lambda), (s,l)\rangle = \int_0^T (v+p)s \ dt +\sum_{i=1}^{N-1}(p_{i+1}(T_i) -p_{i}(T_i) )l_i
\end{align*}
\end{theorem}
\begin{proof}
If we first find $E_v^*$, $E_{\lambda}^*$, $J_v$ and $J_{\lambda}$ find the gradient by simply inserting these expression into (\ref{pen_abs_grad}). We can begin with the $E$ terms:
\begin{align*}
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi) \\
E_{\lambda_{i-1}}^i &= L_{y_{i-1}}^i[\cdot,\phi] = -(\cdot,\delta_{T_{i-1}}\phi)_i
\end{align*}
Notice that both of these forms are symmetric, and we therefore do not need to do more work to find their adjoints, they are however derived from the weak formulation, and it might therefore be easier to translate these forms to their strong counterpart:
\begin{align*}
E_v &=-1 \\
E_{\lambda_{i-1}}^i &= -\delta_{T_{i-1}}
\end{align*}
Then lets differentiate $\hat J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial v}\hat J_{\mu} &= v \\
\frac{\partial}{\partial \Lambda}\hat J_{\mu} &= - \mu \sum_{i=1}^{N-1}(y_{i}(T_i)-\lambda_i)
\end{align*}
Let us now insert these into (\ref{pen_abs_grad}), and firstly find the directional derivative:
\begin{align*}
\langle \hat J_{\mu}'(v,\lambda), (s,l)\rangle&=\langle -(E_v+E_{\lambda})p, (s,l)\rangle + \langle J_v+J_{\lambda}, (s,l)\rangle \\
&= \langle (p+\sum_{i=1}^{N-1} \delta_{T_i}p_{i+1}) , (s,l)\rangle+ \int_0^T us \ dt - \mu \sum_{i=1}^{N-1}(y_{i}(T_i)-\lambda_i)l_i\\
&=\int_0^T (v+p)s \ dt +\sum_{i=1}^{N-1}(p_{i+1}(T_i) -\mu(y_{i}(T_i)-\lambda_i) )l_i \\
&= \int_0^T (v+p)s \ dt +\sum_{i=1}^{N-1}(p_{i+1}(T_i) -p_{i}(T_i) )l_i
\end{align*} 
Here we use that $p_i(T_i) = \mu(y_{i}(T_i)-\lambda_i)$. We also see from this, that the gradient has the form we stated above.
\end{proof} 
\section{Parareal preconditioner} \label{pc sec}
Parallelizing the solution process of optimal control problems with time dependent differential equation constraints comes down to solving a series of penalized control problems. Since we have derived the gradient of these penalized problems for a specific example, we can now solve the control problem numerically using an optimization algorithm. We can for example use the steepest descent method (\ref{SD_itr}), which would create the following iteration for each penalized control problem:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_k\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method}
\end{align}
Alternatively we could use a BFGS iteration (\ref{BFGS_itr}), which would result in the following update:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kH^{k}\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{bfgs_method}
\end{align}
Where $H^k$ is the inverse Hessian approximation defined in (\ref{inv_H_apr}). In order to improve convergence of the unconstrained optimization solvers, we include a preconditioner based on Parareal, proposed in \cite{maday2002parareal}, in our optimization algorithms. The preconditioner $Q$ will be on the form:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 & Q_{\Lambda} \\
	\end{array} \right],\quad Q_{\Lambda}\in\mathbb{R}^{N-1\times N-1} \label{PC_form}
\end{align} 
For steepest descent, we apply $Q$, by modifying (\ref{gradient_method}) in the following way:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kQ\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method2}
\end{align}
For us to expect any improvement in convergence for the preconditioned steepest descent, $Q$ would have to resemble the Hessian of $\hat{J}_{\mu}$, at least for the $\Lambda$ part of the control. Applying $Q$ to the BFGS iteration, is done by setting the initial Hessian approximation $H^0=Q$. To be able to do this, we need $Q$ to be symmetric positive definite, since that is a requirement on $H^0$. Before we write up what $Q_ {\Lambda}$ is, we need to revisit the algebraic formulation of the Parareal scheme defined in chapter \ref{parareal_chap}.
\subsection{Virtual problem}
In order for us to be able to derive the preconditioner from \cite{maday2002parareal}, we define the virtual optimal control problem. In the virtual control problem the objective function is only the penalty part of (\ref{pen_obj_J}). The problem was already written up in section \ref{algebraic_sec}, let us however restate it here:
\begin{align}
&\min_{\Lambda}\bold{J}(\Lambda,y) = \sum_{i=1}^{N-1} (y_{i-1}(T_{i})-\lambda_{i})^2 \label{virtual_func} \\
&\textrm{Subject to } \ y_{i-1}(T_{i}) = \bold F_{\Delta T}(\lambda_{i-1}) \ i=1,...,N-1 \label{virtual}
\end{align}
Where $\Lambda=(\lambda_0=y_0,\lambda_1,...,\lambda_ {N-1}$ and $\bold F_{\Delta T}$ is the fine propagator from section \ref{Parareal_sec}. In the context of the virtual problem (\ref{virtual_func}-\ref{virtual}), $\bold F_{\Delta T}(\omega)$ propagates $\omega$ one time step of length $\Delta T$ using the equation:
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y(t)-ay(t)=0  \ \textrm{for } \ t\in(0,\Delta T),\\
       	y(0)=\omega.
     \end{array}
   \right. \label{virtual_exs}
\end{align} 
Chapter \ref{parareal_chap} explained how this could be solved by setting $\lambda_i= \bold F_{\Delta T}(\lambda_{i-1})$, which is the same as solving $\hat{J}(\Lambda)=0$. This eventually led us to the parareal scheme, defined on matrix form as:
\begin{align}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}(H-M\Lambda^k)\label{par_mat_sys}
\end{align}
Where $M$ and $\bar{M}$ are matrix representation of the fine and coarse resolution propagators $\lambda_i= \bold F_{\Delta T}(\lambda_{i-1})$ and  $\lambda_i= \bold G_{\Delta T}(\lambda_{i-1})$:
\begin{align*}
M = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} & \mathbbold{1}  \\
   \end{array}  \right],
\bar{M} = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{G}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{G}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{G}_{\Delta T} & \mathbbold{1}   \\
   \end{array}  \right]
\end{align*}
In section \ref{algebraic_sec} we said that $\bar M^{-1}$ can be seen as a preconditioner for the fix point iteration solving $\bold{J}(\Lambda,y)=0$. Solving the virtual problem in this way, is only possible since we know that the minimum value of $\bold{J}(\Lambda,y)$ is zero. A more natural approach of solving this problem would first be to reduce $\bold J$, to only depend on $\Lambda$, and then solve $\hat {\bold J}'(\Lambda)=0$. If we could find a preconditioner to a fix point iteration solving $\hat{ \bold J} '(\Lambda)=0$, this would be a candidate for $Q_{\Lambda}$. Let us start by writing up the reduced version of (\ref{virtual_func}-\ref{virtual}):
\begin{align}
\min_{\Lambda\in\mathbb{R}^{N-1}}\hat {\bold J}(\Lambda) = \sum_{i=1}^{N-1} (\bold F_{\Delta T}(\lambda_{i-1})-\lambda_{i})^2,\quad \lambda_0=y_0 \label{reduced_viritual}
\end{align} 
Now that we have an expression for the reduced objective function $\hat {\bold J}$, we can try to find its gradient. Luckily for us we have already derived the gradient for the more general problem (\ref{exs_J}-\ref{exs_E}) in (\ref{penalty grad}). We get the virtual control problem from the more general problem if we remove the control variable $v$ from the state and objective function of (\ref{exs_J}-\ref{exs_E}). $\hat {\bold J}'(\Lambda)$ is therefore given as:
\begin{align*}
\hat{\bold J}'(\Lambda) = \{p_i(T_i)-p_{i-1}(T_i)\}_{i=1}^{N-1}
\end{align*}
Here $p^i$ are the solutions of the adjoint equations (\ref{exs_adjoint}). Since we need the adjoints, let us define the fine adjoint propagator $\bold{F}_{\Delta T}^*(\omega)$ as we did for $\bold{F}_{\Delta T}$, but now $\omega$ is propagated backwards one time step of length $\Delta T$, and the equation $\bold{F}_{\Delta T}^*$ solves is:   
\begin{align}
\left\{
     \begin{array}{lr}
	\frac{\partial }{\partial t}p +ap=0,\quad t\in (0,\Delta T)  \\
	p(\Delta T) = \omega
	\end{array}
   \right. \label{virtual_adjoint_exs}
\end{align}
We also define $\bold G_{\Delta T}^*$ as the fine adjoint propagators coarse counterpart. Next we can define the matrices $M^*$ and $\bar M^*$ as:
\begin{align*}
M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{F}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{F}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{F}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right],
\bar M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{G}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{G}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{G}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right]
\end{align*}
It then turns out that solving $\hat{J}'(\Lambda)=0$ is the same as solving the system:
\begin{align}
M^* \ M \ \Lambda \ = \ M^* \ H \label{vir_grad_sys}
\end{align}
The reason we see by moving $M^*H$ to the left hand side of (\ref{vir_grad_sys}) and writing out what $M^*( \ M \ \Lambda-H)$ means. Firstly:
\begin{align}
M \ H-\Lambda  = \left( \begin{array}{c}
	y_0-\lambda_0  \\
	 \bold{F}_{\Delta T}(\lambda_0)-\lambda_1 \\
	 \bold{F}_{\Delta T}(\lambda_1)-\lambda_2  \\
	\cdots \\
	\bold{F}_{\Delta T}(\lambda_{N-2})-\lambda_{N-1} 
	\end{array} \right)
\end{align}
We recognise $\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i=y^i(T_i) -\lambda_i$, $i=1,...,N-1$ as the initial conditions of the adjoint equations (\ref{exs_adjoint}). This means that $\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i=p^i(T_i)$. This means that: 
\begin{align}
M \ H-\Lambda  = \left( \begin{array}{c}
	y_0-\lambda_0  \\
	 p^1(T_1) \\
	 p^2( T_2) \\
	\cdots \\
	p^{N-1}(T_{N-1}) 
	\end{array} \right)
\end{align}
If we then apply $M^*$ to $M \ H-\Lambda$, we get:
\begin{align}
M^* (M \ H-\Lambda)&=
	\left( \begin{array}{c}
	y_0-\lambda_0 -\bold{F}_{\Delta T}^*(p^1(T_1))\\
	 p^1(T_1)-\bold{F}_{\Delta T}^*(p^2( T_2))\\
	p^2( T_2)-\bold{F}_{\Delta T}^*(p^3( T_3))\\
	\cdots \\
	p^{N-1}(T_{N-1})
	\end{array} \right)
	\\
	&=\left( \begin{array}{c}
	y_0-\lambda_0 - p_1(T_0)\\
	p_1(T_1)-p_2(T_2)\\
	p_3(T_2)-p_2(T_2)\\
	\cdots \\
	p_{N-2}(T_{N-2})-p_{N-1}(T_{N-2}) \\
	p_{N-1}(T_{N-1})
	\end{array} \right)
\end{align}
The last vector, can be recognised as the negative gradient of (\ref{virtual_func}) in its 2nd to $(N-1)$th indices, which shows that solving (\ref{vir_grad_sys}) is thae same as solving $\hat {\bold J}'(\Lambda)=0$. If we then created a fix point iteration for (\ref{vir_grad_sys}), in a similar fashion as (\ref{par_mat_sys}), it would look like this:
\begin{align*}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}\bar M^{-*}(H-M^*M\Lambda^k)
\end{align*}
$\bar{M}^{-1}\bar{M}^{-*}$ could then be thought of as the preconditioner of the iteration, in the sense that $\bar{M}^{-1}\bar{M}^{-*}M^*M$ would be close to $\mathbbold{1}$. If $\bar{M}^{-1}\bar{M}^{-*}$ works as a preconditioner for  $\hat {\bold J}'(\Lambda)=0$ where $\hat {\bold J}$ is the objective function in the virtual problem, \cite{maday2002parareal} proposes $\bar{M}^{-1}\bar{M}^{-*}$ as a preconditioner for the penalized optimal control problem (\ref{decomp_E}-\ref{penalty_func}). Meaning that we set $Q$ to be:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 &  \bar{M}^{-1}\bar{M}^{-*}\\
	\end{array} \right]
\end{align}  
We have derived $Q$ by trying to find a good preconditioner for the fix point iteration for solving $\hat {\bold J}'(\Lambda)=0$. Whether $\bar{M}^{-1}\bar{M}^{-*}$ has anything to do with the inverse Hessian of $\hat J'$ is however not clear. It turns out that $M^*M$ actually is related to the Hessian of the virtual problem, which we will show in the next subsection.
\subsection{$\bar{M}^{*}\bar{M}$ as an approximation of the Hessian}
In order for us to make it easier to differentiate the gradient of the reduced objective function (\ref{reduced_viritual}), we try to write up $\hat{\bold J}$ on vector notation. Even though the propagator $\bold F$ is generally defined, and might very well be non-linear, we will now and for the rest of this subsection assume that the propagator is linear. Let us first define the vector $x(\Lambda)\in\mathbb{R}^{N-1}$ as:
\begin{align}
x(\Lambda)= \left( \begin{array}{c}  
   \lambda_1 - y_1(T_1) \\ 
   \lambda_2 - y_2(T_2) \\
   \cdots  \\
   \lambda_{N-1} -y_{N-1}(T_{N-1})  \\
   \end{array}  \right)
\end{align} 
We can then formulate the problem (\ref{reduced_viritual}) as a least square problem:
\begin{align}
\min_{\Lambda}\hat{\bold J}(\Lambda) = \min_{\Lambda}x(\Lambda)^Tx(\Lambda) \label{vector_J}  
\end{align}
Here $x(\Lambda)^T$ means $x(\Lambda)$ transposed. We then want to express $x$ on matrix notation in terms of $\Lambda$ and $y_0$. To do this, we first find an expression for $\{y_i(T_i)\}_{i=1}^{N-1}$:
\begin{align}
\left( \begin{array}{c}
   y_0(T_1) \\  
   y_1(T_2) \\ 
   \cdots  \\
   y_{N-1}(T_N)  \\
   \end{array}  \right)= 
   \left[ \begin{array}{cccc}  
   0 & 0 & \cdots & 0 \\ 
   \bold{F}_{\Delta T}&0 & 0  & \cdots \\
   0 &  \bold{F}_{\Delta T}&0 & \cdots \\
   0 &\cdots &\bold{F}_{\Delta T}& 0   \\
   \end{array}  \right]
   \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) + 
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right)
\end{align}
and when we insert this into $x$, we get:
\begin{align}
x = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} &  \mathbbold{1}  \\
   \end{array}  \right]
   \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) -
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right)
\end{align}
or:
\begin{align}
x  &= M \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) -
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right) \\
   & = M \Lambda -\left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right) \\
   &=M \Lambda-b
\end{align}
This allows us to write up the reduced functional $\hat{\bold J}$, that only depends on $\Lambda$:
\begin{align}
\hat{\bold J}(\Lambda)&= x^Tx =
(M  \Lambda -b)^T(M  \Lambda -b) \\
&= (M  \Lambda)^T(M  \Lambda) - (M  \Lambda)^Tb-b^T(M  \Lambda) + b^Tb \\
&=\Lambda^TM^TM  \Lambda - 2\Lambda^TM^Tb + b^Tb \label{T_J}
\end{align}
An important thing to note about the above expression, is that $M^T$ does not necessarily equal $M^*$, since $M^*$ is defined by the fine adjoint propagator $\bold F_ {\Delta T}^*$, and is not simply the transposed of $M$. Before we look further into the difference of $M^*$ and $M^T$, we write up the Hessian of $\hat{\bold J}$ in the proposition below:
\begin{proposition}
The Hessian of the reduced objective function $\hat{\bold J}$ (\ref{T_J}) originating from the virtual problem (\ref{virtual_func}-\ref{virtual}) is:
\begin{align}
\nabla^2\hat{\bold J}(\Lambda) = 2 M^TM
\end{align}
\end{proposition}
\begin{proof}
We can easily verify the above proposition by differentiating expression (\ref{T_J}). First the gradient $\nabla \hat{\bold J}$ is:
\begin{align*}
\nabla\hat{\bold J}(\Lambda) = 2 M^TM\Lambda - 2M^Tb
\end{align*}
Notice that if we assume that $M^T=M^*$, setting $\nabla\hat{J}(\Lambda)=0$ gives us the system (\ref{vir_grad_sys}). If we now differentiate $\nabla \hat{\bold J}(\Lambda)$, we get the Hessian we wanted:
\begin{align}
\nabla^2 \hat{\bold J}(\Lambda) = 2 M^TM
\end{align}
\end{proof}
This means that if $M^*=M^T$, then $M^*M$ is the Hessian of $\frac{1}{2}\hat{\bold J}$, and since $\bar{M}$ and $\bar{M}^*$ approximates $M$ and $M^*$, the matrix $\bar{M}^{*}\bar{M}$ should be an approximation of $\frac{1}{2}\nabla^2 \hat{\bold J}$, which means its inverse $\bar{M}^{-1}\bar{M}^{-*}$ should approximate the inverse Hessian. However as we have already said, these conclusions are based on the assumption that $M^*=M^T$. Let us therefore investigate if this assumption holds when we solve the virtual problem (\ref{virtual_func}-\ref{virtual}) for equation (\ref{virtual_exs}). 
\\
\\
Let the fine propagator $\bold F_{\Delta T}$ and the fine adjoint propagator $\bold F_{\Delta T}^*$ solve equations (\ref{virtual_exs}) and (\ref{virtual_adjoint_exs}) exactly. Due to the simplicity of these equations, we can easily write up the solutions of these equations:
\begin{align}
y(t)&=y(0)e^{at}, \\
p(t)&=p(\Delta T) e^{a(\Delta T-t)}
\end{align} 
This means that $\bold F_{\Delta T}(\omega)=y(\Delta T)=\omega e^{a\Delta T}$, and that $\bold F_{\Delta T}^*(\omega)=p(0)=\omega e^{a\Delta T}$. If we insert these expressions into $M$ and $M^*$, we get:
\begin{align*}
M = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -e^{a\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-e^{a\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-e^{a\Delta T} & \mathbbold{1}  \\
   \end{array}  \right],
M^* =\left[ \begin{array}{cccc}
   	\mathbbold{1} & - e^{a\Delta T} & 0 & 0 \\  
  	 0 & \mathbbold{1} & - e^{a\Delta T} & \cdots \\ 
  	 \cdots &0 &  \mathbbold{1} & - e^{a\Delta T} \\
  	 0 &\cdots &\cdots &  \mathbbold{1}  \\
  	 \end{array}  \right]
\end{align*}
Looking at the $M$ and $M^*$ matrices above, it is quite clear that $M^*=M^T$, which means that for the virtual problem (\ref{virtual_func}-\ref{virtual}) governed by the equation (\ref{virtual_exs}), the assumption $M^*=M^T$ holds, when the fine propagator and the fine adjoint propagator solved the state and adjoint equations exactly. If one instead let the underlying solvers of $\bold F_{\Delta T}$ and $\bold F_{\Delta T}^*$ be based on some numerical method, the $M$ and $M^*$ matrices will of course depend on the choice of method. When we discretize the state and adjoint equations, we want the matrices $M$ and $M^*$ to behave as they did for non-discrete $\bold F_{\Delta T}$ and $\bold F_{\Delta T}^*$.
\\
\\
Since $\bar{M}$ and $\bar{M}^{*}$ are only approximations of $M$ and $M^*$, we might think that $\bar{M}^*=\bar{M}^T$ is unnecessary. However since we want to use $\bar{M}^{-1}\bar{M}^{-*}$ as an initial inverse Hessian approximation in our BFGS-optimization algorithm, we need $\bar{M}^{-1}\bar{M}^{-*}$ to be symmetric positive definite. This is true if $\bar{M}^*=\bar{M}^T$, because the matrix product between an invertible matrix and its transpose always is positive definite. This holds because:
\begin{align*}
x^TM^TMx=(Mx)^TMx=||Mx||^2\geq &0, \quad \textrm{and} \\
||Mx||^2=0 \iff x=&0
\end{align*}
The second requirement for positive definiteness is true if $M$ is invertible, which is trivially true, since $M$ is a triangular matrix with the identity on its diagonal, and this means that $M$ has a non-zero determinant. Since $\bar{M}^*=\bar{M}^T$ guaranties that $\bar{M}^{-1}\bar{M}^{-*}$ is positive definite, we want to choose coarse propagators that gives us this property. Let us try to find out how we need to define $\bold G_{\Delta T}^*$, when we use the coarse propagator suggested in \cite{lions2001resolution} written up in (\ref{G_Backward}) based on the implicit Euler finite difference scheme. For equation (\ref{virtual_exs}), we can use (\ref{G_Backward}) to derive an expression for evaluating the coarse propagator $\bold G_{\Delta T}(\omega)$:
\begin{align}
\frac{\bold{G}_{\Delta T}(\omega) -\omega}{\Delta T } - a \bold{G}_{\Delta T}(\omega) &= 0 \\
\bold{G}_{\Delta T}(\omega)(1-a\Delta T)&= \omega \\
\bold{G}_{\Delta T}(\omega)&=\frac{\omega}{(1-a\Delta T)}
\end{align}
The implicit Euler coarse propagator for equation (\ref{virtual_exs}) then reads:
\begin{align*}
\bar M = \left[ \begin{array}{cccc}
   	\mathbbold{1} & 0 & \cdots & 0 \\  
   	\frac{-1}{1-a\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   	0 &\frac{-1}{1-a\Delta T} & \mathbbold{1}  & \cdots \\
   	0 &\cdots &\frac{-1}{1-a\Delta T} & \mathbbold{1}  \\
  	\end{array}  \right]
\end{align*}
In order for us to get $\bar{M}^*=\bar{M}^T$, we the discretization of (\ref{virtual_adjoint_exs}) to produce $\bold{G}_{\Delta T}^*(\omega)=\frac{\omega}{(1-a\Delta T)}$. Maybe unsurprisingly, it turns out that this is achieved by again discretizing (\ref{virtual_adjoint_exs}) using implicit Euler. $\bold{G}_{\Delta T}^*(\omega)$ is found as follows:
\begin{align*}
\frac{\omega-\bold{G}_{\Delta T}^*(\omega)}{\Delta T} + a \bold{G}_{\Delta T}^*(\omega)&=0 \\
\bold{G}_{\Delta T}^*(\omega)(a\Delta T-1)&=-\omega \\
\bold{G}_{\Delta T}^*(\omega)&=\frac{\omega}{(1-a\Delta T)}
\end{align*}
In the above derivation for $\bold{G}_{\Delta T}^*(\omega)$, the forward Euler finite difference scheme is used to discretize (\ref{virtual_adjoint_exs}), but since this equation is solved backwards in time this scheme is implicit. Since $\bold{G}_{\Delta T}^*(\omega)=\bold{G}_{\Delta T}(\omega)$, we get that $\bar{M}^*=\bar{M}^T$, which means that $\bar{M}^{-1}\bar{M}^{-*}$ is a valid choice for an initial inverted Hessian approximation in the BFGS-algorithm.
\subsection{Non-linear state equations}

\subsection{The meaning of $\bar{M}^{-1}$ and $\bar{M}^{-*}$}
Since neither $\bar{M}$ nor $\bar{M}^{*}$ are normal matrices, it might be smart to explain what is meant by taking their inverse. Lets do this by looking at an example, where we try to solve a $4\times 4$ system $\bar{M}x = y$:
\begin{align}
\left[ \begin{array}{cccc}
   I & 0 & 0 & 0 \\  
   -\bold{G}_{\Delta T} & I & 0 & 0 \\ 
   0 &-\bold{G}_{\Delta T} & I  & 0 \\
   0 &0 &-\bold{G}_{\Delta T} & I   \\
   \end{array}  \right]
   \left( \begin{array}{c}
   x_0 \\
   x_1 \\
   x_2 \\
   x_3 \\
   \end{array} \right) =
   \left( \begin{array}{c}
   y_0 \\
   y_1 \\
   y_2 \\
   y_3 \\
   \end{array} \right)	
\end{align}  
If we apply $\bar{M}$ to $x$, we get:
\begin{align}
\left( \begin{array}{c}
   x_0 \\
   x_1 -\bold{G}_{\Delta T}(x_0) \\
   x_2 -\bold{G}_{\Delta T}(x_1)\\
   x_3 -\bold{G}_{\Delta T}(x_2)\\
   \end{array} \right) =
   \left( \begin{array}{c}
   y_0 \\
   y_1 \\
   y_2 \\
   y_3 \\
   \end{array} \right)
\end{align}
This yields the following solution:
\begin{align*}
x_0 &=y_0 \\
x_1 &=y_1 + \bold{G}_{\Delta T}(y_0) \\
x_2 &=y_2 + \bold{G}_{\Delta T}(y_1 + \bold{G}_{\Delta T}(y_0)) \\
x_3 &=y_3 + \bold{G}_{\Delta T}(y_2 + \bold{G}_{\Delta T}(y_1 + \bold{G}_{\Delta T}(y_0))) 
\end{align*}
To better see the connection with \cite{lions2001resolution}, note that we can solve the above system with the propagation technique, i.e. define a $\delta$ and solve:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\delta^{n+1} -\delta^n }{\Delta T } + A\delta^{n+1} = \frac{y^n}{\Delta T} \\
		\delta^0 = 0
	\end{array} \right.
\end{align} 
Then the solution of the system would be 
\begin{align*}
x_0 &=y_0 +\delta^0\\
x_1 &=y_1 + \delta^1 \\
x_2 &=y_2 +\delta^2\\
x_3 &=y_3 + \delta^3 
\end{align*}
For $\bar{M}^{*}$, the system $\bar{M}^{*}x=y$, has a solution that looks similar:
\begin{align*}
x_0 &=y_0 +\bold{G}_{\Delta T}^*(y_1 + \bold{G}_{\Delta T}^*(y_2 + \bold{G}_{\Delta T}^*(y_ 3)))\\
x_1 &=y_1 + \bold{G}_{\Delta T}^*(y_2 + \bold{G}_{\Delta T}^*(y_ 3))\\
x_2 &=y_2 + \bold{G}_{\Delta T}^*(y_ 3) \\
x_3 &=y_3 
\end{align*}
The propagator for the adjoint system would look like this:
\begin{align}
\left\{
     \begin{array}{lr}
		\frac{\delta^{i-1} -\delta^i }{\Delta T } + A^*\delta^{i-1} = \frac{y^i}{\Delta T} \\
		\delta^{N-1} = 0
	\end{array} \right.
\end{align} 
The solution would then again be:
\begin{align*}
x_0 &=y_0 +\delta^0\\
x_1 &=y_1 + \delta^1 \\
x_2 &=y_2 +\delta^2\\
x_3 &=y_3 + \delta^3 
\end{align*}
The above setting is quite general. Lets consider an example where we want to use the parareal preconditioner on an optimal control problem with a time domain $I=[0,T]$ decomposed into four sub-intervals $[T_0,T_1], [T_1,T_2], [T_2,T_3]$ and $[T_3,T_4]$. To enforce continuity at the overlapping boundaries, we need three variables $\lambda_1,\lambda_2$ and $\lambda_3$, that represents the initial condition of the state equation at $T_1,T_2$ and $T_3$. We then add quadratic penalty terms to our functional, and the gradient of our new functional $J(v,\Lambda)$, would then have three components $J_{\lambda_1}, J_{\lambda_2}, J_{\lambda_3}$. These are the components of the gradient that we wish to use (\ref{PC}) on. As far as I understand it, applying $\bar{M}^{*}\bar{M}$ to $J_{\lambda_1}, J_{\lambda_2}, J_{\lambda_3}$, means first to resolve this system:
\begin{align*}
\bar{J_{\lambda_1}} &=J_{\lambda_1} +\bold{G}_{\Delta T}^*(J_{\lambda_2} + \bold{G}_{\Delta T}^*(J_{\lambda_3} + \bold{G}_{\Delta T}^*(0)))\\
\bar{J_{\lambda_2}} &=J_{\lambda_2} + \bold{G}_{\Delta T}^*(J_{\lambda_3} + \bold{G}_{\Delta T}^*(0))\\
\bar{J_{\lambda_3}} &=J_{\lambda_3} + \bold{G}_{\Delta T}^*(0) \\
\end{align*} 
Then you apply the forward system:
\begin{align*}
\bar{\bar{J_{\lambda_1}}}&=\bar{J_{\lambda_1}} + \bold{G}_{\Delta T}(0) \\
\bar{\bar{J_{\lambda_2}}}&=\bar{J_{\lambda_2}}+ \bold{G}_{\Delta T}(\bar{J_{\lambda_1}} + \bold{G}_{\Delta T}(0)) \\
\bar{\bar{J_{\lambda_3}}}&=\bar{J_{\lambda_3}} + \bold{G}_{\Delta T}(\bar{J_{\lambda_2}}+ \bold{G}_{\Delta T}(\bar{J_{\lambda_1}} + \bold{G}_{\Delta T}(0)))
\end{align*} 
The $\Lambda$ part of the gradient is then updated to be $\bar{\bar{J_{\lambda_1}}}, \bar{\bar{J_{\lambda_2}}},\bar{\bar{J_{\lambda_3}}}$. Notice that we in the last line of the adjoint system use $\bold{G}_{\Delta T}^*(0)$, and that we in the first line in the forward system use $\bold{G}_{\Delta T}(0)$. The zeros here represent information at time $T_0=0$ and $T_4=T$. At these times, we really have no information relating to the $\Lambda$ part of the gradient of $J$. We are therefore forced to choose zero.
\\
\\
Since we are only using our coarse solver use $\bold{G}_{\Delta T}$ on the inner points of the interval $I=[0,T]$, when we are applying the inverse of $\bar{M}^{*}$ and $\bar{M}$ on the $\Lambda$ part of the gradient, the coarse model is only used on the interval $[T_1,T_{N-1}]$. This means that we are not using the first and last parts $[0,T_1]$ and $[T_{N-1},T]$ of $I$. A consequence of this is that it refining our coarse solver for each interval $\bold{G}_{\Delta T}$, has little effect since $[0,T_1]$ and $[T_{N-1},T]$ have length $\Delta T$, and when we are not using these intervals the coarse solver has a natural built in error of $\Delta T$, even if we use a finer resolution on $[T_1,T_{N-1}]$. 
