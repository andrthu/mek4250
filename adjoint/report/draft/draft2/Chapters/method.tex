\chapter{Parareal BFGS preconditioner} \label{method_chap}
In the previous chapter we saw that the parareal scheme allows us to parallelize time dependent differential equations in their temporal direction. In this chapter we will look at how to parallelize optimal control problems with time dependent differential equation constraints in temporal direction. 
\\
\\
This chapter consists of three sections. In the first section we decompose the time domain as we did in section \ref{Para_dcomp_sec}, only now in the context of control problems with time dependent DE constraints. Decomposing the time interval leads to a reformulation of the control problem that includes extra constraints on the state equation. How to handle these new constraints are dealt with in section \ref{penalty_sec}. We use the same approach as \cite{maday2002parareal} namely the penalty method. This is a simplified version of the augmented Lagrangian approach used in \cite{rao2016time} for parallel in time 4d variational data assimilation. We demonstrate the use of the penalty method by revisiting the example problem from section \ref{example_sec}.
\\
\\
In the last section a Parareal based preconditioner to be used in the optimization algorithms solving the optimal control problems is presented. This preconditioner originally proposed in \cite{maday2002parareal} is derived using ides from subsection \ref{algebraic_sec} and we will in chapter \ref{Experiments chapter} see that it is crucial for the parallel in time algorithm to obtain any meaningful speedup. 
\section{Optimal control problem with time-dependent DE constraints on a decomposed time interval} \label{decomp_sec}
Before we start to decompose the time interval, let us again state the general problem that we want to solve:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v), \label{initial problem1}\\
\textrm{subject to:} \ &E(y(t),v)=0, \quad t\in [0,T]. \label{initial problem}
\end{align}
To introduce parallelism, we decompose $I=[0,T]$ into $N$ subintervals $I_i=[T_{i-1},T_i]$, with $T_0=0$ and $T_N=T$. To be able to solve the differential equation $E$ on each interval $I_i$, we introduce intermediate initial conditions $y(T_i)=\lambda_i$ for $i=1,...,N-1$. This means that instead of finding $y$ by solving $E$ on the entire time domain $I$, we can now find $y$ by solving $E$ separately on on each subinterval $I_i$. The problem (\ref{initial problem}) now reads:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v),  \label{decomposed problem1}\\
\textrm{subject to:} \ &E^i(y^i(t),v)=0, \quad t\in [T_{i-1},T_i] \quad \forall i. \label{decomposed problem}
\end{align} 
Since we want the state $y$ to be continuous, we also need the following conditions:
\begin{align}
y^{i}(T_i)=y^{i+1}(T_i)=\lambda_i \quad \ i=1,..,N-1. \label{Extra constraints}
\end{align} 
Both the problems (\ref{initial problem1}-\ref{initial problem}) and (\ref{decomposed problem1}-\ref{Extra constraints}) are constrained problems, which we solve by reducing them to unconstrained problems. In the original setting this can easily be done if we assume that each control variable $v$ corresponds to a unique solution $y$ of the state equation $E$. We can then define a reduced objective function $\hat{J}(v)$, and minimize it with respect to $v$, i.e solve the unconstrained problem:
\begin{align*}
\underset{v\in V}{\text{min}} \ \hat J(v).
\end{align*} 
Assuming that the decomposed state equations also can be uniquely resolved $\forall v$, we can again define a reduced objective function  $\hat{J}$. However because of the extra conditions (\ref{Extra constraints}) the reduction of (\ref{decomposed problem}) still produces a constrained problem:
\begin{align}
&\underset{v\in V}{\text{min}} \ \hat J(v), \label{constrained reduced j}\\
&y^{i-1}(T_i)=\lambda_i, \ \quad \forall i. \label{constrained reduced}
\end{align} 
\section{The penalty method} \label{penalty_sec}
To solve the constrained problem (\ref{constrained reduced j}-\ref{constrained reduced}), we will use the penalty method\cite{nocedal2006numerical}, which transforms constrained problems into a series of unconstrained problems by incorporating the constraints into the functional. Incorporating the constraints means penalizing not satisfying the constraints. To use the penalty method on (\ref{constrained reduced j}-\ref{constrained reduced}) we first introduce the initial conditions to the decomposed state equations as variables $\Lambda = (\lambda_1,..,\lambda_{N-1})^T$, and then define the penalized objective function $\hat J_{\mu}$:
\begin{align}
\hat J_{\mu}(v,\Lambda) = \hat J(v) + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{pen_obj_J}
\end{align}
With $\mu>0$. Since the new control variables are created by us, and do not exist in the original control problem, we call $\Lambda$ the virtual control. The control $v$ of the original problem will form now on be referred to as the real control. If we now minimize $\hat{J}_{\mu}$ with respect to $(v,\Lambda)$, while letting $\mu$ tend to infinity, we hope that the solution satisfies the conditions (\ref{Extra constraints}), while also minimizing the actual problem (\ref{constrained reduced j}-\ref{constrained reduced}). The algorithmic framework of this reads:
\\
\\
\begin{algorithm}[H]
\KwData{Choose $\mu_0,\tau_0>0$, and some initial control $(v^0,\Lambda^0$)}
\For{$k=1,2,...$}{
Find $(v^k,\Lambda^k)$ s.t. $\parallel\nabla \hat J_{\mu_{k-1}}(v^k,\Lambda^k)\parallel<\tau_{k-1}$\;
\eIf{STOP CRITERION satisfied}{
$\bold{Stop}$ algorithm\;
}{
Choose new $\tau_k\in(0,\tau_{k-1})$ and $\mu_k\in(\mu_{k-1},\infty) $\;
}
}
\caption{Penalty framework\label{PEN_ALG}}
\end{algorithm}
\noindent
Assuming that we have a solution $v$ minimizing $\hat J$, one would hope that the iterates $\{v^k\}$ from the penalty method converges to the solution of the non-penalized problem, that is:
\begin{align*}
\lim_{k\rightarrow \infty} v^k =v
\end{align*}
From \cite{nocedal2006numerical} we get a result that deals with this:
\begin{theorem}
Assume $v^k$ is the exact global minimizer of $J_{\mu_k}$, then each limit point of the sequence $\{v^k\}$ is a solution of the problem (\ref{initial problem}).
\end{theorem}
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\noindent
The above result shows that the penalty algorithmic framework actually produces a solution to the original problem. There are however parts of the above framework, that still needs special attention, namely how to find $(v^k,\Lambda^k)$ in each iteration, how to update $\mu_k$ and $\tau_k$ and how to choose an adequate stopping criteria. Finding the optimal control for each iteration is done by applying an optimization method for unconstrained problems, that is dependent on the gradient at $(v^k,\Lambda^k)$. Let us therefore differentiate the penalized objective function.
\subsection{The gradient of the penalized objective function}
We have introduced the penalized objective function (\ref{pen_obj_J}), that depends on both the real and virtual control, and we now want to evaluate its gradient. We again take the adjoint approach as we did in section \ref{adjointGrad_sec}, and the expression for $\hat J_{\mu}'(v,\Lambda)$ belonging to the general optimization problem (\ref{decomposed problem1}-\ref{decomposed problem}) is given in proposition \ref{penalty_grad_prop}.
\begin{proposition}[Gradient of the penalized objective function] \label{penalty_grad_prop}
Let $\hat J_{\mu}$ be the penalized objective function (\ref{pen_obj_J}). With similar assumptions as in proposition \ref{redGrad_prop}, the gradient of $\hat J_{\mu}$ is as follows: 
\begin{align}
\hat J_{\mu}'(v,\Lambda)=-(E_v(y,v,\Lambda)^*+E_{\Lambda}(y,v,\Lambda)^*)p+ (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu}(y,v,\Lambda) \label{pen_abs_grad}
\end{align}
where $p$ is the solution of the adjoint equation:
\begin{align}
E_y(y,v,\Lambda)^{*}p=\frac{\partial}{\partial y}J_{\mu}(y,v,\Lambda). \label{penalty adjoint}
\end{align}
\end{proposition}
\begin{proof}
Same reasoning as in proposition \ref{redGrad_prop}.
\end{proof}
\noindent
Notice that the state equation $E$ consists of several equations defined separately on each of the decomposed subintervals. The result is that the adjoint equation also consists of several equations defined on each interval. To see this clearly we will derive the adjoint and the gradient for the example problem (\ref{exs_J}-\ref{exs_E}).
\subsection{Deriving the adjoint for the example problem}
Before we derive the adjoint equation of the decomposed example problem (\ref{exs_J}-\ref{exs_E}) we need to write up the decomposed state equation and the penalized objective function. We start by decomposing the interval $[0,T]$ into $N$ subintervals $\{[T_{i-1},T_{i}]\}_{i=1}^{N}$. We can then define the decomposed state equation on each interval:
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y^i(t)=a y^i(t) + v(t) \quad t\in(T_{i-1},T_{i})\\
       	y^i(T_{i-1})=\lambda_{i-1}
     \end{array}
   \right. \label{decomp_E}
\end{align}
We get the reduced penalized objective function by adding the the penalty terms to the unpenalized objective function (\ref{exs_J}):
\begin{align}
\hat J_{\mu}(v,\Lambda) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{penalty_func}
\end{align}
Having formulated the penalized objective function, we are no ready to write up its gradient. The gradient of (\ref{penalty_func}) is given in proposition \ref{penGrad_prop}, but since the gradient depends on the decomposed adjoint equations, we write up these first.
\begin{proposition} \label{pen_adjoint_prop}
The adjoint equation of problem (\ref{exs_J}-\ref{exs_E}) on interval $[T_{N-1},T_N]$ is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_N =a p_N  \\
	p_N(T_{N}) = \alpha( y_N(T_{N})-y_T)
	\end{array}
   \right. \label{end adjoint}
\end{align}
On $[T_{i-1},T_i]$ the adjoint equation is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_i =ap_i  \\
	p_i(T_{i}) = \mu(y_{i}(T_{i})-\lambda_{i} )
	\end{array}
   \right. \label{exs_adjoint}
\end{align}
\end{proposition} 
\begin{proof}
Lets begin as we did for the non-penalty approach, by writing up the weak formulation of the state equations:
\begin{gather*}
\textrm{Find $y_i \in L^2(T_{i-1},T_i)$ such that }\\
L^i[y_i,\phi] = \int_{T_{i-1}}^{T_{i}}-y_i(t)(\phi'(t) +a \phi(t))+v(t)\phi(t)dt -\lambda_{i-1}\phi(T_{i-1})+ y_i(T_i)\phi(T_i) =0\\ \forall \ \phi \in C^{\infty}((T_{i-1},T_{i}))
\end{gather*} 
To find the adjoint equations we differentiate the $E^i$s and the functional $\hat J_{\mu}$ with respect to $y$. To simplify notation, let $(\cdot,\cdot)_i$ be the $L^2$ inner product of the interval $[T_{i-1},T_i]$. 
\begin{align*}
E_y^i=L_y^i[\cdot,\phi]=(\cdot,-(\frac{\partial}{\partial t} + a - \delta_{T_i})\phi)_i 
\end{align*}
Lets differentiate $\hat J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial y} \hat J_{\mu}= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) + \mu \sum_{i=1}^{N-1} \delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) 
\end{align*}
Since $y$ really is a collection of functions, we can differentiate $\hat J_{\mu}$ with respect to $y_i$. This gives us:
\begin{align*}
\frac{\partial}{\partial y_N}\hat J_{\mu}&= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) \\
\frac{\partial}{\partial y_i}\hat J_{\mu} &= \mu\delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) \ i\neq N
\end{align*}
We will now find the adjoint equations, by finding the adjoint of the $E_y^i$s. This is done as above, by inserting two functions $v$, $w$ into $L_y^i[v,w]$, and then moving the derivative form $w$ to $v$.
\begin{align*}
E_y^i&=L_y^i[v,w]=\int_{T_{i-1}}^{T_i}-v(t)(w'(t)+a w(t))dt + v(T_i)w(T_i) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-av(t))dt + v(T_i)w(T_i)-v(T_i)w(T_i) +v(T_{i-1})w(T_{i-1}) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-a v(t))dt + v(T_{i-1})w(T_{i-1}) \\
&=(L_y^i)^*[w,v]
\end{align*}
this means that $(E_y^i)^*=(L_y^i)^*[\cdot,\psi]$. The weak form of the adjoint equations is then found, by setting setting $(L_y^i)^*[p,\psi]=(J_{y_i},\psi)_i$. This gives two cases:
\\
\\
$i=N$ case:
\begin{align*}
&\textrm{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}p_N(t)\psi'(t)-a p_N(t)\psi(t)dt +p_N(T_{N-1})\psi(T_{N-1})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textrm{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}p_i(t)\psi'(t)-a p_i(t)\psi(t)dt +p_i(T_{i-1})\psi(T_{i-1})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
The strong formulation, is obtained by partial integration:
\\
\\
 $i=N$ case:
\begin{align*}
&\textrm{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}-p_N'(t)\psi(t)-a p_N(t)\psi(t)dt +p_N(T_{N})\psi(T_{N})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textrm{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}-p_i'(t)\psi(t)-a p_i(t)\psi(t)dt +p_i(T_{i})\psi(T_{i})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
This gives us the ODEs we wanted.
\end{proof}
\noindent
With the adjont equations we can find the gradient.
\begin{proposition} \label{penGrad_prop}
The gradient of (\ref{penalty_func}), $\hat J_{\mu}'$, with respect to the control $(v,\Lambda)$ is:
\begin{align}
\hat J_{\mu}'(v,\Lambda) = (v+p,p_{2}(T_1) -p_{1}(T_1),..., p_{N}(T_{N-1}) -p_{N}(T_{N-1})) \label{penalty grad}
\end{align}
\end{proposition}
\begin{proof}
If we first find $E_v^*$, $E_{\lambda}^*$, $J_v$ and $J_{\lambda}$ we can derive the gradient by simply inserting these expression into (\ref{pen_abs_grad}). We begin with the $E$ terms:
\begin{align*}
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi) \\
E_{\lambda_{i-1}}^i &= L_{y_{i-1}}^i[\cdot,\phi] = -(\cdot,\delta_{T_{i-1}}\phi)_i
\end{align*}
Notice that both of these forms are symmetric, and we therefore do not need to do more work to find their adjoints, they are however derived from the weak formulation, and it might therefore be easier to translate these forms to their strong counterpart:
\begin{align*}
E_v &=-1 \\
E_{\lambda_{i-1}}^i &= -\delta_{T_{i-1}}
\end{align*}
Then lets differentiate $ J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial v} J_{\mu} &= v \\
\frac{\partial}{\partial \lambda_i}J_{\mu} &= - \mu (y_{i}(T_i)-\lambda_i)
\end{align*}
We can insert the above derived expressions into formula (\ref{pen_abs_grad}) to find the gradient. To make it simple we separate the $v$ and $\Lambda$ terms. First we look at the gradient associated with the source term $v$:
\begin{align*}
\frac{\partial}{\partial v} \hat J_{\mu}(v,\Lambda) &= -E_vp +  \frac{\partial}{\partial v} J_{\mu} \\
&=p+v
\end{align*}
We then find the components of the gradient related to $\lambda_i$.
\begin{align*}
\frac{\partial}{\partial \lambda_i} \hat J_{\mu}(v,\Lambda) &= -E_{\lambda_i} p +  \frac{\partial}{\partial \lambda_i} J_{\mu} \\
&= p_{i+1}(T_i)-\mu (y_{i}(T_i)-\lambda_i)\\
 &= p_{i+1}(T_i)-p_i(T_i)
\end{align*}
Combining $\frac{\partial}{\partial v} \hat J_{\mu}$ and $ \frac{\partial}{\partial \lambda_i}$ for $i=1,..,N-1$ gives us the gradient \ref{penalty grad}.
\end{proof} 
\section{Parareal preconditioner} \label{pc sec}
Parallelizing the solution process of optimal control problems with time dependent differential equation constraints comes down to solving a series of penalized control problems. Since we have derived the gradient of these penalized problems for a specific example, we can now solve the control problem numerically using an optimization algorithm. We can for example use the steepest descent method (\ref{SD_itr}), which would create the following iteration for each penalized control problem:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_k\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method}
\end{align}
Alternatively we could use a BFGS iteration (\ref{BFGS_itr}), which would result in the following update:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kH^{k}\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{bfgs_method}
\end{align}
Where $H^k$ is the inverse Hessian approximation defined in (\ref{inv_H_apr}). To improve convergence of the unconstrained optimization solvers, we include the Parareal-based preconditioner, proposed in \cite{maday2002parareal}, in our optimization algorithms. The preconditioner $Q$ will be on the form:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 & Q_{\Lambda} \\
	\end{array} \right]\in \mathbb{R}^{n+N\times n+N},\quad Q_{\Lambda}\in\mathbb{R}^{N-1\times N-1} \label{PC_form}
\end{align} 
We see that $Q$ only affects the $N-1$ last components of the gradient, which is the part connected with the $\Lambda$ part of the control. The original control $v$ is therefore not directly affected by $Q$. For steepest descent, we apply $Q$, by modifying (\ref{gradient_method}) in the following way:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kQ\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method2}
\end{align}
For us to expect any improvement in convergence for the preconditioned steepest descent, $Q$ would have to resemble the Hessian of $\hat{J}_{\mu}$, at least for the $\Lambda$ part of the control. Applying $Q$ to the BFGS iteration, is done by setting the initial Hessian approximation $H^0=Q$. To be able to do this, we need $Q$ to be symmetric positive definite, since that is a requirement on $H^0$. 
\\
\\
We derive $Q$ by looking at a constructed optimal control problem that we call the virtual problem. The virtual problem is a control problem decomposed as detailed in section \ref{decomp_sec}, but its objective function $\bold J$ is set to be the penalty term, which only depends on the virtual control $\Lambda$. We already stated this problem in section \ref{algebraic_sec}, and by utilizing the algebraic Parareal formulation, we will try to find a good candidate for $Q_{\Lambda}$.
\subsection{Virtual problem} \label{vir_sec}
The Parareal-based preconditioner only affects the part of the gradient connected to the virtual control $\Lambda$. To motivate and derive $Q$, we therefore consider an optimal control problem where the real control $v$ is removed, and the objective function only depends on $\Lambda$. We have already presented this problem in section \ref{algebraic_sec}, but we restate it here for future reference. However, before we do this let us first properly define the fine and coarse propagators.
\begin{definition}[Fine and coarse propagator] \label{prop_def}
Let $f(y(t),t)=0$ be a time dependent differential equation without a source term. Given $\Delta T=\frac{T}{N}$ and an initial condition $\omega$, let $y_f$ and $y_c$ be a fine and a coarse numerical solution of the initial value problem:
\begin{align}
 \left\{
     \begin{array}{lr}
		f(y(t),t)=0 \ \quad \textrm{For $t \in (0,\Delta T)$} \\
		y(0)=\omega
	\end{array}
	\right.	
\end{align}
We then define the fine propagator as $\bold F_{\Delta T}(\omega) = y_f(\Delta T)$ and the coarse propagator as $\bold G_{\Delta T}(\omega) = y_c(\Delta T)$. We also define the lower triangonal matrices $M,\bar M\in\mathbb{R}^{N-1\times N-1}$ as: 
\begin{align*}
M= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} & \mathbbold{1}  \\
   \end{array}  \right],
\bar M= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{G}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{G}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{G}_{\Delta T} & \mathbbold{1}   \\
   \end{array}  \right].
\end{align*}
\end{definition}
\noindent
We then use the fine propagator $\bold F_{\Delta T}(\omega)$ to define the virtual problem.
\begin{definition}[Virtual problem]
Given a fine propagator $\bold F_{\Delta T}$, that solves a time dependent differential equation $f(y(t),t)=0$, an initial condition $\lambda_0=y_0$ and the control variable $\Lambda=(\lambda_1,...,\lambda_ {N-1})$, the virtual control problem is defined as follows:
\begin{align}
&\min_{\Lambda}\bold{J}(\Lambda,y) = \sum_{i=1}^{N-1} (y_{i-1}(T_{i})-\lambda_{i})^2 \label{virtual_func} \\
&\textrm{Subject to } \ y_{i-1}(T_{i}) = \bold F_{\Delta T}(\lambda_{i-1}) \ i=1,...,N-1 \label{virtual}
\end{align}
\end{definition}
\noindent
In chapter \ref{parareal_chap} we explained how the virtual problem could be solved by setting $\lambda_i= \bold F_{\Delta T}(\lambda_{i-1})$, which is the same as solving $\bold J(\Lambda,y)=0$. This equation could be written up on matrix form as:
\begin{align}
M \ \Lambda = H. \label{Parareal_equation}
\end{align}
The $H$ on right hand side of the above equation is the propagator applied to the initial condition:
\begin{align*}
H = \left[ \begin{array}{c}
   \bold F_{\Delta T}( y_0) \\
   0 \\
   \cdots \\
   0 \\
   \end{array}  \right].
\end{align*}
In section \ref{algebraic_sec} we explained how the Parareal algorithm could be reformulated as a preconditioned fix point iteration solving equation (\ref{Parareal_equation}), expressed as follows:
\begin{align}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}(H-M\Lambda^k)\label{par_mat_sys}
\end{align}
Where $\bar{M}$ is the coarse version of the matrix $M$ stated in definition \ref{prop_def}. When we are solving the original optimal control problem we do not try to find a triple $(v,\Lambda,y)$ that solves $J_{\mu}(v,\Lambda,y)=0$. Instead we try to solve $\hat J_{\mu}'(v,\Lambda)=0$. To find the Parareal-based preconditioner, we therefore try to find a similar expression to (\ref{Parareal_equation}) for $\bold{\hat{J}}'(\Lambda)=0$. To be able to find this expression, we first need to define the coarse and fine adjoint propagators.
\begin{definition}[Fine and coarse adjoint propagator] \label{adjoint_prop_def}
Let $f(y(t),t)=0$ be a time dependent differential equation. Given $\Delta T$ a state $y(t)$ and an initial condition $\omega$, let $p_f$ and $p_c$ be a fine and a coarse numerical solution of the initial value problem:
\begin{align}
 \left\{
     \begin{array}{lr}
		f'(y(t),t)^*p(t)=0 \ \quad \textrm{For $t \in (0,\Delta T)$} \\
		p(\Delta T)=\omega
	\end{array}
	\right.	
\end{align}
We then define the fine adjoint propagator as $\bold F_{\Delta T}^*(\omega) = p_f(0)$ and the coarse adjoint propagator as $\bold G_{\Delta T}^*(\omega) = p_c(0)$. We also define adjoint versions of the matrices $M$ and $\bar M$ as: 
\begin{align*}
M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{F}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{F}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{F}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right],
\bar M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{G}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{G}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{G}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right].
\end{align*}
\end{definition} 
\noindent
Using the matrices from definition \ref{adjoint_prop_def} we can write up the following proposition concerning the gradient of the reduced objective function of the virtual problem.
\begin{proposition} \label{vir_grad_prop}
The reduced objective function of the virtual problem (\ref{virtual_func}-\ref{virtual}) is:
\begin{align}
\bold{\hat J}(\Lambda) = \sum_{i=1}^{N-1} (\bold F_{\Delta T}(\lambda_{i-1})-\lambda_{i})^2.\label{reduced_viritual}
\end{align}
Solving $\bold{\hat J}'(\Lambda)=0$ is equivalent to resolving the system:
\begin{align}
M^* \ M \ \Lambda \ = \ M^* \ H. \label{vir_grad_sys}
\end{align}
A preconditioned fix point iteration for equation (\ref{vir_grad_sys}) inspired by the Parareal formulation (\ref{par_mat_sys}) is therefore:
\begin{align}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}\bar M^{-*}(M^*H-M^*M\Lambda^k). \label{grad_fix_iter}
\end{align}
\end{proposition}
\begin{proof}
Luckily for us we have already derived the gradient of $\bold{\hat J}$ in (\ref{penalty grad}). There we stated the gradient for the penalized version of the example problem (\ref{exs_J}-\ref{exs_E}). If we ignore the part of this gradient related to the real control $v$, we get the following expression for $\bold{\hat J}'$:
\begin{align*}
\hat{\bold J}'(\Lambda) = \{p_{i+1}(T_i)-p_{i}(T_i)\}_{i=1}^{N-1}.
\end{align*}
Here $p_i$ refers to the decomposed adjoint equation on interval $[T_{i-1},T_{i}]$. We now want to show that setting $p_{i+1}(T_i)-p_{i}(T_i)=0$ for $i=1,...,N-1$ is equivalent to equation \ref{vir_grad_sys}. To do this we will simply write out the expression $M^*(M\Lambda-H)$ and show that it equals $\hat{\bold J}'(\Lambda)$. We start with $M\Lambda-H$.
\begin{align*}
M \ \Lambda - H  = \left( \begin{array}{c}
	\lambda_1-\bold{F}_{\Delta T}(\lambda_0)\\
	\lambda_2-\bold{F}_{\Delta T}(\lambda_1) \\
	\cdots \\
	\lambda_{N-1}-\bold{F}_{\Delta T}(\lambda_{N-1}) 
	\end{array} \right).
\end{align*}
Notice that $\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i$ is the initial condition of $i$-th adjoint equation, i.e. $p_i(T_i)=\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i$. By exploiting this, and multiplying $M\Lambda-H$ with $M^*$ we get:
\begin{align}
M^* (M \ \Lambda-H)&=
	\left( \begin{array}{c}
	 \bold{F}_{\Delta T}^*(p_2( T_2))-p_1(T_1)\\
	\bold{F}_{\Delta T}^*(p_3( T_3))-p_2(T_2)\\
	\cdots \\
	-p_{N-1}(T_{N-1})
	\end{array} \right)
	\\
	&=\left( \begin{array}{c}
	p_2(T_1)-p_1(T_1)\\
	p_3(T_2)-p_2(T_2)\\
	\cdots \\
	p_{N-1}(T_{N-2})-p_{N-2}(T_{N-2}) \\
	-p_{N-1}(T_{N-1})
	\end{array} \right).
\end{align}
The last step is done by using $p_i(T_{i-1})=-F_{\Delta T}^*(-p_i(T_i))$, and this is possible since the adjoint equation is always linear. We see that the $i$-th component of $M^* (M \Lambda-H)$ is equal to $p_{i+1}(T_i)-p_{i}(T_i)$ for $i\neq N-1$. The last component of $M^* (M \Lambda-H)$ is $-p_{N-1}(T_{N-1})$, and we are therefore missing $p_N(T_{N-1})$. This is however unproblematic since in context of the the virtual problem $p_N(T_{N-1})=0$. This shows us that $\hat{\bold J}'(\Lambda)= M^* (M \Lambda-H)$, which means that $\hat{\bold J}'(\Lambda)=0 \iff M^*M\Lambda =M^*H$. Since $\bar M$ and $\bar M^*$ approximates $M$ and $M^*$, $\bar{M}^{-1}\bar M^{-*}$ would be a natural preconditioner for a fix point iteration solving $M^*M\Lambda =M^*H$. 
\end{proof}
\noindent
Proposition \ref{vir_grad_prop} motivates $Q_{\Lambda}=\bar{M}^{-1}\bar M^{-*}$ as a preconditioner for solvers of decomposed and penalized optimal control problems, and this is actually the Parareal-based preconditioner proposed in \cite{maday2002parareal}. Inserting $Q_{\Lambda}$ into $Q$ yields the following:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 &  \bar{M}^{-1}\bar{M}^{-*}\\
	\end{array} \right]. \label{Q_PC}
\end{align}  
In \cite{maday2002parareal} $Q$ is proposed as a preconditioner for a steepest descent method. We do however not know if $Q$ is positive definite, or if it is in any shape or form related to the Hessian of the objective function. We will investigate these questions further by reformulating the reduced objective function (\ref{reduced_viritual}) for the virtual problem to a least squares problem.
\subsection{Virtual least squares problem}
Looking at the equation $M^*M\Lambda =M^*H$ we recognize the normal equation, which is connected to linear least squares problems. We therefore suspect that the virtual problem can be reformulated as a least squares problem. It turns out that this is indeed the case. We write up the new formulation in definition \ref{VLSPD}.
\begin{definition}[Virtual least squares problem] \label{VLSPD}
Given a propagator $\bold F_{\Delta T}$ as defined in definition \ref{prop_def} and an initial condition $\lambda_0=y_0$ for the state equation, the least squares formulation of the virtual optimal control problem (\ref{virtual_func}-\ref{virtual}) reads as follows:
\begin{align}
\min_{\Lambda\in\mathbb{R}^{N-1}}\hat{\bold J}(\Lambda) = x(\Lambda)^Tx(\Lambda), \label{non_lin_LS}
\end{align}
where the vector function $x:\mathbb{R}^{N-1}\rightarrow \mathbb{R}^{N-1}$ is:
\begin{align}
x(\Lambda)= \left( \begin{array}{c}  
   \lambda_1 - \bold F_{\Delta T}(\lambda_0) \\ 
   \lambda_2 - \bold F_{\Delta T}(\lambda_1) \\
   \cdots  \\
   \lambda_{N-1} -\bold F_{\Delta T}(\lambda_{N-1}) \\
   \end{array}  \right).
\end{align}
\end{definition}
\noindent
We are now interested in finding the Hessian of $\hat{\bold J}(\Lambda)$, which we hope to relate to the Parareal-based preconditioner. 
\begin{proposition}\label{NonLin_prop}
The Hessian of function (\ref{non_lin_LS}) is
\begin{align*}
\nabla^2 \hat{\bold J}(\Lambda) &= 2\nabla x^T\nabla x + 2\sum_{i=1}^{N-1} \nabla^2 x_i(\Lambda) x_i(\Lambda)\\
&=2M(\Lambda)^TM(\Lambda) + 2D(\Lambda)
\end{align*}
Here $D(\Lambda)$ is a diagonal matrix with diagonal entries 
\begin{align*}
D_i=-\bold{F}_{\Delta T}''(\lambda_i)(\lambda_{i+1}-\bold F_{\Delta T}(\lambda_i)) \quad i=1,...,N-1,
\end{align*}
while $M(\Lambda)$ is the linearised forward model:
\begin{align*}
M(\Lambda) &= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T}'(\lambda_{1}) & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T}'(\lambda_{2}) & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T}'(\lambda_{N-1}) & \mathbbold{1}  \\
   \end{array}  \right]
\end{align*}	
\end{proposition}
\begin{proof}
We start by differentiating $\hat{\bold J}$:
\begin{align*}
\nabla \hat{\bold J}(\Lambda) &= 2 \nabla x(\Lambda)^T x(\Lambda)\\
&=2\sum_{i=1}^{N-1} \nabla x_i(\Lambda) x_i(\Lambda)
\end{align*}
If we now differentiate $\nabla \hat{\bold J}$, we get:
\begin{align*}
\nabla^2 \hat{\bold J}(\Lambda) &= 2\nabla x^T\nabla x + 2\sum_{i=1}^{N-1} \nabla^2 x_i(\Lambda) x_i(\Lambda)
\end{align*}
We see that $\nabla x(\Lambda)=M(\Lambda)$, by looking at $\frac{\partial x_i}{\partial \lambda_j}$
\begin{align*}
\frac{\partial x_i}{\partial \lambda_j} = \left\{
     \begin{array}{lr}
		1 \quad\quad\quad\quad\quad i=j\\
		-\bold F_{\Delta T}'(\lambda_{j}) \quad i>1 \wedge j=i-1 \\
		0 \quad\quad\quad\quad\quad i\neq j \vee j\neq i-1
	\end{array}
   \right.	
\end{align*}
We can similarly find $\nabla^2 x_i$ by differentiating $x$ twice:
\begin{align*}
\frac{\partial^2 x_i}{\partial \lambda_j\partial\lambda_k} = \left\{
     \begin{array}{lr}
		-\bold F_{\Delta T}''(\lambda_{j}) \quad i>1 \wedge j=k=i-1 \\
		0 \quad\textrm{in all other cases}
	\end{array}
   \right.	
\end{align*}
Now summing up the terms $\nabla^2 x_i(\Lambda)x_i(\Lambda)$ would yield the diagonal matrix $D(\Lambda)$ described in proposition \ref{NonLin_prop}.
\end{proof}
\noindent
The first term of $\nabla^2 \hat{\bold J}(\Lambda)=2M(\Lambda)^TM(\Lambda) + 2D(\Lambda)$ resembles $M^*M$ from the previous section, while the second term $2D(\Lambda)$ is new. $D(\Lambda)$ is a diagonal matrix where the diagonal entries consists of products between the second derivative of $\bold F_ {\Delta T}$ and the residuals $\lambda_{i+1}-\bold F_{\Delta T}(\lambda_i)$. If the governing equation of the propagator $\bold F_ {\Delta T}$ is linear, $\bold F_{\Delta T}''(\lambda_i)=0$. This would again mean that $D(\Lambda)=0$ and that $\nabla^2 \hat{\bold J}(\Lambda)=2M(\Lambda)^TM(\Lambda)$. We will therefore split our discussion of the Hessian of $\hat{\bold J}$ into two cases. In the first we assume the sate equation is linear, while in the second case we discuss problems with non-linear state equations.
\subsubsection{Linear state equations}
Assuming that the state equation is linear means that $\nabla^2 \hat{\bold J}(\Lambda)=2M(\Lambda)^TM(\Lambda)$. Differentiating the propagator $\bold F_{\Delta T}$ is the same as linearising its governing equation. When the governing equation is it self linear, linearising it does not change the equation. Therefore $\bold F_{\Delta T}'(\lambda_i)\lambda_i = \bold F_{\Delta T}(\lambda_i)$. This means that the $M$ matrix from section \ref{vir_sec} is equal to $M(\Lambda)$. The same is true for $M^*$ and $M(\Lambda)^T$. Since  $\nabla^2 \hat{\bold J}(\Lambda)=2M^*M$ we see that the Parareal-based preconditioner proposed in \cite{maday2002parareal} is in fact related to the inverse Hessian of the reduced penalized objective function. If we can show that $\bar M^*\bar M$ is a positive definite matrix, we can use $Q$ as an initial approximation of the inverse Hessian in the BFGS optimization algorithm. This is however quite simple to do, as we will see in the proof of the following proposition.
\begin{proposition} \label{pos_def_prop}
 If $\bold G_{\Delta T}$ and $\bold G_{\Delta T}^*$ are based on consistent numerical methods, $\bar M^*=\bar M^T$, and the matrix $\bar M^*\bar M$ is positive definite.
\end{proposition}
\begin{proof}
If $\bold G_{\Delta T}$ and $\bold G_{\Delta T}^*$ are based on consistent numerical methods, $\bold G_{\Delta T}(\omega)=\bold G_{\Delta T}^*(\omega)$. When inserting this into the matrices $\bar M$ and $\bar M^*$ from definition \ref{prop_def} and \ref{adjoint_prop_def}, we clearly see that $\bar M^*=\bar M^T$. For $M^*M$ to be positive definite, the following two conditions must hold:
\begin{align*}
&1.\quad x^T\bar M^*\bar Mx \geq 0 \quad \forall x\in\mathbb{R}^{N-1} \\
&2.\quad x^T\bar M^*\bar Mx =0 \iff x=0
\end{align*}
The first conditions hold due to $\bar M^*=\bar M^T$:
\begin{align*}
x^T\bar M^*\bar Mx = (\bar Mx)^T\bar Mx = ||Mx||^2 \geq 0.
\end{align*}
The second condition hold if $\bar M$ is invertible. This is true because $\bar M$ is a triangular matrix, with identity on its diagonal, and therefore has a determinant equal to 1. The determinant of a matrix being unequal to zero is equivalent with it being invertible, which means that our matrix $\bar M$ is invertible. This also means that $M^*M$ is positive definite, since both requirements for positive definiteness are satisfied. 
\end{proof}
\noindent
Proposition \ref{pos_def_prop} shows that the $\bar M^*\bar M$ matrix stemming from the virtual problem is positive definite. We can therefore use it as an initial Hessian approximation in the BFGS algorithm, at least as long as $\bold G_ {\Delta T}$ and $\bold G_ {\Delta T}^*$ are consistent. Now let us take a look at the case where the governing equation of $\bold F_{\Delta T}$ is non-linear.
\subsubsection{Non-linear state equations}
Unlike the Hessian of the linear problem the Hessian of the non-linear problem consists of two parts. One is the linearised forward model multiplied with its adjoint, while the second part is a diagonal matrix related to the second derivative of the propagator $\bold F_{\Delta T}$, and the residuals $\lambda_i-\bold F_{\Delta T}$. The first part of $\nabla^2 \bold{\hat{J}}$ is analogue to the Hessian of the linear problem. It is symmetric positive definite, and taking its inverse corresponds to first applying the backwards model, and then the forward model. What makes the Hessian of the non-linear problematic is therefore its second term. The first issue with the diagonal matrix $D(\Lambda)$, is how to calculate $\bold F_{\Delta T}''$. Another issue is that we can not guarantee that the sum of $M(\Lambda)^TM(\Lambda)$ and $D(\Lambda)$ is a positive matrix, and the same problem would arise in a coarse approximation of $\nabla^2 \bold{\hat{J}}$. The lack of positivity is a problem since we want to use the coarse approximation as an initial inverted Hessian approximation in the BFGS-algorithm.
\\
\\
A way to get around the $D(\Lambda)$ term in the Hessian for non-linearly constrained problem, is simply to ignore it. This leaves us with the $M(\Lambda)^TM(\Lambda)$ term, which we know how to deal with. Ignoring the term depending on the second derivative and the residual is actually a known strategy for for solving non-linear least square problems. Details can be found in \cite{nocedal2006numerical}. A justification for this approach, is that at least in instances where we are close to a solution, the $\lambda_i-\bold F_{\Delta T}$ terms will be close to zero, and the $M(\Lambda)^TM(\Lambda)$ term will therefore dominate the Hessian. Ignoring the $D(\Lambda)$ term means that we can define an inverse Hessian approximation based on a coarse propagator $\bold G_{\Delta T}$ in the same way as we did for the problem with linear state equation constraints. This means that we define a matrix $\bar M(\Lambda)$:
\begin{align}
\bar M(\Lambda) &= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{G}_{\Delta T}'(\lambda_{1}) & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{G}_{\Delta T}'(\lambda_{2}) & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{G}_{\Delta T}'(\lambda_{N-1}) & \mathbbold{1}  \\
   \end{array}  \right] \label{ppc_linearized}
\end{align}
The term $\bar{M}(\Lambda)^{-1}\bar{M}(\Lambda)^{-*}$ can then be used in an approximation of the inverse Hessian, as detailed in section \ref{vir_sec}.
\subsection{Parareal-based precoditioner for example problem}
To illustrate what $Q$ actually will look like we write up $\bar M^*\bar M$ for our example problem (\ref{exs_J}-\ref{exs_E}). The state and adjoint equation of this problem is:
\begin{align}
y'(t) &= ay(t) + v(t), \label{ppc_state} \\
p'(t) &= -ap(t). \label{ppc_adjoint}
\end{align}
The state equation includes a source term, which will not be included in the governing equation of the propagators, since the propagators are based on the virtual sourceless problem. This means that the governing equation of $\bold G_{\Delta T}$ is $y'(t) = ay(t)$. Alternatively we could let (\ref{ppc_state}) govern $\bold G_{\Delta T}$, but instead use $\bar M(\Lambda)$ from (\ref{ppc_linearized}) in our preconditioner, which would produce the same result. 
\\
\\
Let us now try to write out $\bar M^*\bar M$ for our example problem, when we have decomposed the time interval into $N$ subintervals. We first need to choose a numerical method to discretize the state and adjoint. In this example we will use the implicit Euler scheme from section \ref{FD_sub_sec}, with $\Delta T=\frac{T}{N}$. We can then write up $\bold G_{\Delta T}(\omega)$ and $\bold G_{\Delta T}^*(\omega)$:
\begin{align*}
\frac{\bold G_{\Delta T}(\omega)-\omega}{\Delta T}&=  a\bold G_{\Delta T}(\omega) \\
&\Rightarrow \bold G_{\Delta T}(\omega)= \frac{\omega}{1-a\Delta T} \\
\frac{\omega-\bold G_{\Delta T}^*(\omega)}{\Delta T}&= -a\Delta T \bold G_{\Delta T}^*(\omega) \\
&\Rightarrow \bold G_{\Delta T}^*(\omega)= \frac{\omega}{1-a\Delta T} 
\end{align*}
Since $\bold G_{\Delta T}(\omega)= \bold G_{\Delta T}^*(\omega)$, using implicit Euler both forwards and backwards produce consistent coarse propagators. We can now write up an exact expression for $\bar M\in\mathbb{R}^{N-1\times N-1}$. 
\begin{align*}
\bar M = \left[ \begin{array}{cccc}
   	1 & 0 & \cdots & 0 \\  
   	-\frac{1}{1-a\Delta T} & 1 & 0 & \cdots \\ 
   	0 &-\frac{1}{1-a\Delta T} & 1  & \cdots \\
   	0 &\cdots &-\frac{1}{1-a\Delta T} & 1  \\
  	\end{array}  \right].
\end{align*}
By traversing $\bar M$ we get $\bar M^*$. When we apply $Q$, we are not using $\bar M^*\bar M$, but instead its inverse. Let us illustrate how this is done for our example problem, when $N=4$. We first decompose $I=[0,T]$ into four sub-intervals $[T_0,T_1], [T_1,T_2], [T_2,T_3]$ and $[T_3,T_4]$. If we then evaluate the discrete gradient for a real control variable $v\in\mathbb{R}^{n+1}$ and a virtual control $\Lambda =(\lambda_1,\lambda_2,\lambda_3)$, the result is $\hat J_{\mu}(v,\Lambda)\in\mathbb{R}^{N+n}$. Multiplying $Q$ with $\hat J_{\mu}(v,\Lambda)$ will only affect its three last components, which we name $J_{\lambda_1},J_{\lambda_2}$ and $J_{\lambda_3}$. Applying $Q$ to $\hat J_{\mu}$ is done in two steps. We first multiply with $\bar M^{-*}$ based on on the propagator $\bold G_{\Delta T}^*= -\frac{1}{1-a\Delta T} $ 
\begin{align*}
\bar{J_{\lambda_1}} &=J_{\lambda_1} -\frac{1}{1-a\Delta T}(J_{\lambda_2} -\frac{1}{1-a\Delta T}J_{\lambda_3})\\
\bar{J_{\lambda_2}} &=J_{\lambda_2} -\frac{1}{1-a\Delta T}J_{\lambda_3}\\
\bar{J_{\lambda_3}} &=J_{\lambda_3} 
\end{align*} 
The second step is then to apply the forward system based on the coarse propagator $\bold G_{\Delta T}= -\frac{1}{1-a\Delta T} $:
\begin{align*}
\bar{\bar{J_{\lambda_1}}}&=\bar{J_{\lambda_1}} \\
\bar{\bar{J_{\lambda_2}}}&=\bar{J_{\lambda_2}}-\frac{1}{1-a\Delta T}\bar{J_{\lambda_1}} \\
\bar{\bar{J_{\lambda_3}}}&=\bar{J_{\lambda_3}} -\frac{1}{1-a\Delta T}(\bar{J_{\lambda_2}}-\frac{1}{1-a\Delta T}\bar{J_{\lambda_1}})
\end{align*} 
The result of multiplying $Q$ with the discrete penalized gradient is that the three last components of $\hat J_{\mu}(v,\Lambda)$ is changed to $\bar{\bar{J_{\lambda_1}}},\bar{\bar{J_{\lambda_2}}}$ and $\bar{\bar{J_{\lambda_3}}}$. 
\\
\\
We end the section on the Parareal-based preconditioner with an important note about what happens when $N=2$. If we decompose the time domain into $N=2$ subdomains, both $\bar M$ and $\bar M^*$ becomes the identity matrix. This means that for $N=2$, $Q=\mathbbold{1}$, and therefore have no effect. Since $Q$ has no effect for $N=2$, we might also expect that for "small" $N$ the impact of applying $Q$ to the penalized gradient is only modest, and that the usefulness of $Q$ only materializes for higher values of decomposed subintervals $N$.
