\chapter{Parareal BFGS preconditioner} \label{method_chap}
In the previous chapter we saw that the parareal scheme allows us to parallelize time dependent differential equations in their temporal direction. In this chapter we will look at how to parallelize optimal control problems with time dependent differential equation constraints in temporal direction. 
\\
\\
This chapter consists of three sections. In the first section we decompose the time domain as we did in section \ref{Para_dcomp_sec}, only now in the context of control problems with time dependent DE constraints. Decomposing the time interval leads to a reformulation of the control problem that includes extra constraints on the state equation. How to handle these new constraints are dealt with in section \ref{penalty_sec}. We use the same approach as \cite{maday2002parareal} namely the penalty method. This is a simplified version of the augmented Lagrangian approach used in \cite{rao2016time} for parallel in time 4d variational data assimilation. We demonstrate the use of the penalty method by revisiting the example problem from section \ref{example_sec}.
\\
\\
In the last section a Parareal based preconditioner to be used in the optimization algorithms solving the optimal control problems is presented. This preconditioner originally proposed in \cite{maday2002parareal} is derived using ides from subsection \ref{algebraic_sec} and we will in chapter \ref{Experiments chapter} see that it is crucial for the parallel in time algorithm to obtain any meaningful speedup. 
\section{Optimal control problem with time-dependent DE constraints on a decomposed time interval} \label{decomp_sec}
Before we start to decompose the time interval, let us again state the general problem that we want to solve:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v) \label{initial problem1}\\
\textrm{Subject to:} \ &E(y(t),v)=0  \quad t\in [0,T] \label{initial problem}
\end{align}
To introduce parallelism, we decompose $I=[0,T]$ into $N$ subintervals $I_i=[T_{i-1},T_i]$, with $T_0=0$ and $T_N=T$. To be able to solve the differential equation $E$ on each interval $I_i$, we introduce intermediate initial conditions $y(T_i)=\lambda_i$ for $i=1,...,N-1$. This means that instead of finding $y$ by solving $E$ on the entire time domain $I$, we can now find $y$ by solving $E$ separately on on each subinterval $I_i$. The problem (\ref{initial problem}) now reads:
\begin{align}
\underset{y\in Y,v\in V}{\text{min}} \ &J(y(t),v)  \label{decomposed problem1}\\
\textrm{Subject to:} \ &E^i(y^i(t),v)=0 \quad t\in [T_{i-1},T_i] \quad \forall i \label{decomposed problem}
\end{align} 
Since we want the state $y$ to be continuous, we also need the following conditions:
\begin{align}
y^{i-1}(T_i)=y^i(T_i)=\lambda_i \ \ i=1,..,N-1 \label{Extra constraints}
\end{align} 
Both the problems (\ref{initial problem1}-\ref{initial problem}) and (\ref{decomposed problem1}-\ref{Extra constraints}) are constrained problems, which we solve by reducing them to unconstrained problems. In the original setting this can easily be done if we assume that each control variable $v$ corresponds to a unique solution $y$ of the state equation $E$. We can then define a reduced objective function $\hat{J}(v)$, and minimize it with respect to $v$, i.e solve the unconstrained problem:
\begin{align*}
\underset{v\in V}{\text{min}} \ \hat J(v)
\end{align*} 
Assuming that the decomposed state equations also can be uniquely resolved $\forall v$, we can again define a reduced objective function  $\hat{J}$. However because of the extra conditions (\ref{Extra constraints}) the reduction of (\ref{decomposed problem}) still produces a constrained problem:
\begin{align}
&\underset{v\in V}{\text{min}} \ \hat J(v) \label{constrained reduced j}\\
&y^{i-1}(T_i)=\lambda_i \ \ \forall i \label{constrained reduced}
\end{align} 
\section{The penalty method} \label{penalty_sec}
To solve the constrained problem (\ref{constrained reduced j}-\ref{constrained reduced}), we will use the penalty method\cite{nocedal2006numerical}, which transforms constrained problems into a series of unconstrained problems by incorporating the constraints into the functional. Incorporating the constraints means penalizing not satisfying the constraints. To use the penalty method on (\ref{constrained reduced j}-\ref{constrained reduced}) we first introduce the initial conditions to the decomposed state equations as variables $\Lambda = (\lambda_1,..,\lambda_{N-1})^T$, and then define the penalized objective function $\hat J_{\mu}$:
\begin{align}
\hat J_{\mu}(v,\Lambda) = \hat J(v) + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{pen_obj_J}
\end{align}
With $\mu>0$. If we now minimize $\hat{J}_{\mu}$ with respect to $(v,\Lambda)$, while letting $\mu$ tend to infinity, we hope that the solution satisfies the conditions (\ref{Extra constraints}), while also minimizing the actual problem (\ref{constrained reduced j}-\ref{constrained reduced}). The algorithmic framework of this reads:
\\
\\
\begin{algorithm}[H]
\KwData{Choose $\mu_0,\tau_0>0$, and some initial control $(v^0,\Lambda^0$}
\For{$k=1,2,...$}{
Find $(v^k,\Lambda^k)$ s.t. $\parallel\nabla \hat J_{\mu_{k-1}}(v^k,\Lambda^k)\parallel<\tau_{k-1}$\;
\eIf{STOP CRITERION satisfied}{
$\bold{Stop}$ algorithm\;
}{
Choose new $\tau_k\in(0,\tau_{k-1})$ and $\mu_k\in(\mu_{k-1},\infty) $\;
}
}
\caption{Penalty framework\label{PEN_ALG}}
\end{algorithm}
\noindent
Assuming that we have a solution $v$ minimizing $\hat J$, one would hope that the iterates $\{v^k\}$ from the penalty method converges to the solution of the non-penalized problem, that is:
\begin{align*}
\lim_{k\rightarrow \infty} v^k =v
\end{align*}
From \cite{nocedal2006numerical} we get a result that deals with this:
\begin{theorem}
Assume $v^k$ is the exact global minimizer of $J_{\mu_k}$, then each limit point of the sequence $\{v^k\}$ is a solution of the problem (\ref{initial problem}).
\end{theorem}
\begin{proof}
\cite{nocedal2006numerical}
\end{proof}
\noindent
The above result shows that the penalty algorithmic framework actually produces a solution to the original problem. There are however parts of the above framework, that still needs special attention, namely how to find $(v^k,\Lambda^k)$ in each iteration, how to update $\mu_k$ and $\tau_k$ and how to choose an adequate stopping criteria. Finding the optimal control for each iteration is done by applying an optimization method for unconstrained problems, that is dependent on the gradient at $(v^k,\Lambda^k)$. Let us therefore differentiate the penalized objective function.
\subsection{The gradient of the penalized objective function}
To find the gradient of \ref{pen_obj_J} we start by differentiating $\hat J_{\mu}(v,\Lambda)$:
\begin{align}
\hat J_{\mu}'(v,\Lambda) &= DJ_{\mu}(y(v,\Lambda),v,\Lambda) \\
&= y'(v,\Lambda)^*\frac{\partial}{\partial y} J_{\mu}(y(v,\Lambda),v,\Lambda) + (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu}(y(v,\Lambda),v,\Lambda)
\end{align} 
To find an expression for $y'(v,\Lambda)^*$ we differentiate the state equation $E$:
\begin{align*}
DE(y(v,\Lambda),v,\Lambda)=0 &\Rightarrow E_y(y(v,\Lambda),v,\Lambda)y'(v,\Lambda)=-E_v(y(v,\Lambda),v,\Lambda)- E_{\Lambda}(y(v,\Lambda),v,\Lambda)\\ &\Rightarrow y'(v)=-E_y(y(v,\Lambda),v,\Lambda)^{-1}((E_v(y(v,\Lambda),v,\Lambda)+E_{\Lambda}(y(v,\Lambda),v,\Lambda)) \\ &\Rightarrow y'(v,\Lambda)^* = -(E_v(y(v,\Lambda),v,\Lambda)^*+E_{\Lambda}(y(v,\Lambda),v,\Lambda)^*)E_y(y(v,\Lambda),v,\Lambda)^{-*}
\end{align*}
Inserting the above expression for $ y'(v,\Lambda)^*$ into the gradient yields:
\begin{align}
\hat J_{\mu}'(v,\Lambda) &=-(E_v^*+E_{\Lambda}^*)E_y^{-*}\frac{\partial}{\partial y} J_{\mu} + (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu} \\
&=-(E_v^*+E_{\Lambda}^*)p+ (\frac{\partial}{\partial v}+\frac{\partial}{\partial\Lambda})J_{\mu} \label{pen_abs_grad}
\end{align}
Where $p$ is the solution of the adjoint equation:
\begin{align*}
E_y^*p=\frac{\partial}{\partial y}J_{\mu}
\end{align*} 
Notice that the state equation $E$ consists of several equations defined separately on each of the decomposed subintervals. The result is that the adjoint equation also consists of several equations defined on each interval. To see this clearly we will derive the adjoint and the gradient for the example problem (\ref{exs_J}-\ref{exs_E}).
\subsection{Deriving the adjoint for the example problem}
Let us first recall the example optimal control problem with ODE constraints:
\begin{align*}
&J(y,v) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 \\
&\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t}y(t)=a y(t) + v(t) \ \quad t\in(0,T)\\
       	y(0)=y_0
     \end{array}
   \right.
\end{align*}
We can now decompose the interval $[0,T]$ into $N$ subintervals $\{[T_{i-1},T_{i}]\}_{i=1}^{N}$, and then define the above state equation on each interval, which forces us to penalize the objective function. The decomposed state equations will look like:
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y^i(t)=a y^i(t) + v(t) \quad t\in(T_{i-1},T_{i})\\
       	y^i(T_{i-1})=\lambda_{i-1}
     \end{array}
   \right. \label{decomp_E}
\end{align}
The reduced penalized objective function will be given as:
\begin{align}
\hat J_{\mu}(v,\Lambda) = \frac{1}{2}\int_0^Tv(t)^2dt + \frac{\alpha}{2}(y(T)-y^T)^2 + \frac{\mu}{2}\sum_{i=1}^{N-1}(y^{i-1}(T_i)-\lambda_i)^2 \label{penalty_func}
\end{align}
\begin{theorem}
The adjoint equation of problem (\ref{exs_J}-\ref{exs_E}) on interval $[T_{N-1},T_N]$ is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_N =a p_N  \\
	p_N(T_{N}) = \alpha( y_N(T_{N})-y_T)
	\end{array}
   \right. \label{end adjoint}
\end{align}
On $[T_{i-1},T_i]$ the adjoint equation is:
\begin{align}
\left\{
     \begin{array}{lr}
	-\frac{\partial }{\partial t}p_i =ap_i  \\
	p_i(T_{i}) = \mu(y_{i}(T_{i})-\lambda_{i} )
	\end{array}
   \right. \label{exs_adjoint}
\end{align}
\end{theorem} 
\begin{proof}
Lets begin as we did for the non-penalty approach, by writing up the weak formulation of the state equations:
\begin{gather*}
\textrm{Find $y_i \in L^2(T_{i-1},T_i)$ such that }\\
L^i[y_i,\phi] = \int_{T_{i-1}}^{T_{i}}-y_i(t)(\phi'(t) +a \phi(t))+v(t)\phi(t)dt -\lambda_{i-1}\phi(T_{i-1})+ y_i(T_i)\phi(T_i) =0\\ \forall \ \phi \in C^{\infty}((T_{i-1},T_{i}))
\end{gather*} 
To find the adjoint equations we differentiate the $E^i$s and the functional $\hat J_{\mu}$ with respect to $y$. To simplify notation, let $(\cdot,\cdot)_i$ be the $L^2$ inner product of the interval $[T_{i-1},T_i]$. 
\begin{align*}
E_y^i=L_y^i[\cdot,\phi]=(\cdot,-(\frac{\partial}{\partial t} + a - \delta_{T_i})\phi)_i 
\end{align*}
Lets differentiate $\hat J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial y} \hat J_{\mu}= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) + \mu \sum_{i=1}^{N-1} \delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) 
\end{align*}
Since $y$ really is a collection of functions, we can differentiate $\hat J_{\mu}$ with respect to $y_i$. This gives us:
\begin{align*}
\frac{\partial}{\partial y_N}\hat J_{\mu}&= \alpha\delta_{T_{N}}(y_n(T_{N})-y_T) \\
\frac{\partial}{\partial y_i}\hat J_{\mu} &= \mu\delta_{T_{i}}(y_{i}(T_i)-\lambda_i ) \ i\neq N
\end{align*}
We will now find the adjoint equations, by finding the adjoint of the $E_y^i$s. This is done as above, by inserting two functions $v$, $w$ into $L_y^i[v,w]$, and then moving the derivative form $w$ to $v$.
\begin{align*}
E_y^i&=L_y^i[v,w]=\int_{T_{i-1}}^{T_i}-v(t)(w'(t)+a w(t))dt + v(T_i)w(T_i) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-av(t))dt + v(T_i)w(T_i)-v(T_i)w(T_i) +v(T_{i-1})w(T_{i-1}) \\
&=\int_{T_{i-1}}^{T_i}w(t)(v'(t)-a v(t))dt + v(T_{i-1})w(T_{i-1}) \\
&=(L_y^i)^*[w,v]
\end{align*}
this means that $(E_y^i)^*=(L_y^i)^*[\cdot,\psi]$. The weak form of the adjoint equations is then found, by setting setting $(L_y^i)^*[p,\psi]=(J_{y_i},\psi)_i$. This gives two cases:
\\
\\
$i=N$ case:
\begin{align*}
&\textrm{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}p_N(t)\psi'(t)-a p_N(t)\psi(t)dt +p_N(T_{N-1})\psi(T_{N-1})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textrm{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}p_i(t)\psi'(t)-a p_i(t)\psi(t)dt +p_i(T_{i-1})\psi(T_{i-1})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
The strong formulation, is obtained by partial integration:
\\
\\
 $i=N$ case:
\begin{align*}
&\textrm{Find $p_N \in L^2(T_{N-1},T_N)$ such that }\forall \ \psi \in C^{\infty}((T_{N-1},T_N)) \\
&\int_{T_N-1}^{T_N}-p_N'(t)\psi(t)-a p_N(t)\psi(t)dt +p_N(T_{N})\psi(T_{N})
= \alpha(y(T_N)-y^T)\psi(T_N)\ 
\end{align*}
$i\neq N$ cases:
\begin{align*}
&\textrm{Find $p_i \in L^2(T_{i-1},T_i)$ such that }\forall \ \psi \in C^{\infty}((T_{i-1},T_i))\\
&\int_{T_i-1}^{T_i}-p_i'(t)\psi(t)-a p_i(t)\psi(t)dt +p_i(T_{i})\psi(T_{i})
= \mu(y_{i}(T_i)-\lambda_i )\psi(T_i) \ 
\end{align*}
This gives us the ODEs we wanted.
\end{proof}
\noindent
With the adjont equations we can find the gradient.
\begin{theorem}
The gradient of (\ref{penalty_func}), $\hat J_{\mu}'$, with respect to the control $(v,\Lambda)$ is:
\begin{align}
\hat J_{\mu}'(v,\Lambda) = (v+p,p_{2}(T_1) -p_{1}(T_1),..., p_{N}(T_{N-1}) -p_{N}(T_{N-1})) \label{penalty grad}
\end{align}
\end{theorem}
\begin{proof}
If we first find $E_v^*$, $E_{\lambda}^*$, $J_v$ and $J_{\lambda}$ we can derive the gradient by simply inserting these expression into (\ref{pen_abs_grad}). We begin with the $E$ terms:
\begin{align*}
E_v &= L_v[\cdot,\phi] = -(\cdot,\phi) \\
E_{\lambda_{i-1}}^i &= L_{y_{i-1}}^i[\cdot,\phi] = -(\cdot,\delta_{T_{i-1}}\phi)_i
\end{align*}
Notice that both of these forms are symmetric, and we therefore do not need to do more work to find their adjoints, they are however derived from the weak formulation, and it might therefore be easier to translate these forms to their strong counterpart:
\begin{align*}
E_v &=-1 \\
E_{\lambda_{i-1}}^i &= -\delta_{T_{i-1}}
\end{align*}
Then lets differentiate $ J_{\mu}$:
\begin{align*}
\frac{\partial}{\partial v} J_{\mu} &= v \\
\frac{\partial}{\partial \lambda_i}J_{\mu} &= - \mu (y_{i}(T_i)-\lambda_i)
\end{align*}
We can insert the above derived expressions into formula (\ref{pen_abs_grad}) to find the gradient. To make it simple we separate the $v$ and $\Lambda$ terms. First we look at the gradient associated with the source term $v$:
\begin{align*}
\frac{\partial}{\partial v} \hat J_{\mu}(v,\Lambda) &= -E_vp +  \frac{\partial}{\partial v} J_{\mu} \\
&=p+v
\end{align*}
We then find the components of the gradient related to $\lambda_i$.
\begin{align*}
\frac{\partial}{\partial \lambda_i} \hat J_{\mu}(v,\Lambda) &= -E_{\lambda_i} p +  \frac{\partial}{\partial \lambda_i} J_{\mu} \\
&= p_{i+1}(T_i)-\mu (y_{i}(T_i)-\lambda_i)\\
 &= p_{i+1}(T_i)-p_i(T_i)
\end{align*}
Combining $\frac{\partial}{\partial v} \hat J_{\mu}$ and $ \frac{\partial}{\partial \lambda_i}$ for $i=1,..,N-1$ gives us the gradient \ref{penalty grad}.
\end{proof} 
\section{Parareal preconditioner} \label{pc sec}
Parallelizing the solution process of optimal control problems with time dependent differential equation constraints comes down to solving a series of penalized control problems. Since we have derived the gradient of these penalized problems for a specific example, we can now solve the control problem numerically using an optimization algorithm. We can for example use the steepest descent method (\ref{SD_itr}), which would create the following iteration for each penalized control problem:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_k\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method}
\end{align}
Alternatively we could use a BFGS iteration (\ref{BFGS_itr}), which would result in the following update:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kH^{k}\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{bfgs_method}
\end{align}
Where $H^k$ is the inverse Hessian approximation defined in (\ref{inv_H_apr}). In order to improve convergence of the unconstrained optimization solvers, we include a preconditioner based on Parareal, proposed in \cite{maday2002parareal}, in our optimization algorithms. The preconditioner $Q$ will be on the form:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 & Q_{\Lambda} \\
	\end{array} \right],\quad Q_{\Lambda}\in\mathbb{R}^{N-1\times N-1} \label{PC_form}
\end{align} 
For steepest descent, we apply $Q$, by modifying (\ref{gradient_method}) in the following way:
\begin{align}
(v^{k+1},\Lambda^{k+1}) = (v^{k},\Lambda^{k}) -\rho_kQ\nabla\hat{J}_{\mu}(v^{k},\Lambda^{k}) \label{gradient_method2}
\end{align}
For us to expect any improvement in convergence for the preconditioned steepest descent, $Q$ would have to resemble the Hessian of $\hat{J}_{\mu}$, at least for the $\Lambda$ part of the control. Applying $Q$ to the BFGS iteration, is done by setting the initial Hessian approximation $H^0=Q$. To be able to do this, we need $Q$ to be symmetric positive definite, since that is a requirement on $H^0$. 
\\
\\
We derive $Q$ by looking at a constructed optimal control problem that we call the virtual problem. The virtual problem is a control problem decomposed as detailed in section \ref{decomp_sec}, but its objective function $\bold J$ is set to be the penalty term, and therefore only depends on $\Lambda$. We already stated this problem in section \ref{algebraic_sec}, and by utilizing the algebraic Parareal formulation, we will try to find a good candidate for $Q_{\Lambda}$.
\subsection{Virtual problem} \label{vir_sec}
In order for us to be able to derive the preconditioner from \cite{maday2002parareal}, we need the virtual optimal control problem from section \ref{algebraic_sec}. For reference let us restate it below:
\begin{align}
&\min_{\Lambda}\bold{J}(\Lambda,y) = \sum_{i=1}^{N-1} (y_{i-1}(T_{i})-\lambda_{i})^2 \label{virtual_func} \\
&\textrm{Subject to } \ y_{i-1}(T_{i}) = \bold F_{\Delta T}(\lambda_{i-1}) \ i=1,...,N-1 \label{virtual}
\end{align}
Here $\Lambda=(\lambda_0=y_0,\lambda_1,...,\lambda_ {N-1})$ and $\bold F_{\Delta T}$ is the fine propagator from section \ref{Parareal_sec}. In the context of the virtual problem (\ref{virtual_func}-\ref{virtual}), $\bold F_{\Delta T}(\omega)$ propagates $\omega$ one time step of length $\Delta T$ using the equation:
\begin{align}
\left\{
     \begin{array}{lr}
       	\frac{\partial}{\partial t} y(t)-ay(t)=0  \ \textrm{for } \ t\in(0,\Delta T),\\
       	y(0)=\omega.
     \end{array}
   \right. \label{virtual_exs}
\end{align} 
In chapter \ref{parareal_chap} we explained how the virtual problem could be solved by setting $\lambda_i= \bold F_{\Delta T}(\lambda_{i-1})$, which is the same as solving $\hat{J}(\Lambda)=0$. This eventually led us to the parareal scheme, defined on matrix form as:
\begin{align}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}(H-M\Lambda^k)\label{par_mat_sys}
\end{align}
Where $M$ and $\bar{M}$ are matrix representations of the fine and coarse resolution propagators $\lambda_i= \bold F_{\Delta T}(\lambda_{i-1})$ and  $\lambda_i= \bold G_{\Delta T}(\lambda_{i-1})$:
\begin{align*}
M = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} & \mathbbold{1}  \\
   \end{array}  \right],
\bar{M} = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{G}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{G}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{G}_{\Delta T} & \mathbbold{1}   \\
   \end{array}  \right]
\end{align*}
In section \ref{algebraic_sec} we said that $\bar M^{-1}$ can be seen as a preconditioner for the fix point iteration solving $\bold{J}(\Lambda,y)=0$. Solving the virtual problem in this way is only possible since we know that the minimum value of $\bold{J}(\Lambda,y)$ is zero. A more natural approach to solving this problem, would first be to reduce $\bold J$ to only depend on $\Lambda$, and then solve $\hat {\bold J}'(\Lambda)=0$. If we could find a preconditioner to a fix point iteration solving $\hat{ \bold J} '(\Lambda)=0$, this would be a candidate for $Q_{\Lambda}$. Let us start by writing up the reduced version of (\ref{virtual_func}-\ref{virtual}):
\begin{align}
\min_{\Lambda\in\mathbb{R}^{N-1}}\hat {\bold J}(\Lambda) = \sum_{i=1}^{N-1} (\bold F_{\Delta T}(\lambda_{i-1})-\lambda_{i})^2,\quad \lambda_0=y_0 \label{reduced_viritual}
\end{align} 
Now that we have an expression for the reduced objective function $\hat {\bold J}$, we can try to find its gradient. Luckily for us we have already derived the gradient for the more general problem (\ref{exs_J}-\ref{exs_E}) in (\ref{penalty grad}). We get the virtual control problem from the more general problem if we remove the control variable $v$ from the state and objective function of (\ref{exs_J}-\ref{exs_E}). $\hat {\bold J}'(\Lambda)$ is therefore given as:
\begin{align*}
\hat{\bold J}'(\Lambda) = \{p_i(T_i)-p_{i-1}(T_i)\}_{i=1}^{N-1}
\end{align*}
Here $p^i$ are the solutions of the adjoint equations (\ref{exs_adjoint}). Since we need the adjoints, let us define the fine adjoint propagator $\bold{F}_{\Delta T}^*(\omega)$ as we did for $\bold{F}_{\Delta T}$, but now $\omega$ is propagated backwards one time step of length $\Delta T$, and the equation $\bold{F}_{\Delta T}^*$ solves is:   
\begin{align}
\left\{
     \begin{array}{lr}
	\frac{\partial }{\partial t}p +ap=0,\quad t\in (0,\Delta T)  \\
	p(\Delta T) = \omega
	\end{array}
   \right. \label{virtual_adjoint_exs}
\end{align}
We also define $\bold G_{\Delta T}^*$ as the fine adjoint propagators coarse counterpart. Next we can define the matrices $M^*$ and $\bar M^*$ as:
\begin{align*}
M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{F}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{F}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{F}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right],
\bar M^*= \left[ \begin{array}{cccc}
   \mathbbold{1} & -\bold{G}_{\Delta T}^* & 0 & 0 \\  
   0 & \mathbbold{1} & -\bold{G}_{\Delta T}^* & \cdots \\ 
   \cdots &0 &  \mathbbold{1} & -\bold{G}_{\Delta T}^* \\
   0 &\cdots &\cdots &  \mathbbold{1}  \\
   \end{array}  \right]
\end{align*}
It then turns out that solving $\hat{J}'(\Lambda)=0$ is the same as solving the system:
\begin{align}
M^* \ M \ \Lambda \ = \ M^* \ H \label{vir_grad_sys}
\end{align}
The reason we see by moving $M^*H$ to the left hand side of (\ref{vir_grad_sys}) and writing out what $M^*( \ M \ \Lambda-H)$ means. Firstly:
\begin{align}
M \ H-\Lambda  = \left( \begin{array}{c}
	y_0-\lambda_0  \\
	 \bold{F}_{\Delta T}(\lambda_0)-\lambda_1 \\
	 \bold{F}_{\Delta T}(\lambda_1)-\lambda_2  \\
	\cdots \\
	\bold{F}_{\Delta T}(\lambda_{N-2})-\lambda_{N-1} 
	\end{array} \right)
\end{align}
We recognise $\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i=y^i(T_i) -\lambda_i$, $i=1,...,N-1$ as the initial conditions of the adjoint equations (\ref{exs_adjoint}). This means that $\bold{F}_{\Delta T}(\lambda_{i-1})-\lambda_i=p^i(T_i)$, and that: 
\begin{align}
M \ H-\Lambda  = \left( \begin{array}{c}
	y_0-\lambda_0  \\
	 p^1(T_1) \\
	 p^2( T_2) \\
	\cdots \\
	p^{N-1}(T_{N-1}) 
	\end{array} \right)
\end{align}
If we then apply $M^*$ to $M \ H-\Lambda$, we get:
\begin{align}
M^* (M \ H-\Lambda)&=
	\left( \begin{array}{c}
	y_0-\lambda_0 -\bold{F}_{\Delta T}^*(p^1(T_1))\\
	 p^1(T_1)-\bold{F}_{\Delta T}^*(p^2( T_2))\\
	p^2( T_2)-\bold{F}_{\Delta T}^*(p^3( T_3))\\
	\cdots \\
	p^{N-1}(T_{N-1})
	\end{array} \right)
	\\
	&=\left( \begin{array}{c}
	y_0-\lambda_0 - p_1(T_0)\\
	p_1(T_1)-p_2(T_2)\\
	p_3(T_2)-p_2(T_2)\\
	\cdots \\
	p_{N-2}(T_{N-2})-p_{N-1}(T_{N-2}) \\
	p_{N-1}(T_{N-1})
	\end{array} \right)
\end{align}
The last vector, can be recognised as the negative gradient of (\ref{virtual_func}) in its 2nd to $(N-1)$th indices, which shows that solving (\ref{vir_grad_sys}) is the same as solving $\hat {\bold J}'(\Lambda)=0$. If we then created a fix point iteration for (\ref{vir_grad_sys}), in a similar fashion as (\ref{par_mat_sys}), it would look like this:
\begin{align*}
\Lambda^{k+1} = \Lambda^k + \bar{M}^{-1}\bar M^{-*}(H-M^*M\Lambda^k)
\end{align*}
$\bar{M}^{-1}\bar{M}^{-*}$ could then be thought of as the preconditioner of the iteration, in the sense that $\bar{M}^{-1}\bar{M}^{-*}M^*M$ would be close to $\mathbbold{1}$. If $\bar{M}^{-1}\bar{M}^{-*}$ works as a preconditioner for  $\hat {\bold J}'(\Lambda)=0$ where $\hat {\bold J}$ is the objective function in the virtual problem, \cite{maday2002parareal} proposes $\bar{M}^{-1}\bar{M}^{-*}$ as a preconditioner for the penalized optimal control problem (\ref{decomp_E}-\ref{penalty_func}). Meaning that we set $Q$ to be:
\begin{align}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 &  \bar{M}^{-1}\bar{M}^{-*}\\
	\end{array} \right] \label{Q_PC}
\end{align}  
We have derived $Q$ by trying to find a good preconditioner for the fix point iteration for solving $\hat {\bold J}'(\Lambda)=0$. Whether $\bar{M}^{-1}\bar{M}^{-*}$ has anything to do with the inverse Hessian of $\hat J'$ is however not clear. It turns out that $M^*M$ actually is related to the Hessian of the virtual problem, which we will show in the next subsection.
\subsection{$\bar{M}^{*}\bar{M}$ as an approximation of the Hessian} 
In order for us to make it easier to differentiate the gradient of the reduced objective function (\ref{reduced_viritual}), we try to write up $\hat{\bold J}$ on vector notation. Even though the propagator $\bold F$ is generally defined, and might very well be non-linear, we will now and for the rest of this subsection assume that the propagator is linear. Let us first define the vector $x(\Lambda)\in\mathbb{R}^{N-1}$ as:
\begin{align}
x(\Lambda)= \left( \begin{array}{c}  
   \lambda_1 - y_1(T_1) \\ 
   \lambda_2 - y_2(T_2) \\
   \cdots  \\
   \lambda_{N-1} -y_{N-1}(T_{N-1})  \\
   \end{array}  \right)
\end{align} 
We can then formulate the problem (\ref{reduced_viritual}) as a least square problem:
\begin{align}
\min_{\Lambda}\hat{\bold J}(\Lambda) = \min_{\Lambda}x(\Lambda)^Tx(\Lambda) \label{vector_J}  
\end{align}
Here $x(\Lambda)^T$ means $x(\Lambda)$ transposed. We then want to express $x$ on matrix notation in terms of $\Lambda$ and $y_0$. To do this, we first find an expression for $\{y_i(T_i)\}_{i=1}^{N-1}$:
\begin{align}
\left( \begin{array}{c}
   y_0(T_1) \\  
   y_1(T_2) \\ 
   \cdots  \\
   y_{N-1}(T_N)  \\
   \end{array}  \right)= 
   \left[ \begin{array}{cccc}  
   0 & 0 & \cdots & 0 \\ 
   \bold{F}_{\Delta T}&0 & 0  & \cdots \\
   0 &  \bold{F}_{\Delta T}&0 & \cdots \\
   0 &\cdots &\bold{F}_{\Delta T}& 0   \\
   \end{array}  \right]
   \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) + 
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right)
\end{align}
and when we insert this into $x$, we get:
\begin{align}
x = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T} &  \mathbbold{1}  \\
   \end{array}  \right]
   \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) -
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right)
\end{align}
or:
\begin{align}
x  &= M \left( \begin{array}{c}
   \lambda_1 \\  
   \lambda_2 \\ 
   \cdots  \\
   \lambda_{N-1}  \\
   \end{array}  \right) -
   \left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right) \\
   & = M \Lambda -\left( \begin{array}{c}
   \bold{F}_{\Delta T}(y_0)\\  
   0\\ 
   \cdots  \\
   0  \\
   \end{array}  \right) \\
   &=M \Lambda-b
\end{align}
This allows us to write up the reduced functional $\hat{\bold J}$, that only depends on $\Lambda$:
\begin{align}
\hat{\bold J}(\Lambda)&= x^Tx =
(M  \Lambda -b)^T(M  \Lambda -b) \\
&= (M  \Lambda)^T(M  \Lambda) - (M  \Lambda)^Tb-b^T(M  \Lambda) + b^Tb \\
&=\Lambda^TM^TM  \Lambda - 2\Lambda^TM^Tb + b^Tb \label{T_J}
\end{align}
An important thing to note about the above expression, is that $M^T$ does not necessarily equal $M^*$, since $M^*$ is defined by the fine adjoint propagator $\bold F_ {\Delta T}^*$, and is not simply the transposed of $M$. Before we look further into the difference of $M^*$ and $M^T$, we write up the Hessian of $\hat{\bold J}$ in the proposition below:
\begin{proposition}\label{prop_lLS}
The Hessian of the reduced objective function $\hat{\bold J}$ (\ref{T_J}) originating from the virtual problem (\ref{virtual_func}-\ref{virtual}) is:
\begin{align}
\nabla^2\hat{\bold J}(\Lambda) = 2 M^TM
\end{align}
\end{proposition}
\begin{proof}
We can easily verify the above proposition by differentiating expression (\ref{T_J}). First the gradient $\nabla \hat{\bold J}$ is:
\begin{align*}
\nabla\hat{\bold J}(\Lambda) = 2 M^TM\Lambda - 2M^Tb
\end{align*}
Notice that if we assume that $M^T=M^*$, setting $\nabla\hat{J}(\Lambda)=0$ gives us the system (\ref{vir_grad_sys}). If we now differentiate $\nabla \hat{\bold J}(\Lambda)$, we get the Hessian we wanted:
\begin{align}
\nabla^2 \hat{\bold J}(\Lambda) = 2 M^TM
\end{align}
\end{proof}
\noindent
This means that if $M^*=M^T$, then $M^*M$ is the Hessian of $\frac{1}{2}\hat{\bold J}$, and since $\bar{M}$ and $\bar{M}^*$ approximates $M$ and $M^*$, the matrix $\bar{M}^{*}\bar{M}$ should be an approximation of $\frac{1}{2}\nabla^2 \hat{\bold J}$, which means its inverse $\bar{M}^{-1}\bar{M}^{-*}$ should approximate the inverse Hessian. However as we have already said, these conclusions are based on the assumption that $M^*=M^T$. Let us therefore investigate if this assumption holds when we solve the virtual problem (\ref{virtual_func}-\ref{virtual}) for equation (\ref{virtual_exs}). 
\\
\\
Let the fine propagator $\bold F_{\Delta T}$ and the fine adjoint propagator $\bold F_{\Delta T}^*$ solve equations (\ref{virtual_exs}) and (\ref{virtual_adjoint_exs}) exactly. Due to the simplicity of these equations, we can easily write up the solutions of these equations:
\begin{align}
y(t)&=y(0)e^{at}, \\
p(t)&=p(\Delta T) e^{a(\Delta T-t)}
\end{align} 
This means that $\bold F_{\Delta T}(\omega)=y(\Delta T)=\omega e^{a\Delta T}$, and that $\bold F_{\Delta T}^*(\omega)=p(0)=\omega e^{a\Delta T}$. If we insert these expressions into $M$ and $M^*$, we get:
\begin{align*}
M = \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -e^{a\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   0 &-e^{a\Delta T} & \mathbbold{1}  & \cdots \\
   0 &\cdots &-e^{a\Delta T} & \mathbbold{1}  \\
   \end{array}  \right],
M^* =\left[ \begin{array}{cccc}
   	\mathbbold{1} & - e^{a\Delta T} & 0 & 0 \\  
  	 0 & \mathbbold{1} & - e^{a\Delta T} & \cdots \\ 
  	 \cdots &0 &  \mathbbold{1} & - e^{a\Delta T} \\
  	 0 &\cdots &\cdots &  \mathbbold{1}  \\
  	 \end{array}  \right]
\end{align*}
Looking at the $M$ and $M^*$ matrices above, it is quite clear that $M^*=M^T$, which means that for the virtual problem (\ref{virtual_func}-\ref{virtual}) governed by the equation (\ref{virtual_exs}), the assumption $M^*=M^T$ holds, when the fine propagator and the fine adjoint propagator solved the state and adjoint equations exactly. If one instead let the underlying solvers of $\bold F_{\Delta T}$ and $\bold F_{\Delta T}^*$ be based on some numerical method, the $M$ and $M^*$ matrices will of course depend on the choice of this method. When we discretize the state and adjoint equations, we want the matrices $M$ and $M^*$ to behave as they did for non-discrete $\bold F_{\Delta T}$ and $\bold F_{\Delta T}^*$.
\\
\\
Since $\bar{M}$ and $\bar{M}^{*}$ are only approximations of $M$ and $M^*$, we might think that $\bar{M}^*=\bar{M}^T$ is unnecessary. However since we want to use $\bar{M}^{-1}\bar{M}^{-*}$ as an initial inverse Hessian approximation in our BFGS-optimization algorithm, we need $\bar{M}^{-1}\bar{M}^{-*}$ to be symmetric positive definite. This is true if $\bar{M}^*=\bar{M}^T$, because the matrix product between an invertible matrix and its transpose always is positive definite. This holds because:
\begin{align*}
x^TM^TMx=(Mx)^TMx=||Mx||^2\geq &0, \quad \textrm{and} \\
||Mx||^2=0 \iff x=&0
\end{align*}
The second requirement for positive definiteness is true if $M$ is invertible, which is trivially true, since $M$ is a triangular matrix with the identity on its diagonal, and this means that $M$ has a non-zero determinant. Since $\bar{M}^*=\bar{M}^T$ guaranties that $\bar{M}^{-1}\bar{M}^{-*}$ is positive definite, we want to choose coarse propagators that gives us this property. Let us try to find out how we need to define $\bold G_{\Delta T}^*$, when we use the coarse propagator suggested in \cite{lions2001resolution} written up in (\ref{G_Backward}) based on the implicit Euler finite difference scheme. For equation (\ref{virtual_exs}), we can use (\ref{G_Backward}) to derive an expression for evaluating the coarse propagator $\bold G_{\Delta T}(\omega)$:
\begin{align}
\frac{\bold{G}_{\Delta T}(\omega) -\omega}{\Delta T } - a \bold{G}_{\Delta T}(\omega) &= 0 \\
\bold{G}_{\Delta T}(\omega)(1-a\Delta T)&= \omega \\
\bold{G}_{\Delta T}(\omega)&=\frac{\omega}{(1-a\Delta T)}
\end{align}
The implicit Euler coarse propagator for equation (\ref{virtual_exs}) then reads:
\begin{align*}
\bar M = \left[ \begin{array}{cccc}
   	\mathbbold{1} & 0 & \cdots & 0 \\  
   	\frac{-1}{1-a\Delta T} & \mathbbold{1} & 0 & \cdots \\ 
   	0 &\frac{-1}{1-a\Delta T} & \mathbbold{1}  & \cdots \\
   	0 &\cdots &\frac{-1}{1-a\Delta T} & \mathbbold{1}  \\
  	\end{array}  \right]
\end{align*}
In order for us to get $\bar{M}^*=\bar{M}^T$, we the discretization of (\ref{virtual_adjoint_exs}) to produce $\bold{G}_{\Delta T}^*(\omega)=\frac{\omega}{(1-a\Delta T)}$. Maybe unsurprisingly, it turns out that this is achieved by again discretizing (\ref{virtual_adjoint_exs}) using implicit Euler. $\bold{G}_{\Delta T}^*(\omega)$ is found as follows:
\begin{align*}
\frac{\omega-\bold{G}_{\Delta T}^*(\omega)}{\Delta T} + a \bold{G}_{\Delta T}^*(\omega)&=0 \\
\bold{G}_{\Delta T}^*(\omega)(a\Delta T-1)&=-\omega \\
\bold{G}_{\Delta T}^*(\omega)&=\frac{\omega}{(1-a\Delta T)}
\end{align*}
In the above derivation for $\bold{G}_{\Delta T}^*(\omega)$, the forward Euler finite difference scheme is used to discretize (\ref{virtual_adjoint_exs}), but since this equation is solved backwards in time this scheme is implicit. Since $\bold{G}_{\Delta T}^*(\omega)=\bold{G}_{\Delta T}(\omega)$, we get that $\bar{M}^*=\bar{M}^T$, which means that $\bar{M}^{-1}\bar{M}^{-*}$ is a valid choice for an initial inverted Hessian approximation in the BFGS-algorithm.
\subsection{Non-linear state equations}
If the governing equation of the fine propagator $\bold F_{\Delta T}$ in system (\ref{virtual_func}-\ref{virtual}) is non-linear, the Hessian of the non-linearly constrained objective function would not be equal to the Hessian stated in proposition \ref{prop_lLS}. However, we can still use the least square formulation (\ref{vector_J}) for minimization of the reduced objective function, which looks like: 
\begin{align}
\hat{\bold J}(\Lambda) = x(\Lambda)^Tx(\Lambda), \quad x:\mathbb{R}^{N-1}\rightarrow \mathbb{R}^{N-1} \label{non_lin_LS}
\end{align} 
Here the components of $x$ are defined as $x_i(\Lambda)=\lambda_{i}-\bold F_{\Delta T}(\lambda_{i-1})$, $i=1,...,N-1$. Let us now state the Hessian of the reduced objective function (\ref{non_lin_LS}) in terms of $x$, and the propagator in a proposition:
\begin{proposition}\label{NonLin_prop}
The Hessian of function (\ref{non_lin_LS}) is
\begin{align*}
\nabla^2 \hat{\bold J}(\Lambda) &= 2\nabla x^T\nabla x + 2\sum_{i=1}^{N-1} \nabla^2 x_i(\Lambda) x_i(\Lambda)\\
&=2M(\Lambda)^TM(\Lambda) + 2D(\Lambda)
\end{align*}
Here $D(\Lambda)$ is a diagonal matrix with diagonal entries 
\begin{align*}
D_i=-\bold{F}_{\Delta T}''(\lambda_i)(\lambda_{i+1}-\bold F_{\Delta T}(\lambda_i)) \quad i=1,...,N-1,
\end{align*}
while $M(\Lambda)$ is the linearised forward model:
\begin{align*}
M(\Lambda) &= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{F}_{\Delta T}'(\lambda_{1}) & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{F}_{\Delta T}'(\lambda_{2}) & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{F}_{\Delta T}'(\lambda_{N-1}) & \mathbbold{1}  \\
   \end{array}  \right]
\end{align*}	
\end{proposition}
\begin{proof}
We start by differentiating $\hat{\bold J}$:
\begin{align*}
\nabla \hat{\bold J}(\Lambda) &= 2 \nabla x(\Lambda)^T x(\Lambda)\\
&=2\sum_{i=1}^{N-1} \nabla x_i(\Lambda) x_i(\Lambda)
\end{align*}
If we now differentiate $\nabla \hat{\bold J}$, we get:
\begin{align*}
\nabla^2 \hat{\bold J}(\Lambda) &= 2\nabla x^T\nabla x + 2\sum_{i=1}^{N-1} \nabla^2 x_i(\Lambda) x_i(\Lambda)
\end{align*}
We see that $\nabla x(\Lambda)=M(\Lambda)$, by looking at $\frac{\partial x_i}{\partial \lambda_j}$
\begin{align*}
\frac{\partial x_i}{\partial \lambda_j} = \left\{
     \begin{array}{lr}
		1 \quad\quad\quad\quad\quad i=j\\
		-\bold F_{\Delta T}'(\lambda_{j}) \quad i>1 \wedge j=i-1 \\
		0 \quad\quad\quad\quad\quad i\neq j \vee j\neq i-1
	\end{array}
   \right.	
\end{align*}
We can similarly find $\nabla^2 x_i$ by differentiating $x$ twice:
\begin{align*}
\frac{\partial^2 x_i}{\partial \lambda_j\partial\lambda_k} = \left\{
     \begin{array}{lr}
		-\bold F_{\Delta T}''(\lambda_{j}) \quad i>1 \wedge j=k=i-1 \\
		0 \quad\textrm{in all other cases}
	\end{array}
   \right.	
\end{align*}
Now summing up the terms $\nabla^2 x_i(\Lambda)x_i(\Lambda)$ would yield the diagonal matrix $D(\Lambda)$ described in proposition \ref{NonLin_prop}.
\end{proof}
\noindent
As we have seen the Hessian of the non-linear problem consists of two parts. One is the linearised forward model multiplied with its adjoint, while the second part is a diagonal matrix related to the second derivative of the propagator $\bold F_{\Delta T}$, and the residuals $\lambda_i-\bold F_{\Delta T}$. The first part of $\nabla^2 \bold{\hat{J}}$ is analogue to the Hessian of the linear problem. It is symmetric positive definite, and taking its inverse corresponds to first applying the backwards model, and then the forward model. What makes the Hessian of the non-linear problematic is therefore its second term. The first issue with the diagonal matrix $D(\Lambda)$, is how to calculate $\bold F_{\Delta T}''$. Another issue is that we can not guarantee that the sum of $M(\Lambda)^TM(\Lambda)$ and $D(\Lambda)$ is a positive matrix, and the same problem would arise in a coarse approximation of $\nabla^2 \bold{\hat{J}}$. The lack of positivity is a problem since we want to use the coarse approximation as an initial inverted Hessian approximation in the BFGS-algorithm.
\\
\\
A way to get around the $D(\Lambda)$ term in the Hessian for non-linearly constrained problem, is simply to ignore it. This leaves us with the $M(\Lambda)^TM(\Lambda)$ term, which we know how to deal with. Ignoring the term depending on the second derivative and the residual is actually a known strategy for for solving non-linear least square problems. Details can be found in \cite{nocedal2006numerical}. A justification for this approach, is that at least in instances where we are close to a solution, the $\lambda_i-\bold F_{\Delta T}$ terms will be close to zero, and the $M(\Lambda)^TM(\Lambda)$ term will therefore dominate the Hessian. Ignoring the $D(\Lambda)$ term means that we can define an inverse Hessian approximation based on a coarse propagator $\bold G_{\Delta T}$ in the same way as we did for the problem with linear state equation constraints. This means that we define a matrix $\bar M(\Lambda)$:
\begin{align*}
\bar M(\Lambda) &= \left[ \begin{array}{cccc}
   \mathbbold{1} & 0 & \cdots & 0 \\  
   -\bold{G}_{\Delta T}'(\lambda_{1}) & \mathbbold{1} & 0 & \cdots \\ 
   0 &-\bold{G}_{\Delta T}'(\lambda_{2}) & \mathbbold{1}  & \cdots \\
   0 &\cdots &-\bold{G}_{\Delta T}'(\lambda_{N-1}) & \mathbbold{1}  \\
   \end{array}  \right]
\end{align*}
The term $\bar{M}(\Lambda)^{-1}\bar{M}(\Lambda)^{-*}$ can then be used in an approximation of the inverse Hessian, as detailed in section \ref{vir_sec}.
\subsection{Summary and remarks}
We end this chapter with a short summary of section \ref{pc sec} and with some remarks on the proposed preconditioner $Q$ in (\ref{Q_PC}). The starting point of the section is the decomposed optimal control problem with time dependent DE constraints on the form:
\begin{align}
&\min_{(v,\Lambda)}\hat J_{\mu}(v,\Lambda) =\hat{J}(v)+ \sum_{i=1}^{N-1} (y_{i-1}(T_{i})-\lambda_{i})^2  \\
&\textrm{Subject to } \ y_{i-1}(T_{i}) = \bold F_{\Delta T}(\lambda_{i-1}) \ i=1,...,N-1 
\end{align}
We have presented a the preconditoner $Q\in\mathbb{R}^{n+N\times n+N}$, where $n+1$ is the dimension of the space where the discretized control variable $v$ lives, while $N$ is the number of decomposed subintervals. $Q$ was defined in (\ref{Q_PC}) based on the matrices $\bar M,\bar M^*\in\mathbb{R}^{N-1\times N-1}$, and on block form $Q$ reads as the following:
\begin{align*}
Q = \left[ \begin{array}{cc}
	\mathbbold{1} & 0 \\
	0 &  \bar{M}^{-1}\bar{M}^{-*}\\
	\end{array} \right] 
\end{align*}
$Q$ is a preconditioner for optimization algorithms, and is supposed to be applied to the the gradient $\nabla \hat J_{\mu}(v,\Lambda)\in\mathbb{R}^{N+n}$ at every iteration. We see that $Q$ only effects the last part of the gradient, which is the $\Lambda$ part of the gradient. One important thing of note about $Q$, is that when $N=2$ both $\bar M$ and $\bar M^*$ are the identity, which means that for $N=2$, $Q=\mathbbold{1}$. Since $Q$ has no effect for $N=2$, we might also expect that for "small" $N$, the effect of $Q$ is only modest, and that the usefulness of only materializes for higher values of decomposed subintervals $N$. The last remark on our preconditionr will be a quick explanation of how we typically apply it to the gradient. We do this through a simple example.   
\\
\\
Assume that we decompose $I=[0,T]$ into four sub-intervals $[T_0,T_1], [T_1,T_2], [T_2,T_3]$ and $[T_3,T_4]$. To enforce continuity at the overlapping boundaries, we need three variables $\lambda_1,\lambda_2$ and $\lambda_3$, that represents the initial condition of the state equation at $T_1,T_2$ and $T_3$. We then add quadratic penalty terms to our functional as outlined in section \ref{penalty_sec}. The gradient of our new functional $J(v,\Lambda)$ would then have three components $J_{\lambda_1}, J_{\lambda_2}, J_{\lambda_3}$. These are the components of the gradient that we wish to apply $Q$ on. Applying $Q$ is then done in two steps. We first resolve the backward system that we get from $\bar M^{-*}$:
\begin{align*}
\bar{J_{\lambda_1}} &=J_{\lambda_1} +\bold{G}_{\Delta T}^*(J_{\lambda_2} + \bold{G}_{\Delta T}^*(J_{\lambda_3}))\\
\bar{J_{\lambda_2}} &=J_{\lambda_2} + \bold{G}_{\Delta T}^*(J_{\lambda_3})\\
\bar{J_{\lambda_3}} &=J_{\lambda_3} 
\end{align*} 
Then second step is to apply the forward system:
\begin{align*}
\bar{\bar{J_{\lambda_1}}}&=\bar{J_{\lambda_1}} \\
\bar{\bar{J_{\lambda_2}}}&=\bar{J_{\lambda_2}}+ \bold{G}_{\Delta T}(\bar{J_{\lambda_1}}) \\
\bar{\bar{J_{\lambda_3}}}&=\bar{J_{\lambda_3}} + \bold{G}_{\Delta T}(\bar{J_{\lambda_2}}+ \bold{G}_{\Delta T}(\bar{J_{\lambda_1}}))
\end{align*} 
The result of using $Q$ on the gradient, is that $\Lambda$ part of the gradient is changed to $\bar{\bar{J_{\lambda_1}}}, \bar{\bar{J_{\lambda_2}}},\bar{\bar{J_{\lambda_3}}}$.  

