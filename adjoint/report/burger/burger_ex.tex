\documentclass[11pt,a4paper]{report}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}

\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


\usepackage{graphicx}


\begin{document}
\begin{center}

\LARGE Burgers equation


\end{center}
Lets look at the optimal control with PDE constraint problem, where the equation is the burgers equation:
\begin{align*}
u_t + uu_x + \nu u_{xx} &= 0 \ \text{for $(x,t)\in \Omega\times(0,\infty)$}\\
u(x,t) &= h(x,t) \ \text{for $(x,t) \in\partial\Omega\times(0,\infty)$ } \\
u(x,0) &= g(x) \ \text{for $x \in\Omega$ }
\end{align*} 
Here $\Omega = (a,b)$. The functional is on the form:
\begin{align*}
J(u(g),g) = \int_0^T\int_{\Omega} u(x,t)^2 dxdt
\end{align*}
Here we want to minimize $J$ with respect to the initial condition $g$. If we differentiate $J$ with respect to $g$, we get:
\begin{align*}
\hat{J}'(g)(s) &= \langle u'(g)^*J_u,s \rangle \\
&= \langle -E_gp,s \rangle
\end{align*}
where $p$ is the solution of the adjoint equation:
\begin{align*}
E_u^*p = J_u
\end{align*}
By $E$ I mean burgers equation, which means that:
\begin{align*}
E_u &= \frac{\partial}{\partial t} + u_x + u\frac{\partial}{\partial x} + \nu\Delta + \delta_{t=0} + \delta_{\partial \Omega} \\
E_g &= -\delta_{t=0} \\
E_u^* &= -\frac{\partial}{\partial t}  -u\frac{\partial}{\partial x}+ \nu\Delta + \delta_{t=T} + (1+h)\delta_{\partial \Omega} \\
J_u &= 2u
\end{align*}
The adjoint equation would then look like:
\begin{align*}
-p_t -up_x + \nu p_{xx} &= 2u \ \text{for $(x,t)\in \Omega\times(0,\infty)$}\\
p(x,t) &= 0 \ \text{for $(x,t) \in\partial\Omega\times(0,\infty)$ } \\
p(x,T) &= 0 \ \text{for $x \in\Omega$ }
\end{align*}
This would mean that the gradient of $\hat{J}$ is:
\begin{align*}
\hat{J}'(g)(s) = \langle p(\cdot,0), s\rangle
\end{align*}
\textbf{Two time intervals}
\\
Divide the time interval $[0,T]$ into $[0,T_1]$ and $[T_1,T_2]$, with $T_2=T$. Then solve the equation separately on the two intervals for functions $u^1$ and $u^2$. The only difference is that $u^2$ has its own initial condition at $t=T_1$, that we call $\lambda$. To solve the problem, we now have to add a penalty term to the functional. The new functional looks as the following:
\begin{align*}
J(u(g),g,\lambda) = \int_0^T\int_{\Omega} u(x,t)^2 dxdt + \frac{\mu}{2}\int_{\Omega} (u^1(x,T_1)-\lambda(x))^2dx
\end{align*} 
We now have a new unknown $\lambda$, which is the initial condition of the second time interval. We now want the gradient of the new functional:
\begin{align*}
\langle \hat{J}'(g,\lambda), (s,l)\rangle &= \langle \frac{\partial u(g,\lambda)}{\partial(g,\lambda)}^* J_g(u(g,\lambda),g,\lambda), (s,l)\rangle + \langle J_g+J_{\lambda}, (s,l)\rangle \\
&=\langle -(E_g+E_{\lambda})p , (s,l)\rangle + \langle J_g+J_{\lambda}, (s,l)\rangle
\end{align*}
Again $p$ is the solution of the adjoint equation, and as for $u$, $p$ is separated into two equations. To derive the gradient lets divide $E$ into $E^1$ and $E^2$:
\begin{align*}
E^1 &= u_t^1 + u^1u_x^1 + \nu u_{xx}^1 +\delta_{t=0}(u^1-g) + \delta_{\partial \Omega}(u^1-g)\\
E^2 &= u_t^2 + u^2u_x^2 + \nu u_{xx}^2 +\delta_{t=T_1}(u^2-\lambda) + \delta_{\partial \Omega}(u^2-g)
\end{align*} 
If we differentiate $E$ we get the following:
\begin{align*}
E_u^i&=\frac{\partial}{\partial t} + u_x^i + u^i\frac{\partial}{\partial x} + \nu\Delta + \delta_{t=0} + \delta_{\partial \Omega} \\
E_g^1 &= -\delta_{t=0} \\
E_{\lambda}^2 &= -\delta_{t=T_1}
\end{align*}
Now let us differentiate $J$:
\begin{align*}
J_g &= 0 \\
J_u &= 2u + \mu(u^1-\lambda)\delta_{t=T_1}\\
\langle J_{\lambda},l\rangle &= -\mu\int_{\Omega} (u^1(x,T_1)-\lambda(x))l(x)dx
\end{align*}
We need the adjoint of $E_u^i$, but this is almost the same as above, so instead, we can write down the two adjoint equations:
\\
$i=2$:
\begin{align*}
-p_t^2 -u^2p_x^2 + \nu p_{xx}^2 &= 2u^2 \ \text{for $(x,t)\in \Omega\times(T_1,T_2)$}\\
p^2(x,t) &= 0 \ \text{for $(x,t) \in\partial\Omega\times(T_1,T_2)$ } \\
p^2(x,T_2) &= 0 \ \text{for $x \in\Omega$ }
\end{align*}
$i=1$:
\begin{align*}
-p_t^1 -u^1p_x^1 + \nu p_{xx}^1 &= 2u^1 \ \text{for $(x,t)\in \Omega\times(0,T_1)$}\\
p^1(x,t) &= 0 \ \text{for $(x,t) \in\partial\Omega\times(0,T_1)$ } \\
p^1(x,T_1) &= \mu(u^1(x,T_1)-\lambda(x)) \ \text{for $x \in\Omega$ }
\end{align*}
This gives us the following gradient:
\begin{align*}
\langle \hat{J}'(g,\lambda), (s,l)\rangle &=\langle -(E_g+E_{\lambda})p , (s,l)\rangle + \langle J_g+J_{\lambda}, (s,l)\rangle \\
&= \int_{\Omega} p^1(x,0)s(x)dx + \int_{\Omega} p^2(x,T_1)l(x)dx -\mu\int_{\Omega} (u^1(x,T_1)-\lambda(x))l(x)dx \\
&= \int_{\Omega} p^1(x,0)s(x)dx + \int_{\Omega} (p^2(x,T_1)-p^1(x,T_1))l(x)dx
\end{align*}
\end{document}